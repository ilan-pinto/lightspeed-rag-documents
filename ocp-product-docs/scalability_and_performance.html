<!DOCTYPE html>
<html lang="en" dir="ltr" prefix="og: https://ogp.me/ns#">
  <head>
    <meta charset="utf-8" />
<link rel="canonical" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/index" />
<link rel="shortlink" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/index" />
<meta property="og:site_name" content="Red Hat Customer Portal" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/index" />
<meta property="og:title" content="Scalability and performance OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<meta property="og:description" content="This document provides instructions for scaling your cluster and optimizing the performance of your OpenShift Container Platform environment." />
<meta property="og:image" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:description" content="This document provides instructions for scaling your cluster and optimizing the performance of your OpenShift Container Platform environment." />
<meta name="twitter:title" content="Scalability and performance OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<meta name="twitter:url" content="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/index" />
<meta name="twitter:image" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
<meta name="title" content="Scalability and performance OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<link rel="alternate" hreflang="en" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/index" />
<link rel="alternate" hreflang="ko" href="https://access.redhat.com/documentation/ko-kr/openshift_container_platform/4.13/html-single/scalability_and_performance/index" />
<link rel="alternate" hreflang="zh-hans" href="https://access.redhat.com/documentation/zh-cn/openshift_container_platform/4.13/html-single/scalability_and_performance/index" />
<link rel="alternate" hreflang="ja" href="https://access.redhat.com/documentation/ja-jp/openshift_container_platform/4.13/html-single/scalability_and_performance/index" />
<link rel="alternate" hreflang="es" href="https://access.redhat.com/documentation/es-es/openshift_container_platform/4.13/html-single/scalability_and_performance/index" />
<link rel="alternate" hreflang="ru" href="https://access.redhat.com/documentation/ru-ru/openshift_container_platform/4.13/html-single/scalability_and_performance/index" />
<link rel="alternate" hreflang="pt-br" href="https://access.redhat.com/documentation/pt-br/openshift_container_platform/4.13/html-single/scalability_and_performance/index" />
<link rel="alternate" hreflang="it" href="https://access.redhat.com/documentation/it-it/openshift_container_platform/4.13/html-single/scalability_and_performance/index" />
<link rel="alternate" hreflang="de" href="https://access.redhat.com/documentation/de-de/openshift_container_platform/4.13/html-single/scalability_and_performance/index" />
<link rel="alternate" hreflang="fr" href="https://access.redhat.com/documentation/fr-fr/openshift_container_platform/4.13/html-single/scalability_and_performance/index" />
<link rel="alternate" hreflang="zh-hant" href="https://access.redhat.com/documentation/zh-tw/openshift_container_platform/4.13/html-single/scalability_and_performance/index" />
<link rel="alternate" hreflang="id" href="https://access.redhat.com/documentation/id-id/openshift_container_platform/4.13/html-single/scalability_and_performance/index" />
<link rel="alternate" hreflang="th" href="https://access.redhat.com/documentation/th-th/openshift_container_platform/4.13/html-single/scalability_and_performance/index" />
<link rel="alternate" hreflang="vi" href="https://access.redhat.com/documentation/vi-vn/openshift_container_platform/4.13/html-single/scalability_and_performance/index" />
<meta name="Generator" content="Drupal 9 (https://www.drupal.org)" />
<meta name="MobileOptimized" content="width" />
<meta name="HandheldFriendly" content="true" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="revision" product="b0738f19-59ac-47eb-9512-8a439cd6dfb0" title="6c79da75-ea8e-4f76-b804-d0be249fc73b" page="cdf38755-bcce-41db-aa4e-2ceea58db6c2" revision="dccd48b1f252cc22dad43e9ae6252b9287d98ca1:en-us" body="d13faa4a2e944ab32732c4896a650294.html" toc="767aa0b9af0323a81ff2d39d87ba3952.json" />

    <title>Scalability and performance OpenShift Container Platform 4.13 | Red Hat Customer Portal</title>
    <link rel="stylesheet" media="all" href="/sites/dxp-docs/files/css/css_87GMcmxT1ib8ziQiU2KUAnTDFtZQV6iP-KGslA9LigM.css" />
<link rel="stylesheet" media="all" href="/sites/dxp-docs/files/css/css__Xq4GfgPDJw9K_yYJFmlRZGJeCENu3R3r4s0K7Tr_9g.css" />

    
    <script type="application/json" data-drupal-selector="drupal-settings-json">{"path":{"baseUrl":"\/","scriptPath":null,"pathPrefix":"","currentPath":"documentation\/en-us\/openshift_container_platform\/4.13\/html-single\/scalability_and_performance\/index","currentPathIsAdmin":false,"isFront":false,"currentLanguage":"en"},"pluralDelimiter":"\u0003","suppressDeprecationErrors":true,"red_hat_jwt":{"client_id":"customer-portal","cookie_name":"rh_jwt","leeway":"0","realm":"redhat-external","sso_host":"https:\/\/sso.redhat.com\/","user_integration":1,"user_plugin":"drupal_user_auth","use_external_js":0,"use_internal_js":0,"use_in_admin":0},"user":{"uid":0,"permissionsHash":"d8ea0bce2d740dacbdfe0257cf55baa0e33f7fb8468a26d055ce75daaaa2d315"}}</script>
<script src="/sites/dxp-docs/files/js/js_EQWKo9EokWkWS99x_e1oM-NEM0zlKyTkp_83mGdm5Ks.js"></script>

    <!-- CP_PRIMER_HEAD -->  <!-- TrustArc & DTM -->
  <script src="//static.redhat.com/libs/redhat/marketing/latest/trustarc/trustarc.js"></script>
  <script src="//www.redhat.com/dtm.js"></script><meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<!--[if IEMobile]><meta http-equiv="cleartype" content="on"><![endif]-->

<!-- metaInclude -->
<meta name="avalon-host-info" content="dxp-kbase-prod-139-77b4fb8768-25dr9" />
<meta name="avalon-version" content="27861f77" />
<meta name="cp-chrome-build-date" content="2023-10-06T19:17:59.039Z" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<!-- Chrome, Firefox OS and Opera -->
<meta name="theme-color" content="#000000">
<!-- Windows Phone -->
<meta name="msapplication-navbutton-color" content="#000000">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-status-bar-style" content="#000000">
<link rel="manifest" href="https://access.redhat.com/webassets/avalon/j/manifest.json">
<!-- Open Search - Tap to search -->
<link rel="search" type="application/opensearchdescription+xml" title="Red Hat Customer Portal" href="https://access.redhat.com/webassets/avalon/j/opensearch.xml" />
<!-- title -->
<title>Red Hat Customer Portal - Access to 24x7 support and knowledge</title>
<!-- /title -->
<script type="text/javascript">
    window.portal = {
        analytics : {},
        host      : "https://access.redhat.com",
        idp_url   : "https://sso.redhat.com",
        lang      : "en", 
        version   : "27861f77",
        builddate : "2023-10-06T19:17:59.039Z",        fetchdate : "2023-10-10T17:45:08-0400",        nrid      : "NOLONGERSUPPORTED",
        nrlk      : "NOLONGERSUPPORTED"
    };
</script>
<script type="text/javascript">
    if (!/\/logout.*/.test(location.pathname) && portal.host === location.origin && document.cookie.indexOf('rh_sso_session') >= 0 && !(document.cookie.indexOf('rh_jwt') >= 0)) window.location = '/login?redirectTo=' + encodeURIComponent(window.location.href);
</script>
<!-- cssInclude -->

<link rel="shortcut icon" href="https://access.redhat.com/webassets/avalon/g/favicon.ico" /><link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/bootstrap.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/bootstrap-grid.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/main.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/components.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/pages.css?v=27861f77" />

<link href="https://access.redhat.com/webassets/avalon/s/chosen.css?v=27861f77" rel="stylesheet" type="text/css" />

<!--[if lte IE 9]>
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/ie.css" />
<![endif]--><noscript>
    <style type="text/css" media="screen"> .primary-nav { display: block; } </style>
</noscript>
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/webassets/avalon/j/public_modules/node_modules/@cpelements/pfe-navigation/dist/pfe-navigation--lightdom.min.css" />
<!-- /cssInclude -->
<script src="https://access.redhat.com/webassets/avalon/j/public_modules/node_modules/@cpelements/pfe-navigation/dist/ie-polyfills.js?v=27861f77"></script>

<script async>
  if (!HTMLScriptElement.supports || !HTMLScriptElement.supports('importmap')) {
    import("https://www.redhatstatic.com/dx/v1-alpha/es-module-shims@1.7.3.js");
  }
</script>
<script type="importmap">
{
  "imports": {
    "@patternfly/elements/" : "https://www.redhatstatic.com/dx/v1-alpha/@patternfly/elements@2.2.2/",
    "@rhds/elements/":"https://www.redhatstatic.com/dx/v1-alpha/@rhds/elements@1.1.0/elements/",
    "@rhds/elements/lib/":"https://www.redhatstatic.com/dx/v1-alpha/@rhds/elements@1.1.0/lib/",
    "@cpelements/elements/":"https://www.redhatstatic.com/dx/v1-alpha/@cpelements/elements@2.0.0-alpha.7/elements/"
  }
}
</script><script type="text/javascript" src="https://access.redhat.com/webassets/avalon/j/lib/require.js?v=27861f77" data-main="/webassets/avalon/j/"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<script src="https://access.redhat.com/chrome_themes/nimbus/js/ie8.js"></script>
<![endif]-->
<script type="text/javascript" src="https://access.redhat.com/chrome_themes/nimbus/js/new-nav.js?v=27861f77" ></script>
<!-- /CP_PRIMER_HEAD -->

  </head>
  <body>
    
      <div class="dialog-off-canvas-main-canvas" data-off-canvas-main-canvas>
      <!-- CP_PRIMER_HEADER -->
<div id="page-wrap" class="page-wrap">
    <div id="pers-top-page-wrap" class="top-page-wrap pers-loader-bg">

      <div id="hero-bg-top-left" class="summit-bg-shapes"></div>
      <div id="hero-bg-top-right" class="summit-bg-shapes"></div>

        <!--googleoff: all-->
        <header class="masthead" id="masthead">

            <a href="#pfe-navigation" id="global-skip-to-nav" class="skip-link visually-hidden">Skip to navigation</a>
            <a href="#cp-main" class="skip-link visually-hidden">Skip to main content</a>            <nav id="portal-utility-nav" class="utility-navigation utility-navigation--bar hidden-at-mobile" data-analytics-region="utility" aria-labelledby="nav__utility-nav--desktop">
                <h3 id="nav__utility-nav--desktop" class="element-invisible">Utilities
                </h3>
                <ul aria-labelledby="nav__utility-nav--desktop">
                    <li id="nav-subscription" data-portal-tour-1="1">
                        <a class="top-nav-subscriptions" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Subscription" href="https://access.redhat.com/management/" >Subscriptions
                        </a>
                    </li>
                    <li id="nav-downloads" data-portal-tour-1="2">
                        <a class="top-nav-downloads" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Downloads" href="https://access.redhat.com/downloads/" >Downloads
                        </a>
                    </li>
                    <li id="nav-containers">
                        <a class="top-nav-containers" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Containers" href="https://catalog.redhat.com/software/containers/explore/" >Containers
                        </a>
                    </li>
                    <li id="nav-support" data-portal-tour-1="3">
                        <a class="top-nav-support-cases" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Support Cases" href="https://access.redhat.com/support/cases/" >Support Cases
                        </a>
                    </li>
                </ul>
            </nav>

            <pfe-navigation id="pfe-navigation" data-analytics-region="mega menu">
                <div class="pfe-navigation__logo-wrapper" id="pfe-navigation__logo-wrapper">
                    <a href="https://access.redhat.com/" class="pfe-navigation__logo-link" data-analytics-text="logo" data-analytics-category="MM|logo">
                        <img class="pfe-navigation__logo-image" alt="Red Hat Customer Portal" src="https://access.redhat.com/chrome_themes/nimbus/img/red-hat-customer-portal.svg" />
                    </a>
                </div>

                <nav class="pfe-navigation" aria-label="Main Navigation" data-analytics-region="main nav">
                    <ul class="pfe-navigation__menu" id="pfe-navigation__menu">                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-subscription--mobile" data-portal-tour-1="1">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Subscription" href="https://access.redhat.com/management/" >Subscriptions
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-downloads--mobile" data-portal-tour-1="2">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Downloads" href="https://access.redhat.com/downloads/" >Downloads
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-containers--mobile">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Containers" href="https://catalog.redhat.com/software/containers/explore/" >Containers
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-support--mobile" data-portal-tour-1="3">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Support Cases" href="https://access.redhat.com/support/cases/" >Support Cases
                            </a>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a href="https://access.redhat.com/products/" class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Products and Services">Products &amp; Services
                            </a>
                            <div class="pfe-navigation__dropdown has-primary-detail">                                <div class="desktop-col-span-2 tablet-col-span-all">
                                    <h3>
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Products" href="https://access.redhat.com/products/">Products
                                        </a>
                                    </h3>
                                    <slot name="main-menu__dropdown--product__product-listing"></slot>
                                </div>                                <div>
                                    <h3 id="nav__products__support">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Support" href="https://access.redhat.com/support">Support
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__support">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Production Support" href="https://access.redhat.com/support/offerings/production/">Production Support
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Development Support" href="https://access.redhat.com/support/offerings/developer/">Development Support
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Product Life Cycles" href="https://access.redhat.com/product-life-cycles/">Product Life Cycles
                                                    </a></li>
                                    </ul>

                                    <h3 id="nav__products__services">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Services" href="https://www.redhat.com/en/services">Services
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__services">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Consulting" href="https://www.redhat.com/en/services/consulting">Consulting
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Technical Account Management" href="https://access.redhat.com/support/offerings/tam/">Technical Account Management
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Training and Certifications" href="https://www.redhat.com/en/services/training-and-certification">Training &amp; Certifications
                                                    </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__products__documentation">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Documentation" href="https://access.redhat.com/documentation">Documentation
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__documentation">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat Enterprise Linux" href="https://access.redhat.com/documentation/en/red_hat_enterprise_linux">Red Hat Enterprise Linux
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat JBoss Enterprise Application Platform" href="https://access.redhat.com/documentation/en/red_hat_jboss_enterprise_application_platform">Red Hat JBoss Enterprise Application Platform
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat OpenStack Platform" href="https://access.redhat.com/documentation/en/red_hat_openstack_platform">Red Hat OpenStack Platform
                                                    </a></li>
                                                    <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat OpenShift Container Platform" href="https://access.redhat.com/documentation/en/openshift_container_platform">Red Hat OpenShift Container Platform
                                                        </a></li>
                                    </ul>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="All Documentation" data-analytics-linkType="cta" href="https://access.redhat.com/documentation">All Documentation
                                        </a>
                                    </pfe-cta>

                                    <h3 id="nav__products__catalog"><a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Ecosystem Catalog" href="https://catalog.redhat.com/">Ecosystem Catalog
                                        </a></h3>
                                        <ul aria-labelledby="nav__products__catalog">
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Ecosystem Catalog" data-analytics-text="Red Hat Partner Ecosystem" href="https://access.redhat.com/ecosystem/">Red Hat Partner Ecosystem
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Ecosystem Catalog" data-analytics-text="Partner Resources" href="https://access.redhat.com/ecosystem/partner-resources">Partner Resources
                                                    </a></li>
                                        </ul>
                                </div>
                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Tools" href="https://access.redhat.com/labs/">Tools
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="nav__tools__tools" data-analytics-level="2" data-analytics-text="Tools" data-analytics-category="Tools">Tools
                                    </h3>
                                    <ul aria-labelledby="nav__tools__tools">
                                        <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Solution Engine" href="https://access.redhat.com/support/cases/#/troubleshoot">Troubleshoot a product issue
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Packages" href="https://access.redhat.com/downloads/content/package-browser">Packages
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Errata" href="https://access.redhat.com/errata/">Errata
                                                    </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__tools__labs">
                                        <a data-analytics-level="2" data-analytics-category="Tools" data-analytics-text="Customer Portal Labs" href="https://access.redhat.com/labs/">Customer Portal Labs
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__tools__labs">
                                        <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Configuration" href="https://access.redhat.com/labs/#!?type=config">Configuration
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Deployment" href="https://access.redhat.com/labs/#!?type=deploy">Deployment
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Security" href="https://access.redhat.com/labs/#!?type=security">Security
                                                    </a></li>                                                    <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Troubleshooting" href="https://access.redhat.com/labs/#!?type=troubleshoot">Troubleshoot
                                                        </a></li>
                                    </ul>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="All Labs" data-analytics-linkType="cta" href="https://access.redhat.com/labs/">All labs
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h4 id="nav__tools__red-hat-insights">
                                        <a data-analytics-level="2" data-analytics-category="Tools" data-analytics-text="Red Hat Insights" href="//www.redhat.com/en/technologies/management/insights">Red Hat Insights
                                        </a>
                                    </h4>
                                    <p>Increase visibility into IT operations to detect and resolve technical issues before they impact your business.</p>
                                    <a data-analytics-level="3" data-analytics-category="Tools|Red Hat Insights" data-analytics-text="Learn more" href="https://www.redhat.com/en/technologies/management/insights">Learn More
                                    </a>
                                    <br>
                                    <a data-analytics-level="3" data-analytics-category="Tools|Red Hat Insights" data-analytics-text="Go to Insights" href="https://cloud.redhat.com/insights">Go to Insights
                                    </a>
                                </div>
                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Security" href="https://access.redhat.com/security/">Security
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="security__security-center">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Red Hat Product Security Center" href="https://access.redhat.com/security">Red Hat Product Security Center
                                        </a>
                                    </h3>
                                    <p>Engage with our Red Hat Product Security team, access security updates, and ensure your environments are not exposed to any known security vulnerabilities.
                                    </p>
                                    <pfe-cta pfe-priority="primary">
                                        <a data-analytics-level="3" data-analytics-category="Security|Red Hat Product Security Center" data-analytics-text="Product Security Center" data-analytics-linkType="cta" href="https://access.redhat.com/security/">Product Security Center
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__security__updates" data-analytics-level="2" data-analytics-text="Security Updates" data-analytics-category="Security">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Security Updates" href="/security">Security Updates
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__security__updates">
                                        <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Security Advisories" href="https://access.redhat.com/security/security-updates/#/security-advisories">Security Advisories
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Red Hat CVE Database" href="https://access.redhat.com/security/security-updates/#/cve">Red Hat CVE Database
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Security Labs" href="https://access.redhat.com/security/security-updates/#/security-labs">Security Labs
                                                    </a></li>
                                    </ul>
                                    <p class="margin-top-xl">Keep your systems secure with Red Hat&#039;s specialized responses to security vulnerabilities.
                                    </p>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="View Responses" data-analytics-linkType="cta" href="https://access.redhat.com/security/vulnerability">View Responses
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__security__resources">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Resources" href="https://access.redhat.com/security/overview">Resources
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__security__resources">                                            <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Security Blog" href="//redhat.com/en/blog/channel/security">Security Blog
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Security Measurement" href="https://www.redhat.com/security/data/metrics/">Security Measurement
                                                    </a></li>
                                                    <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Severity Ratings" href="https://access.redhat.com/security/updates/classification/">Severity Ratings
                                                        </a></li>
                                                        <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Backporting Policies" href="https://access.redhat.com/security/updates/backporting/">Backporting Policies
                                                            </a></li>
                                                            <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Product Signing (GPG) Keys" href="https://access.redhat.com/security/team/key/">Product Signing (GPG) Keys
                                                                </a></li>
                                    </ul>
                                </div>

                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a href="https://access.redhat.com/community/" class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Community">Community
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="nav__community__cp-community">
                                        <a href="https://access.redhat.com/community" data-analytics-level="2" data-analytics-text="Customer Portal Community" data-analytics-text="Customer Portal Community" data-analytics-category="Community">Customer Portal Community
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__community__cp-community">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Discussions" href="https://access.redhat.com/discussions">Discussions
                                            </a></li>                                                <li><a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Private Groups" href="https://access.redhat.com/groups/">Private Groups
                                                    </a></li>
                                    </ul>

                                    <pfe-cta pfe-priority="primary">
                                        <a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Community Activity" data-analytics-linkType="cta" href="https://access.redhat.com/community/">Community Activity
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__community__events" data-analytics-level="2" data-analytics-text="Customer Events" data-analytics-category="Community">Customer Events
                                    </h3>
                                    <ul aria-labelledby="nav__community__events">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Customer Events" data-analytics-text="Red Hat Convergence" href="https://access.redhat.com/convergence/">Red Hat Convergence
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Community|Customer Events" data-analytics-text="Red Hat Summit" href="http://www.redhat.com/summit/">Red Hat Summit
                                                </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__community__stories" data-analytics-level="2" data-analytics-text="Stories" data-analytics-category="Community">Stories
                                    </h3>
                                    <ul aria-labelledby="nav__community__stories">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Stories" data-analytics-text="Red Hat Subscription Value" href="https://access.redhat.com/subscription-value/">Red Hat Subscription Value
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-text="You Asked. We Acted." data-analytics-category="Community|Stories" href="https://access.redhat.com/you-asked-we-acted/">You Asked. We Acted.
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Community|Stories" data-analytics-text="Open Source Communities" href="http://www.redhat.com/en/open-source">Open Source Communities
                                                    </a></li>
                                    </ul>
                                </div>
                            </div>
                        </li>
                    </ul>                </nav>                <div id="site-search" slot="search" class="utility-link site-search">
                    <div class="content">
                        <form class="ng-pristine ng-valid topSearchForm" id="topSearchForm" name="topSearchForm" action="/search/browse/search/" method="get" enctype="application/x-www-form-urlencoded">
                            <cp-search-autocomplete class="push-bottom" path="/webassets/avalon/j/data.json"></cp-search-autocomplete>                            <div>Or <a href="/support/cases/#/troubleshoot">troubleshoot an issue</a>.
                            </div>
                        </form>
                    </div>
                </div>


                <div slot="secondary-links" id="localesMenu">
                    <button class="pfe-navigation__secondary-link">
                        <pfe-icon icon="web-icon-globe" size="sm" aria-hidden="true"></pfe-icon>English
                    </button>

                    <pfe-navigation-dropdown dropdown-width="single">
                        <h2 class="utility-header">Select Your Language
                        </h2>
                        <ul class="reset">
                            <li><a href="https://access.redhat.com/changeLanguage?language=en" data-lang="en" id="en" data-analytics-text="English">English</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=ko" data-lang="ko" id="ko" data-analytics-text="Korean">한국어</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=ja"    data-lang="ja"    id="ja" data-analytics-text="Japanese">日本語</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=zh_CN" data-lang="zh_CN" id="zh_CN" data-analytics-text="Chinese">中文 (中国)</a></li>
                        </ul>

                    </pfe-navigation-dropdown>
                </div>                <rh-account-dropdown slot="account"></rh-account-dropdown>                <pfe-primary-detail breakpoint-width="600" class="main-menu__dropdown--product__product-listing" slot="main-menu__dropdown--product__product-listing" consistent-height>
                    <h3 slot="details-nav">Infrastructure and Management                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Enterprise Linux" href="https://access.redhat.com/products/red-hat-enterprise-linux/">Red Hat Enterprise Linux
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Satellite" href="https://access.redhat.com/products/red-hat-satellite/">Red Hat Satellite
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Subscription Management" href="https://access.redhat.com/products/red-hat-subscription-management/">Red Hat Subscription Management
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Insights" href="https://access.redhat.com/products/red-hat-insights/">Red Hat Insights
                                </a>
                            </li>
                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Ansible Automation Platform" href="https://access.redhat.com/products/red-hat-ansible-automation-platform/">Red Hat Ansible Automation Platform
                                </a></li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Cloud Computing                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift" href="https://access.redhat.com/products/openshift">Red Hat OpenShift
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenStack Platform" href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat OpenShift Container Platform" href="https://access.redhat.com/products/red-hat-openshift-container-platform/">Red Hat OpenShift Container Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat OpenShift Data Science" href="https://access.redhat.com/products/red-hat-openshift-data-science/">Red Hat OpenShift Data Science
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift Dedicated" href="https://access.redhat.com/products/openshift-dedicated-red-hat/">Red Hat OpenShift Dedicated
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat Advanced Cluster Security for Kubernetes" href="https://access.redhat.com/products/red-hat-advanced-cluster-security-for-kubernetes/">Red Hat Advanced Cluster Security for Kubernetes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat Advanced Cluster Management for Kubernetes" href="https://access.redhat.com/products/red-hat-advanced-cluster-management-for-kubernetes/">Red Hat Advanced Cluster Management for Kubernetes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat Quay" href="https://access.redhat.com/products/red-hat-quay/">Red Hat Quay
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat CodeReady Workspaces" href="https://access.redhat.com/products/red-hat-codeready-workspaces/">OpenShift Dev Spaces
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift Service on AWS" href="https://access.redhat.com/products/red-hat-openshift-service-aws">Red Hat OpenShift Service on AWS
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Storage                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Gluster Storage" href="https://access.redhat.com/products/red-hat-storage/">Red Hat Gluster Storage
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Hyperconverged Infrastructure" href="https://access.redhat.com/products/red-hat-hyperconverged-infrastructure/">Red Hat Hyperconverged Infrastructure
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Ceph Storage" href="https://access.redhat.com/products/red-hat-ceph-storage/">Red Hat Ceph Storage
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Openshift Container Storage" href="https://access.redhat.com/products/red-hat-openshift-data-foundation">Red Hat OpenShift Data Foundation
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Runtimes                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Runtimes" href="https://access.redhat.com/products/red-hat-runtimes/">Red Hat Runtimes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat JBoss Enterprise Application Platform" href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/">Red Hat JBoss Enterprise Application Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Data Grid" href="https://access.redhat.com/products/red-hat-data-grid/">Red Hat Data Grid
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat JBoss Web Server" href="https://access.redhat.com/products/red-hat-jboss-web-server/">Red Hat JBoss Web Server
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Single Sign On" href="https://access.redhat.com/products/red-hat-single-sign-on/">Red Hat Single Sign On
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat support for Spring Boot" href="https://access.redhat.com/products/spring-boot/">Red Hat support for Spring Boot
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat build of Node.js" href="https://access.redhat.com/products/nodejs/">Red Hat build of Node.js
                                </a>
                            </li>                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat build of Quarkus" href="https://access.redhat.com/products/quarkus/">Red Hat build of Quarkus
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Integration and Automation                    </h3>
                    <div slot="details">
                        <ul class="border-bottom" id="portal-menu-border-bottom">
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat Application Foundations" href="https://access.redhat.com/products/red-hat-application-foundations/">Red Hat Application Foundations
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat Fuse" href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat AMQ" href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat 3scale API Management" href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management
                                </a>
                            </li>
                        </ul>
                    </div>
                    <div slot="details-nav--footer">
                        <pfe-cta pfe-priority="primary">
                            <a href="https://access.redhat.com/products/" class="pfe-navigation__menu-link" data-analytics-level="2" data-analytics-text="All Products" data-analytics-category="Products and Services|Products:" data-analytics-linkType="cta">All Products
                            </a>
                        </pfe-cta>
                    </div>
                </pfe-primary-detail>

            </pfe-navigation>

            <div id="scroll-anchor"></div>

            <!--[if IE 8]>
                <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                <span class="icon-warning alert-icon" aria-hidden="true"></span>
                You are using an unsupported web browser. Update to a supported browser for the best experience. <a href="/announcements/2120951">Read the announcement</a>.
                </div>
                </div>
            <![endif]-->
            <!--[if IE 9]>
                <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                <span class="icon-warning alert-icon" aria-hidden="true"></span>As of March 1, 2016, the Red Hat Customer Portal will no longer support Internet Explorer 9. See our new <a href="/help/browsers">browser support policy</a> for more information.
                </div>
                </div>
            <![endif]-->
            <div id="site-section"></div>
        </header>
        <!--googleon: all-->

        <main id="cp-main" class="portal-content-area">
            <div id="cp-content" class="main-content">                            <!-- /CP_PRIMER_HEADER -->

      <div class="container">
        

                                                                                                        <script>breadcrumbs = [["Products & Services","\/products\/"],["Product Documentation","\/documentation"],["OpenShift Container Platform","\/documentation\/en-us\/openshift_container_platform"],["4.13","\/documentation\/en-us\/openshift_container_platform\/4.13"],["Scalability and performance","\/documentation\/en-us\/openshift_container_platform\/4.13\/html\/scalability_and_performance"],["Scalability and performance","\/documentation\/en-us\/openshift_container_platform\/4.13\/html\/scalability_and_performance\/--single-page-document--"]]</script>

<div data-drupal-messages-fallback class="hidden"></div>


    </div>
        <div class="container">
        

  

  


  <article class="pvof-doc__content-wrapper__outer pvof-doc__content-wrapper__outer--css-not-removed">
    <script>
      'use strict';

            var $outerWrapper = document.querySelector('.pvof-doc__content-wrapper__outer');
      if ($outerWrapper && $outerWrapper.closest) {
        var $containerWrapper = $outerWrapper.closest('.container');
        if ($containerWrapper) {
          $containerWrapper.classList.remove('container');
          $containerWrapper.classList.add('j-chrome-content-container');
        }
      }

            var cssRemoved = false;
      try {
        var $crapCss = document.querySelectorAll(
          'link[href*="/chrome_themes/nimbus/css/pages.css"], link[href*="/chrome_themes/nimbus/css/components.css"]'
        );
        if ($crapCss.length) {
          for (let index = 0; index < $crapCss.length; index++) {
            const $stylesheet = $crapCss[index];
            $stylesheet.remove();
          }
        }
        cssRemoved = true;
      }
      catch (error) {
        console.error('Ran into an issue while trying to retheme page', error);
        cssRemoved = false;
      }

            if (cssRemoved) {
        var $pvofOuterWrapper = document.querySelector('.pvof-doc__content-wrapper__outer--css-not-removed');
        if ($pvofOuterWrapper) {
          $pvofOuterWrapper.classList.remove('pvof-doc__content-wrapper__outer--css-not-removed');
        }
      }
    </script>
    <div itemscope="" itemtype="https://schema.org/TechArticle" itemref="techArticle-md1 techArticle-md2 techArticle-md3"></div>
    <div itemscope="" itemtype="https://schema.org/SoftwareApplication" itemref="softwareApplication-md1 softwareApplication-md2 softwareApplication-md3 softwareApplication-md4"></div>
    <div class="pvof-doc__content-wrapper pvof-doc__content-wrapper--has-sidebar">
                                <div class="pvof-doc__content-wrapper__inner j-superdoc j-superdoc--has-nav">
                            <div class="pvof-sidebar__wrapper j-doc-nav j-superdoc__nav">
            <div class="j-sidebar__menu-container">
              <button class="j-sidebar__menu-trigger content-expander__trigger">
                <span class="j-sidebar__menu-trigger__open-text">Jump To</span>
                <span class="j-sidebar__menu-trigger__close-text">Close</span>
              </button>

              <div class="pvof-sidebar__inner-wrapper j-doc-nav__wrapper content-expander">
                <div class="j-sidebar__menu-details-container">
                  <button class="j-sidebar__menu-details-button j-sidebar__menu-details-button--expand">
                    Expand all
                  </button>
                  <button class="j-sidebar__menu-details-button j-sidebar__menu-details-button--collapse">
                    Collapse all
                  </button>
                </div>
                

  <nav id="pvof-doc__toc" class="pvof-doc__toc">
  <h2 class="j-doc-nav__title" id="j-doc-nav__title">
    Table of contents
  </h2>
  <div class="pvof-doc__toc-inner">
              <ol class="j-doc-nav__list" aria-labelledby="j-doc-nav__title">
                  <li class="j-doc-nav__list-item">
                                    

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance" class="j-doc-nav__link ">
    Scalability and performance
  </a>
  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#recommended-performance-and-scalability-practices" class="j-doc-nav__link j-doc-nav__link--has-children">
    1. Recommended performance and scalability practices
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "1. Recommended performance and scalability practices"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "1. Recommended performance and scalability practices"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#recommended-control-plane-practices" class="j-doc-nav__link j-doc-nav__link--has-children">
    1.1. Recommended control plane practices
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "1.1. Recommended control plane practices"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "1.1. Recommended control plane practices"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#recommended-scale-practices_recommended-control-plane-practices" class="j-doc-nav__link ">
    1.1.1. Recommended practices for scaling the cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#master-node-sizing_recommended-control-plane-practices" class="j-doc-nav__link j-doc-nav__link--has-children">
    1.1.2. Control plane node sizing
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "1.1.2. Control plane node sizing"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "1.1.2. Control plane node sizing"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#increasing-aws-flavor-size_recommended-control-plane-practices" class="j-doc-nav__link j-doc-nav__link--has-children">
    1.1.2.1. Selecting a larger Amazon Web Services instance type for control plane machines
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "1.1.2.1. Selecting a larger Amazon Web Services instance type for control plane machines"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "1.1.2.1. Selecting a larger Amazon Web Services instance type for control plane machines"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cpms-changing-aws-instance-type_recommended-control-plane-practices" class="j-doc-nav__link ">
    1.1.2.1.1. Changing the Amazon Web Services instance type by using a control plane machine set
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#aws-console-changing-aws-instance-type_recommended-control-plane-practices" class="j-doc-nav__link ">
    1.1.2.1.2. Changing the Amazon Web Services instance type by using the AWS console
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#recommended-infrastructure-practices" class="j-doc-nav__link j-doc-nav__link--has-children">
    1.2. Recommended infrastructure practices
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "1.2. Recommended infrastructure practices"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "1.2. Recommended infrastructure practices"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#infrastructure-node-sizing_recommended-infrastructure-practices" class="j-doc-nav__link ">
    1.2.1. Infrastructure node sizing
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#scaling-cluster-monitoring-operator_recommended-infrastructure-practices" class="j-doc-nav__link ">
    1.2.2. Scaling the Cluster Monitoring Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#prometheus-database-storage-requirements_recommended-infrastructure-practices" class="j-doc-nav__link ">
    1.2.3. Prometheus database storage requirements
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#configuring-cluster-monitoring_recommended-infrastructure-practices" class="j-doc-nav__link ">
    1.2.4. Configuring cluster monitoring
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#additional-resources" class="j-doc-nav__link ">
    1.2.5. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#recommended-etcd-practices" class="j-doc-nav__link j-doc-nav__link--has-children">
    1.3. Recommended etcd practices
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "1.3. Recommended etcd practices"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "1.3. Recommended etcd practices"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#recommended-etcd-practices_recommended-etcd-practices" class="j-doc-nav__link ">
    1.3.1. Recommended etcd practices
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#move-etcd-different-disk_recommended-etcd-practices" class="j-doc-nav__link ">
    1.3.2. Moving etcd to a different disk
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#etcd-defrag_recommended-etcd-practices" class="j-doc-nav__link j-doc-nav__link--has-children">
    1.3.3. Defragmenting etcd data
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "1.3.3. Defragmenting etcd data"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "1.3.3. Defragmenting etcd data"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#automatic-defrag-etcd-data_recommended-etcd-practices" class="j-doc-nav__link ">
    1.3.3.1. Automatic defragmentation
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#manual-defrag-etcd-data_recommended-etcd-practices" class="j-doc-nav__link ">
    1.3.3.2. Manual defragmentation
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#planning-your-environment-according-to-object-maximums" class="j-doc-nav__link j-doc-nav__link--has-children">
    2. Planning your environment according to object maximums
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2. Planning your environment according to object maximums"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2. Planning your environment according to object maximums"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cluster-maximums-major-releases_object-limits" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.1. OpenShift Container Platform tested cluster maximums for major releases
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.1. OpenShift Container Platform tested cluster maximums for major releases"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.1. OpenShift Container Platform tested cluster maximums for major releases"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cluster-maximums-major-releases-example-scenario_object-limits" class="j-doc-nav__link ">
    2.1.1. Example scenario
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cluster-maximums-environment_object-limits" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.2. OpenShift Container Platform environment and configuration on which the cluster maximums are tested
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.2. OpenShift Container Platform environment and configuration on which the cluster maximums are tested"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.2. OpenShift Container Platform environment and configuration on which the cluster maximums are tested"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#aws-cloud-platform" class="j-doc-nav__link ">
    2.2.1. AWS cloud platform
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ibm-power-platform" class="j-doc-nav__link ">
    2.2.2. IBM Power platform
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ibm-z-platform" class="j-doc-nav__link ">
    2.2.3. IBM Z platform
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#how-to-plan-according-to-cluster-maximums_object-limits" class="j-doc-nav__link ">
    2.3. How to plan your environment according to tested cluster maximums
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#how-to-plan-according-to-application-requirements_object-limits" class="j-doc-nav__link ">
    2.4. How to plan your environment according to application requirements
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ibm-z-recommended-host-practices" class="j-doc-nav__link j-doc-nav__link--has-children">
    3. Recommended host practices for IBM Z &amp; IBM(R) LinuxONE environments
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3. Recommended host practices for IBM Z &amp; IBM(R) LinuxONE environments"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3. Recommended host practices for IBM Z &amp; IBM(R) LinuxONE environments"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ibm-z-managing-cpu-overcommitment_ibm-z-recommended-host-practices" class="j-doc-nav__link ">
    3.1. Managing CPU overcommitment
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ibm-z-disable-thp_ibm-z-recommended-host-practices" class="j-doc-nav__link ">
    3.2. Disable Transparent Huge Pages
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ibm-z-boost-networking-performance-with-rfs_ibm-z-recommended-host-practices" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.3. Boost networking performance with Receive Flow Steering
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.3. Boost networking performance with Receive Flow Steering"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.3. Boost networking performance with Receive Flow Steering"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#use-the-mco-to-activate-rfs_ibm-z-recommended-host-practices" class="j-doc-nav__link ">
    3.3.1. Use the Machine Config Operator (MCO) to activate RFS
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ibm-z-choose-networking-setup_ibm-z-recommended-host-practices" class="j-doc-nav__link ">
    3.4. Choose your networking setup
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ibm-z-ensure-high-disk-performance-hyperpav_ibm-z-recommended-host-practices" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.5. Ensure high disk performance with HyperPAV on z/VM
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.5. Ensure high disk performance with HyperPAV on z/VM"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.5. Ensure high disk performance with HyperPAV on z/VM"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#use-the-mco-to-activate-hyperpav-aliases-in-nodes-using-zvm-full-pack-minidisks_ibm-z-recommended-host-practices" class="j-doc-nav__link ">
    3.5.1. Use the Machine Config Operator (MCO) to activate HyperPAV aliases in nodes using z/VM full-pack minidisks
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ibm-z-rhel-kvm-host-recommendations_ibm-z-recommended-host-practices" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.6. RHEL KVM on IBM Z host recommendations
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.6. RHEL KVM on IBM Z host recommendations"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.6. RHEL KVM on IBM Z host recommendations"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#use-multiple-queues-for-your-virtio-network-interfaces_ibm-z-recommended-host-practices" class="j-doc-nav__link ">
    3.6.1. Use multiple queues for your VirtIO network interfaces
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#use-io-threads-for-your-virtual-block-devices_ibm-z-recommended-host-practices" class="j-doc-nav__link ">
    3.6.2. Use I/O threads for your virtual block devices
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#avoid-virtual-scsi-devices_ibm-z-recommended-host-practices" class="j-doc-nav__link ">
    3.6.3. Avoid virtual SCSI devices
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#configure-guest-caching-for-disk_ibm-z-recommended-host-practices" class="j-doc-nav__link ">
    3.6.4. Configure guest caching for disk
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#exclude-the-memory-ballon-device_ibm-z-recommended-host-practices" class="j-doc-nav__link ">
    3.6.5. Exclude the memory balloon device
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#tune-the-cpu-migration-algorithm-of-the-host-scheduler_ibm-z-recommended-host-practices" class="j-doc-nav__link ">
    3.6.6. Tune the CPU migration algorithm of the host scheduler
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#disable-the-cpuset-cgroup-controller_ibm-z-recommended-host-practices" class="j-doc-nav__link ">
    3.6.7. Disable the cpuset cgroup controller
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#tune-the-polling-period-for-idle-virtual-cpus_ibm-z-recommended-host-practices" class="j-doc-nav__link ">
    3.6.8. Tune the polling period for idle virtual CPUs
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#using-node-tuning-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    4. Using the Node Tuning Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4. Using the Node Tuning Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4. Using the Node Tuning Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#about-node-tuning-operator_node-tuning-operator" class="j-doc-nav__link ">
    4.1. About the Node Tuning Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#accessing-an-example-node-tuning-operator-specification_node-tuning-operator" class="j-doc-nav__link ">
    4.2. Accessing an example Node Tuning Operator specification
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#custom-tuning-default-profiles-set_node-tuning-operator" class="j-doc-nav__link ">
    4.3. Default profiles set on a cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#verifying-tuned-profiles-are-applied_node-tuning-operator" class="j-doc-nav__link ">
    4.4. Verifying that the TuneD profiles are applied
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#custom-tuning-specification_node-tuning-operator" class="j-doc-nav__link ">
    4.5. Custom tuning specification
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#custom-tuning-example_node-tuning-operator" class="j-doc-nav__link ">
    4.6. Custom tuning examples
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#supported-tuned-daemon-plug-ins_node-tuning-operator" class="j-doc-nav__link ">
    4.7. Supported TuneD daemon plugins
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#node-tuning-hosted-cluster_node-tuning-operator" class="j-doc-nav__link ">
    4.8. Configuring node tuning in a hosted cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#advanced-node-tuning-hosted-cluster_node-tuning-operator" class="j-doc-nav__link ">
    4.9. Advanced node tuning for hosted clusters by setting kernel boot parameters
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#using-cpu-manager" class="j-doc-nav__link j-doc-nav__link--has-children">
    5. Using CPU Manager and Topology Manager
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "5. Using CPU Manager and Topology Manager"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "5. Using CPU Manager and Topology Manager"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#seting_up_cpu_manager_using-cpu-manager-and-topology_manager" class="j-doc-nav__link ">
    5.1. Setting up CPU Manager
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#topology_manager_policies_using-cpu-manager-and-topology_manager" class="j-doc-nav__link ">
    5.2. Topology Manager policies
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#seting_up_topology_manager_using-cpu-manager-and-topology_manager" class="j-doc-nav__link ">
    5.3. Setting up Topology Manager
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#pod-interactions-with-topology-manager_using-cpu-manager-and-topology_manager" class="j-doc-nav__link ">
    5.4. Pod interactions with Topology Manager policies
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-numa-aware-scheduling" class="j-doc-nav__link j-doc-nav__link--has-children">
    6. Scheduling NUMA-aware workloads
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6. Scheduling NUMA-aware workloads"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6. Scheduling NUMA-aware workloads"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-about-numa-aware-scheduling_numa-aware" class="j-doc-nav__link ">
    6.1. About NUMA-aware scheduling
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#installing-the-numa-resources-operator_numa-aware" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.2. Installing the NUMA Resources Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.2. Installing the NUMA Resources Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.2. Installing the NUMA Resources Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-installing-numa-resources-operator-cli_numa-aware" class="j-doc-nav__link ">
    6.2.1. Installing the NUMA Resources Operator using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-installing-numa-resources-operator-console_numa-aware" class="j-doc-nav__link ">
    6.2.2. Installing the NUMA Resources Operator using the web console
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-scheduling-numa-aware-workloads-overview_numa-aware" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.3. Scheduling NUMA-aware workloads
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.3. Scheduling NUMA-aware workloads"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.3. Scheduling NUMA-aware workloads"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-creating-nrop-cr_numa-aware" class="j-doc-nav__link ">
    6.3.1. Creating the NUMAResourcesOperator custom resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-deploying-the-numa-aware-scheduler_numa-aware" class="j-doc-nav__link ">
    6.3.2. Deploying the NUMA-aware secondary pod scheduler
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-scheduling-numa-aware-workloads_numa-aware" class="j-doc-nav__link ">
    6.3.3. Scheduling workloads with the NUMA-aware scheduler
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-scheduling-numa-aware-workloads-with-manual-perofrmance-settings_numa-aware" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.4. Scheduling NUMA-aware workloads with manual performance settings
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.4. Scheduling NUMA-aware workloads with manual performance settings"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.4. Scheduling NUMA-aware workloads with manual performance settings"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-creating-nrop-cr-with-manual-performance-settings_numa-aware" class="j-doc-nav__link ">
    6.4.1. Creating the NUMAResourcesOperator custom resource with manual performance settings
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-deploying-the-numa-aware-scheduler-with-manual-performance-settings_numa-aware" class="j-doc-nav__link ">
    6.4.2. Deploying the NUMA-aware secondary pod scheduler with manual performance settings
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-scheduling-numa-aware-workloads-with-manual-performance-setttings_numa-aware" class="j-doc-nav__link ">
    6.4.3. Scheduling workloads with the NUMA-aware scheduler with manual performance settings
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-configuring-node-groups-for-the-numaresourcesoperator_numa-aware" class="j-doc-nav__link ">
    6.5. Optional: Configuring polling operations for NUMA resources updates
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-troubleshooting-numa-aware-workloads_numa-aware" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.6. Troubleshooting NUMA-aware scheduling
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.6. Troubleshooting NUMA-aware scheduling"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.6. Troubleshooting NUMA-aware scheduling"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-checking-numa-aware-scheduler-logs_numa-aware" class="j-doc-nav__link ">
    6.6.1. Checking the NUMA-aware scheduler logs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-troubleshooting-resource-topo-exporter_numa-aware" class="j-doc-nav__link ">
    6.6.2. Troubleshooting the resource topology exporter
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-troubleshooting-missing-rte-config-maps_numa-aware" class="j-doc-nav__link ">
    6.6.3. Correcting a missing resource topology exporter config map
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#scalability-and-performance-optimization" class="j-doc-nav__link j-doc-nav__link--has-children">
    7. Scalability and performance optimization
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7. Scalability and performance optimization"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7. Scalability and performance optimization"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#optimizing-storage" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.1. Optimizing storage
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.1. Optimizing storage"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.1. Optimizing storage"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#available-persistent-storage-options_persistent-storage" class="j-doc-nav__link ">
    7.1.1. Available persistent storage options
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#recommended-configurable-storage-technology_persistent-storage" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.1.2. Recommended configurable storage technology
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.1.2. Recommended configurable storage technology"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.1.2. Recommended configurable storage technology"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#specific-application-storage-recommendations" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.1.2.1. Specific application storage recommendations
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.1.2.1. Specific application storage recommendations"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.1.2.1. Specific application storage recommendations"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#registry" class="j-doc-nav__link ">
    7.1.2.1.1. Registry
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#scaled-registry" class="j-doc-nav__link ">
    7.1.2.1.2. Scaled registry
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#metrics" class="j-doc-nav__link ">
    7.1.2.1.3. Metrics
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#logging" class="j-doc-nav__link ">
    7.1.2.1.4. Logging
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#applications" class="j-doc-nav__link ">
    7.1.2.1.5. Applications
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#other-specific-application-storage-recommendations" class="j-doc-nav__link ">
    7.1.2.2. Other specific application storage recommendations
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#data-storage-management_persistent-storage" class="j-doc-nav__link ">
    7.1.3. Data storage management
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#optimizing-storage-azure_persistent-storage" class="j-doc-nav__link ">
    7.1.4. Optimizing storage performance for Microsoft Azure
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#admission-plug-ins-additional-resources" class="j-doc-nav__link ">
    7.1.5. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#routing-optimization" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.2. Optimizing routing
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.2. Optimizing routing"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.2. Optimizing routing"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#baseline-router-performance_routing-optimization" class="j-doc-nav__link ">
    7.2.1. Baseline Ingress Controller (router) performance
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ingress-liveness-readiness-startup-probes_routing-optimization" class="j-doc-nav__link ">
    7.2.2. Configuring Ingress Controller liveness, readiness, and startup probes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#configuring-haproxy-interval_routing-optimization" class="j-doc-nav__link ">
    7.2.3. Configuring HAProxy reload interval
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#optimizing-networking" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.3. Optimizing networking
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.3. Optimizing networking"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.3. Optimizing networking"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#optimizing-mtu_optimizing-networking" class="j-doc-nav__link ">
    7.3.1. Optimizing the MTU for your network
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#recommended-install-practices_optimizing-networking" class="j-doc-nav__link ">
    7.3.2. Recommended practices for installing large scale clusters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ipsec-impact_optimizing-networking" class="j-doc-nav__link ">
    7.3.3. Impact of IPsec
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#optimizing-networking-additional-resources" class="j-doc-nav__link ">
    7.3.4. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#optimizing-cpu-usage" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.4. Optimizing CPU usage with mount namespace encapsulation
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.4. Optimizing CPU usage with mount namespace encapsulation"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.4. Optimizing CPU usage with mount namespace encapsulation"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#optimizing-cpu-usage_optimizing-cpu-usage" class="j-doc-nav__link ">
    7.4.1. Encapsulating mount namespaces
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#enabling-encapsulation_optimizing-cpu-usage" class="j-doc-nav__link ">
    7.4.2. Configuring mount namespace encapsulation
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#supporting-encapsulation_optimizing-cpu-usage" class="j-doc-nav__link ">
    7.4.3. Inspecting encapsulated namespaces
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#running-services-with-encapsulation_optimizing-cpu-usage" class="j-doc-nav__link ">
    7.4.4. Running additional services in the encapsulated namespace
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#optimizing-cpu-usage-additional-resources" class="j-doc-nav__link ">
    7.4.5. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#managing-bare-metal-hosts" class="j-doc-nav__link j-doc-nav__link--has-children">
    8. Managing bare metal hosts
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8. Managing bare metal hosts"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8. Managing bare metal hosts"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#about-bare-metal-hosts-and-nodes_managing-bare-metal-hosts" class="j-doc-nav__link ">
    8.1. About bare metal hosts and nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#maintaining-bare-metal-hosts_managing-bare-metal-hosts" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.2. Maintaining bare metal hosts
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.2. Maintaining bare metal hosts"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.2. Maintaining bare metal hosts"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#adding-bare-metal-host-to-cluster-using-web-console_managing-bare-metal-hosts" class="j-doc-nav__link ">
    8.2.1. Adding a bare metal host to the cluster using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#adding-bare-metal-host-to-cluster-using-yaml_managing-bare-metal-hosts" class="j-doc-nav__link ">
    8.2.2. Adding a bare metal host to the cluster using YAML in the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#automatically-scaling-machines-to-available-bare-metal-hosts_managing-bare-metal-hosts" class="j-doc-nav__link ">
    8.2.3. Automatically scaling machines to the number of available bare metal hosts
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#removing-bare-metal-hosts-from-provisioner_managing-bare-metal-hosts" class="j-doc-nav__link ">
    8.2.4. Removing bare metal hosts from the provisioner node
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#what-huge-pages-do-and-how-they-are-consumed" class="j-doc-nav__link j-doc-nav__link--has-children">
    9. What huge pages do and how they are consumed by applications
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9. What huge pages do and how they are consumed by applications"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9. What huge pages do and how they are consumed by applications"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#what-huge-pages-do_huge-pages" class="j-doc-nav__link ">
    9.1. What huge pages do
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#how-huge-pages-are-consumed-by-apps_huge-pages" class="j-doc-nav__link ">
    9.2. How huge pages are consumed by apps
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#consuming-huge-pages-resource-using-the-downward-api_huge-pages" class="j-doc-nav__link ">
    9.3. Consuming huge pages resources using the Downward API
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#configuring-huge-pages_huge-pages" class="j-doc-nav__link ">
    9.4. Configuring huge pages at boot time
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#disable-thp_huge-pages" class="j-doc-nav__link ">
    9.5. Disabling Transparent Huge Pages
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-low-latency-tuning" class="j-doc-nav__link j-doc-nav__link--has-children">
    10. Low latency tuning
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10. Low latency tuning"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10. Low latency tuning"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-understanding-low-latency_cnf-master" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.1. Understanding low latency
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.1. Understanding low latency"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.1. Understanding low latency"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#about_hyperthreading_for_low_latency_and_real_time_applications_cnf-master" class="j-doc-nav__link ">
    10.1.1. About hyperthreading for low latency and real-time applications
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-provisioning-real-time-and-low-latency-workloads_cnf-master" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.2. Provisioning real-time and low latency workloads
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.2. Provisioning real-time and low latency workloads"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.2. Provisioning real-time and low latency workloads"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#node-tuning-operator-known-limitations-for-real-time_cnf-master" class="j-doc-nav__link ">
    10.2.1. Known limitations for real-time
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#node-tuning-operator-provisioning-worker-with-real-time-capabilities_cnf-master" class="j-doc-nav__link ">
    10.2.2. Provisioning a worker with real-time capabilities
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#node-tuning-operator-verifying-real-time-kernel-installation_cnf-master" class="j-doc-nav__link ">
    10.2.3. Verifying the real-time kernel installation
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#node-tuning-operator-creating-workload-that-works-in-real-time_cnf-master" class="j-doc-nav__link ">
    10.2.4. Creating a workload that works in real-time
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#node-tuning-operator-creating-pod-with-guaranteed-qos-class_cnf-master" class="j-doc-nav__link ">
    10.2.5. Creating a pod with a QoS class of Guaranteed
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#node-tuning-operator-disabling-cpu-load-balancing-for-dpdk_cnf-master" class="j-doc-nav__link ">
    10.2.6. Optional: Disabling CPU load balancing for DPDK
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#node-tuning-operator-assigning-proper-node-selector_cnf-master" class="j-doc-nav__link ">
    10.2.7. Assigning a proper node selector
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#node-tuning-operator-scheduling-workload-onto-worker-with-real-time-capabilities_cnf-master" class="j-doc-nav__link ">
    10.2.8. Scheduling a workload onto a worker with real-time capabilities
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#node-tuning-operator-disabling-CPUs-for-power-consumption_cnf-master" class="j-doc-nav__link ">
    10.2.9. Reducing power consumption by taking CPUs offline
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#node-tuning-operator-pod-power-saving-config_cnf-master" class="j-doc-nav__link ">
    10.2.10. Optional: Power saving configurations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#managing-device-interrupt-processing-for-guaranteed-pod-isolated-cpus_cnf-master" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.2.11. Managing device interrupt processing for guaranteed pod isolated CPUs
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.2.11. Managing device interrupt processing for guaranteed pod isolated CPUs"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.2.11. Managing device interrupt processing for guaranteed pod isolated CPUs"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#disabling-cpu-cfs-quota_cnf-master" class="j-doc-nav__link ">
    10.2.11.1. Disabling CPU CFS quota
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#configuring-global-device-interrupts-handling-for-isolated-cpus_cnf-master" class="j-doc-nav__link ">
    10.2.11.2. Disabling global device interrupts handling in Node Tuning Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#disabling_interrupt_processing_for_individual_pods_cnf-master" class="j-doc-nav__link ">
    10.2.11.3. Disabling interrupt processing for individual pods
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#use-device-interrupt-processing-for-isolated-cpus_cnf-master" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.2.12. Upgrading the performance profile to use device interrupt processing
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.2.12. Upgrading the performance profile to use device interrupt processing"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.2.12. Upgrading the performance profile to use device interrupt processing"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#nto_supported_api_versions_cnf-master" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.2.12.1. Supported API Versions
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.2.12.1. Supported API Versions"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.2.12.1. Supported API Versions"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#upgrading_nto_api_from_v1alpha1_to_v1_cnf-master" class="j-doc-nav__link ">
    10.2.12.1.1. Upgrading Node Tuning Operator API from v1alpha1 to v1
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#upgrading_nto_api_from_v1alpha1_to_v1_or_v2_cnf-master" class="j-doc-nav__link ">
    10.2.12.1.2. Upgrading Node Tuning Operator API from v1alpha1 or v1 to v2
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-tuning-nodes-for-low-latency-via-performanceprofile_cnf-master" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.3. Tuning nodes for low latency with the performance profile
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.3. Tuning nodes for low latency with the performance profile"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.3. Tuning nodes for low latency with the performance profile"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-configuring-huge-pages_cnf-master" class="j-doc-nav__link ">
    10.3.1. Configuring huge pages
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-allocating-multiple-huge-page-sizes_cnf-master" class="j-doc-nav__link ">
    10.3.2. Allocating multiple huge page sizes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#configuring_for_irq_dynamic_load_balancing_cnf-master" class="j-doc-nav__link ">
    10.3.3. Configuring a node for IRQ dynamic load balancing
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#about_irq_affinity_setting_cnf-master" class="j-doc-nav__link ">
    10.3.4. About support of IRQ affinity setting
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#configuring_hyperthreading_for_a_cluster_cnf-master" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.3.5. Configuring hyperthreading for a cluster
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.3.5. Configuring hyperthreading for a cluster"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.3.5. Configuring hyperthreading for a cluster"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#disabling_hyperthreading_for_low_latency_applications_cnf-master" class="j-doc-nav__link ">
    10.3.5.1. Disabling hyperthreading for low latency applications
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-understanding-workload-hints_cnf-master" class="j-doc-nav__link ">
    10.3.6. Understanding workload hints
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#configuring-workload-hints_cnf-master" class="j-doc-nav__link ">
    10.3.7. Configuring workload hints manually
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-cpu-infra-container_cnf-master" class="j-doc-nav__link ">
    10.3.8. Restricting CPUs for infra and application containers
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#reducing-nic-queues-using-the-node-tuning-operator_cnf-master" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.4. Reducing NIC queues using the Node Tuning Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.4. Reducing NIC queues using the Node Tuning Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.4. Reducing NIC queues using the Node Tuning Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#adjusting-nic-queues-with-the-performance-profile_cnf-master" class="j-doc-nav__link ">
    10.4.1. Adjusting the NIC queues with the performance profile
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#verifying-queue-status_cnf-master" class="j-doc-nav__link ">
    10.4.2. Verifying the queue status
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#logging-associated-with-adjusting-nic-queues_cnf-master" class="j-doc-nav__link ">
    10.4.3. Logging associated with adjusting NIC queues
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-debugging-low-latency-cnf-tuning-status_cnf-master" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.5. Debugging low latency CNF tuning status
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.5. Debugging low latency CNF tuning status"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.5. Debugging low latency CNF tuning status"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-debugging-low-latency-cnf-tuning-status-machineconfigpools_cnf-master" class="j-doc-nav__link ">
    10.5.1. Machine config pools
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-collecting-low-latency-tuning-debugging-data-for-red-hat-support_cnf-master" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.6. Collecting low latency tuning debugging data for Red Hat Support
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.6. Collecting low latency tuning debugging data for Red Hat Support"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.6. Collecting low latency tuning debugging data for Red Hat Support"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-about-must-gather_cnf-master" class="j-doc-nav__link ">
    10.6.1. About the must-gather tool
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-about-collecting-low-latency-data_cnf-master" class="j-doc-nav__link ">
    10.6.2. About collecting low latency tuning data
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-about-gathering-data_cnf-master" class="j-doc-nav__link ">
    10.6.3. Gathering data about specific features
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-performing-platform-verification-latency-tests" class="j-doc-nav__link j-doc-nav__link--has-children">
    11. Performing latency tests for platform verification
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11. Performing latency tests for platform verification"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11. Performing latency tests for platform verification"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-latency-tests-prerequisites_cnf-latency-tests" class="j-doc-nav__link ">
    11.1. Prerequisites for running latency tests
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#discovery-mode_cnf-latency-tests" class="j-doc-nav__link ">
    11.2. About discovery mode for latency tests
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-measuring-latency_cnf-latency-tests" class="j-doc-nav__link ">
    11.3. Measuring latency
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-performing-end-to-end-tests-running-the-tests_cnf-latency-tests" class="j-doc-nav__link j-doc-nav__link--has-children">
    11.4. Running the latency tests
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11.4. Running the latency tests"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11.4. Running the latency tests"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-performing-end-to-end-tests-running-hwlatdetect_cnf-latency-tests" class="j-doc-nav__link ">
    11.4.1. Running hwlatdetect
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-performing-end-to-end-tests-running-cyclictest_cnf-latency-tests" class="j-doc-nav__link ">
    11.4.2. Running cyclictest
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-performing-end-to-end-tests-running-oslat_cnf-latency-tests" class="j-doc-nav__link ">
    11.4.3. Running oslat
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-performing-end-to-end-tests-test-failure-report_cnf-latency-tests" class="j-doc-nav__link ">
    11.5. Generating a latency test failure report
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-performing-end-to-end-tests-junit-test-output_cnf-latency-tests" class="j-doc-nav__link ">
    11.6. Generating a JUnit latency test report
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-performing-end-to-end-tests-running-in-single-node-cluster_cnf-latency-tests" class="j-doc-nav__link ">
    11.7. Running latency tests on a single-node OpenShift cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-performing-end-to-end-tests-disconnected-mode_cnf-latency-tests" class="j-doc-nav__link ">
    11.8. Running latency tests in a disconnected cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-performing-end-to-end-tests-troubleshooting_cnf-latency-tests" class="j-doc-nav__link ">
    11.9. Troubleshooting errors with the cnf-tests container
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#scaling-worker-latency-profiles" class="j-doc-nav__link j-doc-nav__link--has-children">
    12. Improving cluster stability in high latency environments using worker latency profiles
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12. Improving cluster stability in high latency environments using worker latency profiles"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12. Improving cluster stability in high latency environments using worker latency profiles"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#nodes-cluster-worker-latency-profiles-about_scaling-worker-latency-profiles" class="j-doc-nav__link ">
    12.1. Understanding worker latency profiles
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#nodes-cluster-worker-latency-profiles-using_scaling-worker-latency-profiles" class="j-doc-nav__link ">
    12.2. Using worker latency profiles
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-create-performance-profiles" class="j-doc-nav__link j-doc-nav__link--has-children">
    13. Creating a performance profile
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13. Creating a performance profile"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13. Creating a performance profile"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-about-the-profile-creator-tool_cnf-create-performance-profiles" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.1. About the Performance Profile Creator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.1. About the Performance Profile Creator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.1. About the Performance Profile Creator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#gathering-data-about-your-cluster-using-must-gather_cnf-create-performance-profiles" class="j-doc-nav__link ">
    13.1.1. Gathering data about your cluster using the must-gather command
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#running-the-performance-profile-profile-cluster-using-podman_cnf-create-performance-profiles" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.1.2. Running the Performance Profile Creator using podman
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.1.2. Running the Performance Profile Creator using podman"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.1.2. Running the Performance Profile Creator using podman"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#how-to-run-podman-to-create-a-profile_cnf-create-performance-profiles" class="j-doc-nav__link ">
    13.1.2.1. How to run podman to create a performance profile
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#running-the-performance-profile-creator-wrapper-script_cnf-create-performance-profiles" class="j-doc-nav__link ">
    13.1.3. Running the Performance Profile Creator wrapper script
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#performance-profile-creator-arguments_cnf-create-performance-profiles" class="j-doc-nav__link ">
    13.1.4. Performance Profile Creator arguments
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-create-performance-profiles-reference" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.2. Reference performance profiles
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.2. Reference performance profiles"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.2. Reference performance profiles"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#installation-openstack-ovs-dpdk-performance-profile_cnf-create-performance-profiles" class="j-doc-nav__link ">
    13.2.1. A performance profile template for clusters that use OVS-DPDK on OpenStack
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-create-performance-profiles-additional-resources" class="j-doc-nav__link ">
    13.3. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                    

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#enabling-workload-partitioning" class="j-doc-nav__link ">
    14. Workload partitioning
  </a>
  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#using-node-observability-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    15. Requesting CRI-O and Kubelet profiling data by using the Node Observability Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "15. Requesting CRI-O and Kubelet profiling data by using the Node Observability Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "15. Requesting CRI-O and Kubelet profiling data by using the Node Observability Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#workflow-node-observability-operator_node-observability-operator" class="j-doc-nav__link ">
    15.1. Workflow of the Node Observability Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#install-node-observability-operator_node-observability-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    15.2. Installing the Node Observability Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "15.2. Installing the Node Observability Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "15.2. Installing the Node Observability Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#install-node-observability-using-cli_node-observability-operator" class="j-doc-nav__link ">
    15.2.1. Installing the Node Observability Operator using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#install-node-observability-using-web-console_node-observability-operator" class="j-doc-nav__link ">
    15.2.2. Installing the Node Observability Operator using the web console
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#creating-node-observability-custom-resource_node-observability-operator" class="j-doc-nav__link ">
    15.3. Creating the Node Observability custom resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#running-profiling-query_node-observability-operator" class="j-doc-nav__link ">
    15.4. Running the profiling query
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#clusters-at-the-network-far-edge" class="j-doc-nav__link j-doc-nav__link--has-children">
    16. Clusters at the network far edge
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16. Clusters at the network far edge"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16. Clusters at the network far edge"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-deploying-far-edge-clusters-at-scale" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.1. Challenges of the network far edge
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.1. Challenges of the network far edge"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.1. Challenges of the network far edge"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-challenges-of-far-edge-deployments_ztp-deploying-far-edge-clusters-at-scale" class="j-doc-nav__link ">
    16.1.1. Overcoming the challenges of the network far edge
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#about-ztp_ztp-deploying-far-edge-clusters-at-scale" class="j-doc-nav__link ">
    16.1.2. Using GitOps ZTP to provision clusters at the network far edge
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-creating-ztp-crs-for-multiple-managed-clusters_ztp-deploying-far-edge-clusters-at-scale" class="j-doc-nav__link ">
    16.1.3. Installing managed clusters with SiteConfig resources and RHACM
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-configuring-cluster-policies_ztp-deploying-far-edge-clusters-at-scale" class="j-doc-nav__link ">
    16.1.4. Configuring managed clusters with policies and PolicyGenTemplate resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-preparing-the-hub-cluster" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.2. Preparing the hub cluster for ZTP
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.2. Preparing the hub cluster for ZTP"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.2. Preparing the hub cluster for ZTP"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-telco-ran-software-versions_ztp-preparing-the-hub-cluster" class="j-doc-nav__link ">
    16.2.1. Telco RAN 4.13 validated solution software versions
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-gitops-ztp-max-spoke-clusters_ztp-preparing-the-hub-cluster" class="j-doc-nav__link ">
    16.2.2. Recommended hub cluster specifications and managed cluster limits for GitOps ZTP
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#installing-disconnected-rhacm_ztp-preparing-the-hub-cluster" class="j-doc-nav__link ">
    16.2.3. Installing GitOps ZTP in a disconnected environment
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-acm-adding-images-to-mirror-registry_ztp-preparing-the-hub-cluster" class="j-doc-nav__link ">
    16.2.4. Adding RHCOS ISO and RootFS images to the disconnected mirror host
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#enabling-assisted-installer-service-on-bare-metal_ztp-preparing-the-hub-cluster" class="j-doc-nav__link ">
    16.2.5. Enabling the assisted service
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-configuring-the-cluster-for-a-disconnected-environment_ztp-preparing-the-hub-cluster" class="j-doc-nav__link ">
    16.2.6. Configuring the hub cluster to use a disconnected mirror registry
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-configuring-the-hub-cluster-to-use-unauthenticated-registries_ztp-preparing-the-hub-cluster" class="j-doc-nav__link ">
    16.2.7. Configuring the hub cluster to use unauthenticated registries
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-configuring-hub-cluster-with-argocd_ztp-preparing-the-hub-cluster" class="j-doc-nav__link ">
    16.2.8. Configuring the hub cluster with ArgoCD
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-preparing-the-ztp-git-repository_ztp-preparing-the-hub-cluster" class="j-doc-nav__link ">
    16.2.9. Preparing the GitOps ZTP site configuration repository
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-deploying-far-edge-sites" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.3. Installing managed clusters with RHACM and SiteConfig resources
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.3. Installing managed clusters with RHACM and SiteConfig resources"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.3. Installing managed clusters with RHACM and SiteConfig resources"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-talo-integration_ztp-deploying-far-edge-sites" class="j-doc-nav__link ">
    16.3.1. GitOps ZTP and Topology Aware Lifecycle Manager
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-ztp-building-blocks_ztp-deploying-far-edge-sites" class="j-doc-nav__link ">
    16.3.2. Overview of deploying managed clusters with GitOps ZTP
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-creating-the-site-secrets_ztp-deploying-far-edge-sites" class="j-doc-nav__link ">
    16.3.3. Creating the managed bare-metal host secrets
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#setting-managed-bare-metal-host-kernel-arguments_ztp-deploying-far-edge-sites" class="j-doc-nav__link ">
    16.3.4. Configuring Discovery ISO kernel arguments for installations using GitOps ZTP
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-deploying-a-site_ztp-deploying-far-edge-sites" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.3.5. Deploying a managed cluster with SiteConfig and GitOps ZTP
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.3.5. Deploying a managed cluster with SiteConfig and GitOps ZTP"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.3.5. Deploying a managed cluster with SiteConfig and GitOps ZTP"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-siteconfig-config-reference_ztp-deploying-far-edge-sites" class="j-doc-nav__link ">
    16.3.5.1. Single-node OpenShift SiteConfig CR installation reference
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-monitoring-deployment-progress_ztp-deploying-far-edge-sites" class="j-doc-nav__link ">
    16.3.6. Monitoring managed cluster installation progress
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-troubleshooting-ztp-gitops-installation-crs_ztp-deploying-far-edge-sites" class="j-doc-nav__link ">
    16.3.7. Troubleshooting GitOps ZTP by validating the installation CRs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-site-cleanup_ztp-deploying-far-edge-sites" class="j-doc-nav__link ">
    16.3.8. Removing a managed cluster site from the GitOps ZTP pipeline
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-removing-obsolete-content_ztp-deploying-far-edge-sites" class="j-doc-nav__link ">
    16.3.9. Removing obsolete content from the GitOps ZTP pipeline
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-tearing-down-the-pipeline_ztp-deploying-far-edge-sites" class="j-doc-nav__link ">
    16.3.10. Tearing down the GitOps ZTP pipeline
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-configuring-managed-clusters-policies" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.4. Configuring managed clusters with policies and PolicyGenTemplate resources
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.4. Configuring managed clusters with policies and PolicyGenTemplate resources"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.4. Configuring managed clusters with policies and PolicyGenTemplate resources"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-the-policygentemplate_ztp-configuring-managed-clusters-policies" class="j-doc-nav__link ">
    16.4.1. About the PolicyGenTemplate CRD
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-pgt-config-best-practices_ztp-configuring-managed-clusters-policies" class="j-doc-nav__link ">
    16.4.2. Recommendations when customizing PolicyGenTemplate CRs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-policygentemplates-for-ran_ztp-configuring-managed-clusters-policies" class="j-doc-nav__link ">
    16.4.3. PolicyGenTemplate CRs for RAN deployments
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-customizing-a-managed-site-using-pgt_ztp-configuring-managed-clusters-policies" class="j-doc-nav__link ">
    16.4.4. Customizing a managed cluster with PolicyGenTemplate CRs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-monitoring-policy-deployment-progress_ztp-configuring-managed-clusters-policies" class="j-doc-nav__link ">
    16.4.5. Monitoring managed cluster policy deployment progress
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-validating-the-generation-of-configuration-policy-crs_ztp-configuring-managed-clusters-policies" class="j-doc-nav__link ">
    16.4.6. Validating the generation of configuration policy CRs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-restarting-policies-reconciliation_ztp-configuring-managed-clusters-policies" class="j-doc-nav__link ">
    16.4.7. Restarting policy reconciliation
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-removing-content-from-managed-clusters_ztp-configuring-managed-clusters-policies" class="j-doc-nav__link ">
    16.4.8. Changing applied managed cluster CRs using policies
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-definition-of-done-for-ztp-installations_ztp-configuring-managed-clusters-policies" class="j-doc-nav__link ">
    16.4.9. Indication of done for GitOps ZTP installations
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-manual-install" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.5. Manually installing a single-node OpenShift cluster with ZTP
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.5. Manually installing a single-node OpenShift cluster with ZTP"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.5. Manually installing a single-node OpenShift cluster with ZTP"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-generating-install-and-config-crs-manually_ztp-manual-install" class="j-doc-nav__link ">
    16.5.1. Generating GitOps ZTP installation and configuration CRs manually
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-creating-the-site-secrets_ztp-manual-install" class="j-doc-nav__link ">
    16.5.2. Creating the managed bare-metal host secrets
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#setting-managed-bare-metal-host-kernel-arguments_ztp-manual-install" class="j-doc-nav__link ">
    16.5.3. Configuring Discovery ISO kernel arguments for manual installations using GitOps ZTP
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-manually-install-a-single-managed-cluster_ztp-manual-install" class="j-doc-nav__link ">
    16.5.4. Installing a single managed cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-checking-the-managed-cluster-status_ztp-manual-install" class="j-doc-nav__link ">
    16.5.5. Monitoring the managed cluster installation status
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-troubleshooting-the-managed-cluster_ztp-manual-install" class="j-doc-nav__link ">
    16.5.6. Troubleshooting the managed cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-installation-crs_ztp-manual-install" class="j-doc-nav__link ">
    16.5.7. RHACM generated cluster installation CRs reference
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#sno-configure-for-vdu" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.6. Recommended single-node OpenShift cluster configuration for vDU application workloads
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.6. Recommended single-node OpenShift cluster configuration for vDU application workloads"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.6. Recommended single-node OpenShift cluster configuration for vDU application workloads"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-low-latency_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.1. Running low latency applications on OpenShift Container Platform
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-install-sno-hardware-reqs_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.2. Recommended cluster host requirements for vDU application workloads
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-du-configuring-host-firmware-requirements_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.3. Configuring host firmware for low latency and high performance
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-managed-cluster-network-prereqs_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.4. Connectivity prerequisites for managed cluster networks
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-workload-partitioning-sno_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.5. Workload partitioning in single-node OpenShift with GitOps ZTP
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-install-time-cluster-config" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.6.6. Recommended cluster install manifests
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.6.6. Recommended cluster install manifests"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.6.6. Recommended cluster install manifests"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-du-enabling-workload-partitioning_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.6.1. Workload partitioning
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-du-configuring-the-container-mountspace_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.6.2. Reduced platform management footprint
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-du-enabling-sctp_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.6.3. SCTP
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-du-accelerating-container-startup_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.6.4. Accelerated container startup
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-du-enabling-kdump_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.6.5. Automatic kernel crash dumps with kdump
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-du-disabling-crio-wipe_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.6.6. Disable automatic CRI-O cache wipe
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-du-configuring-crun-container-runtime_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.6.7. Configuring crun as the default container runtime
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-post-install-time-cluster-config" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.6.7. Recommended post-installation cluster configurations
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.6.7. Recommended post-installation cluster configurations"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.6.7. Recommended post-installation cluster configurations"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-du-configuring-the-operators_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.7.1. Operator namespaces and Operator groups
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-du-subscribing-to-the-operators-needed-for-platform-configuration_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.7.2. Operator subscriptions
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-du-configuring-logging-locally-and-forwarding_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.7.3. Cluster logging and log forwarding
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-du-configuring-performance-addons_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.7.4. Performance profile
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-du-configuring-ptp_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.7.5. PTP
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-du-tuning-the-performance-patch_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.7.6. Extended Tuned profile
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-du-configuring-sriov_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.7.7. SR-IOV
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-du-removing-the-console-operator_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.7.8. Console Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-du-reducing-resource-usage-with-cluster-monitoring_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.7.9. Grafana and Alertmanager
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#lvms-configuring-lvms-on-sno_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.7.10. LVM Storage
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-du-disabling-network-diagnostics_sno-configure-for-vdu" class="j-doc-nav__link ">
    16.6.7.11. Network diagnostics
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-vdu-configuration-reference" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.7. Validating single-node OpenShift cluster tuning for vDU application workloads
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.7. Validating single-node OpenShift cluster tuning for vDU application workloads"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.7. Validating single-node OpenShift cluster tuning for vDU application workloads"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-du-firmware-config-reference_vdu-config-ref" class="j-doc-nav__link ">
    16.7.1. Recommended firmware configuration for vDU cluster hosts
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-du-cluster-config-reference_vdu-config-ref" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.7.2. Recommended cluster configurations to run vDU applications
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.7.2. Recommended cluster configurations to run vDU applications"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.7.2. Recommended cluster configurations to run vDU applications"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-recommended-cluster-mc-crs_vdu-config-ref" class="j-doc-nav__link ">
    16.7.2.1. Recommended cluster MachineConfig CRs for single-node OpenShift clusters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-recommended-cluster-operators_vdu-config-ref" class="j-doc-nav__link ">
    16.7.2.2. Recommended cluster Operators
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-recommended-cluster-kernel-config_vdu-config-ref" class="j-doc-nav__link ">
    16.7.2.3. Recommended cluster kernel configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-checking-kernel-rt-in-cluster_vdu-config-ref" class="j-doc-nav__link ">
    16.7.2.4. Checking the realtime kernel version
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-checking-du-cluster-config_vdu-config-ref" class="j-doc-nav__link ">
    16.7.3. Checking that the recommended cluster configurations are applied
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-advanced-install-ztp" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.8. Advanced managed cluster configuration with SiteConfig resources
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.8. Advanced managed cluster configuration with SiteConfig resources"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.8. Advanced managed cluster configuration with SiteConfig resources"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-customizing-the-install-extra-manifests_ztp-advanced-install-ztp" class="j-doc-nav__link ">
    16.8.1. Customizing extra installation manifests in the GitOps ZTP pipeline
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-filtering-ai-crs-using-siteconfig_ztp-advanced-install-ztp" class="j-doc-nav__link ">
    16.8.2. Filtering custom resources using SiteConfig filters
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-advanced-policy-config" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.9. Advanced managed cluster configuration with PolicyGenTemplate resources
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.9. Advanced managed cluster configuration with PolicyGenTemplate resources"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.9. Advanced managed cluster configuration with PolicyGenTemplate resources"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-deploying-additional-changes-to-clusters_ztp-advanced-policy-config" class="j-doc-nav__link ">
    16.9.1. Deploying additional changes to clusters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-using-pgt-to-update-source-crs_ztp-advanced-policy-config" class="j-doc-nav__link ">
    16.9.2. Using PolicyGenTemplate CRs to override source CRs content
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-adding-new-content-to-gitops-ztp_ztp-advanced-policy-config" class="j-doc-nav__link ">
    16.9.3. Adding new content to the GitOps ZTP pipeline
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-configuring-pgt-compliance-eval-timeouts_ztp-advanced-policy-config" class="j-doc-nav__link ">
    16.9.4. Configuring policy compliance evaluation timeouts for PolicyGenTemplate CRs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-creating-a-validator-inform-policy_ztp-advanced-policy-config" class="j-doc-nav__link ">
    16.9.5. Signalling GitOps ZTP cluster deployment completion with validator inform policies
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-using-pgt-to-configure-power-saving-states_ztp-advanced-policy-config" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.9.6. Configuring power states using PolicyGenTemplates CRs
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.9.6. Configuring power states using PolicyGenTemplates CRs"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.9.6. Configuring power states using PolicyGenTemplates CRs"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-using-pgt-to-configure-performance-mode_ztp-advanced-policy-config" class="j-doc-nav__link ">
    16.9.6.1. Configuring performance mode using PolicyGenTemplate CRs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-using-pgt-to-configure-high-performance-mode_ztp-advanced-policy-config" class="j-doc-nav__link ">
    16.9.6.2. Configuring high-performance mode using PolicyGenTemplate CRs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-using-pgt-to-configure-power-saving-mode_ztp-advanced-policy-config" class="j-doc-nav__link ">
    16.9.6.3. Configuring power saving mode using PolicyGenTemplate CRs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-using-pgt-to-maximize-power-savings-mode_ztp-advanced-policy-config" class="j-doc-nav__link ">
    16.9.6.4. Maximizing power savings
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-provisioning-lvm-storage_ztp-advanced-policy-config" class="j-doc-nav__link ">
    16.9.7. Configuring LVM Storage using PolicyGenTemplate CRs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-advanced-policy-config-ptp_ztp-advanced-policy-config" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.9.8. Configuring PTP events with PolicyGenTemplate CRs
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.9.8. Configuring PTP events with PolicyGenTemplate CRs"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.9.8. Configuring PTP events with PolicyGenTemplate CRs"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-configuring-ptp-fast-events_ztp-advanced-policy-config" class="j-doc-nav__link ">
    16.9.8.1. Configuring PTP events that use HTTP transport
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-configuring-ptp-fast-events-amqp_ztp-advanced-policy-config" class="j-doc-nav__link ">
    16.9.8.2. Configuring PTP events that use AMQP transport
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-advanced-policy-config-bare-metal_ztp-advanced-policy-config" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.9.9. Configuring bare-metal events with PolicyGenTemplate CRs
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.9.9. Configuring bare-metal events with PolicyGenTemplate CRs"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.9.9. Configuring bare-metal events with PolicyGenTemplate CRs"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-creating-hwevents_ztp-advanced-policy-config" class="j-doc-nav__link ">
    16.9.9.1. Configuring bare-metal events that use HTTP transport
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-creating-hwevents-amqp_ztp-advanced-policy-config" class="j-doc-nav__link ">
    16.9.9.2. Configuring bare-metal events that use AMQP transport
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-add-local-reg-for-sno-duprofile_ztp-advanced-policy-config" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.9.10. Configuring the Image Registry Operator for local caching of images
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.9.10. Configuring the Image Registry Operator for local caching of images"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.9.10. Configuring the Image Registry Operator for local caching of images"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-configuring-disk-partitioning_ztp-advanced-policy-config" class="j-doc-nav__link ">
    16.9.10.1. Configuring disk partitioning with SiteConfig
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-configuring-pgt-image-registry_ztp-advanced-policy-config" class="j-doc-nav__link ">
    16.9.10.2. Configuring the image registry using PolicyGenTemplate CRs
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-using-hub-cluster-templates_ztp-advanced-policy-config" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.9.11. Using hub templates in PolicyGenTemplate CRs
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.9.11. Using hub templates in PolicyGenTemplate CRs"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.9.11. Using hub templates in PolicyGenTemplate CRs"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-example-hub-template-functions_ztp-advanced-policy-config" class="j-doc-nav__link ">
    16.9.11.1. Example hub templates
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-specifying-nics-in-pgt-crs-with-hub-cluster-templates_ztp-advanced-policy-config" class="j-doc-nav__link ">
    16.9.11.2. Specifying host NICs in site PolicyGenTemplate CRs with hub cluster templates
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-managing-sriov-vlan-with-hub-cluster-templates-in-pgt_ztp-advanced-policy-config" class="j-doc-nav__link ">
    16.9.11.3. Specifying VLAN IDs in group PolicyGenTemplate CRs with hub cluster templates
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-syncing-new-configmap-changes-to-existing-pgt-crs_ztp-advanced-policy-config" class="j-doc-nav__link ">
    16.9.11.4. Syncing new ConfigMap changes to existing PolicyGenTemplate CRs
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-talm-for-cluster-updates" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.10. Updating managed clusters with the Topology Aware Lifecycle Manager
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.10. Updating managed clusters with the Topology Aware Lifecycle Manager"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.10. Updating managed clusters with the Topology Aware Lifecycle Manager"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-about-topology-aware-lifecycle-manager-config_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.1. About the Topology Aware Lifecycle Manager configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-about-topology-aware-lifecycle-manager-about-policies_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.2. About managed policies used with Topology Aware Lifecycle Manager
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#installing-topology-aware-lifecycle-manager-using-web-console_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.3. Installing the Topology Aware Lifecycle Manager by using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#installing-topology-aware-lifecycle-manager-using-cli_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.4. Installing the Topology Aware Lifecycle Manager by using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-about-cgu-crs_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.10.5. About the ClusterGroupUpgrade CR
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.10.5. About the ClusterGroupUpgrade CR"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.10.5. About the ClusterGroupUpgrade CR"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#selecting_clusters_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.5.1. Selecting clusters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#validating_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.5.2. Validating
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#precaching_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.5.3. Pre-caching
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#creating_backup_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.5.4. Creating a backup
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#updating_clusters_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.5.5. Updating clusters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#update_status_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.5.6. Update status
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-about-topology-aware-lifecycle-manager-blocking-crs_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.5.7. Blocking ClusterGroupUpgrade CRs
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-policies-concept_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.10.6. Update policies on managed clusters
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.10.6. Update policies on managed clusters"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.10.6. Update policies on managed clusters"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-about-subscription-crs_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.6.1. Configuring Operator subscriptions for managed clusters that you install with TALM
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-apply-policies_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.6.2. Applying update policies to managed clusters
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-backup-feature-concept_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.10.7. Creating a backup of cluster resources before upgrade
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.10.7. Creating a backup of cluster resources before upgrade"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.10.7. Creating a backup of cluster resources before upgrade"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-backup-start_and_update_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.7.1. Creating a ClusterGroupUpgrade CR with backup
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-backup-recovery_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.7.2. Recovering a cluster after a failed upgrade
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-precache-feature-concept_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.10.8. Using the container image pre-cache feature
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.10.8. Using the container image pre-cache feature"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.10.8. Using the container image pre-cache feature"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-precache-feature-image-filter_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.8.1. Using the container image pre-cache filter
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-precache-start_and_update_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.8.2. Creating a ClusterGroupUpgrade CR with pre-caching
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-troubleshooting_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.10.9. Troubleshooting the Topology Aware Lifecycle Manager
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.10.9. Troubleshooting the Topology Aware Lifecycle Manager"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.10.9. Troubleshooting the Topology Aware Lifecycle Manager"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-general-troubleshooting_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.9.1. General troubleshooting
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-troubleshooting-modify-cgu_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.9.2. Cannot modify the ClusterUpgradeGroup CR
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-troubleshooting-managed-policies_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.9.3. Managed policies
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-troubleshooting-clusters_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.9.4. Clusters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-troubleshooting-remediation-strategy_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.9.5. Remediation Strategy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-troubleshooting-remediation-talo_cnf-topology-aware-lifecycle-manager" class="j-doc-nav__link ">
    16.10.9.6. Topology Aware Lifecycle Manager
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-topology-aware-lifecycle-manager" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.11. Updating managed clusters in a disconnected environment with the Topology Aware Lifecycle Manager
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.11. Updating managed clusters in a disconnected environment with the Topology Aware Lifecycle Manager"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.11. Updating managed clusters in a disconnected environment with the Topology Aware Lifecycle Manager"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-platform-prepare-end-to-end_ztp-talm" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.11.1. Updating clusters in a disconnected environment
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.11.1. Updating clusters in a disconnected environment"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.11.1. Updating clusters in a disconnected environment"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-platform-prepare-for-update-env-setup_ztp-talm" class="j-doc-nav__link ">
    16.11.1.1. Setting up the environment
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-platform-update_ztp-talm" class="j-doc-nav__link ">
    16.11.1.2. Performing a platform update
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-operator-update_ztp-talm" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.11.1.3. Performing an Operator update
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.11.1.3. Performing an Operator update"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.11.1.3. Performing an Operator update"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#cnf-topology-aware-lifecycle-manager-operator-troubleshooting_ztp-talm" class="j-doc-nav__link ">
    16.11.1.3.1. Troubleshooting missed Operator updates due to out-of-date policy compliance states
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-operator-and-platform-update_ztp-talm" class="j-doc-nav__link ">
    16.11.1.4. Performing a platform and an Operator update together
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talm-pao-update_ztp-talm" class="j-doc-nav__link ">
    16.11.1.5. Removing Performance Addon Operator subscriptions from deployed clusters
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#talo-precache-autocreated-cgu-for-ztp_ztp-talm" class="j-doc-nav__link ">
    16.11.2. About the auto-created ClusterGroupUpgrade CR for GitOps ZTP
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-updating-gitops" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.12. Updating GitOps ZTP
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.12. Updating GitOps ZTP"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.12. Updating GitOps ZTP"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-updating-gitops-ztp_ztp-updating-gitops" class="j-doc-nav__link ">
    16.12.1. Overview of the GitOps ZTP update process
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-preparing-for-the-gitops-ztp-upgrade_ztp-updating-gitops" class="j-doc-nav__link ">
    16.12.2. Preparing for the upgrade
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-labeling-the-existing-clusters_ztp-updating-gitops" class="j-doc-nav__link ">
    16.12.3. Labeling the existing clusters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-stopping-the-existing-gitops-ztp-applications_ztp-updating-gitops" class="j-doc-nav__link ">
    16.12.4. Stopping the existing GitOps ZTP applications
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-required-changes-to-the-git-repository_ztp-updating-gitops" class="j-doc-nav__link ">
    16.12.5. Required changes to the Git repository
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-installing-the-new-gitops-ztp-applications_ztp-updating-gitops" class="j-doc-nav__link ">
    16.12.6. Installing the new GitOps ZTP applications
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-roll-out-the-configuration-changes_ztp-updating-gitops" class="j-doc-nav__link ">
    16.12.7. Rolling out the GitOps ZTP configuration changes
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-sno-additional-worker-node" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.13. Expanding single-node OpenShift clusters with GitOps ZTP
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.13. Expanding single-node OpenShift clusters with GitOps ZTP"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.13. Expanding single-node OpenShift clusters with GitOps ZTP"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-additional-worker-apply-du-profile_sno-additional-worker" class="j-doc-nav__link ">
    16.13.1. Applying profiles to the worker node
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-additional-worker-daemon-selector-comp_sno-additional-worker" class="j-doc-nav__link ">
    16.13.2. (Optional) Ensuring PTP and SR-IOV daemon selector compatibility
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-additional-worker-node-selector-comp_sno-additional-worker" class="j-doc-nav__link ">
    16.13.3. PTP and SR-IOV node selector compatibility
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-additional-worker-policies_sno-additional-worker" class="j-doc-nav__link ">
    16.13.4. Using PolicyGenTemplate CRs to apply worker node policies to worker nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-additional-worker-sno-proc_sno-additional-worker" class="j-doc-nav__link ">
    16.13.5. Adding worker nodes to single-node OpenShift clusters with GitOps ZTP
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-pre-staging-tool" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.14. Pre-caching images for single-node OpenShift deployments
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.14. Pre-caching images for single-node OpenShift deployments"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.14. Pre-caching images for single-node OpenShift deployments"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-getting-tool_pre-caching" class="j-doc-nav__link ">
    16.14.1. Getting the factory-precaching-cli tool
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-booting-from-live-os_pre-caching" class="j-doc-nav__link ">
    16.14.2. Booting from a live operating system image
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-partitioning_pre-caching" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.14.3. Partitioning the disk
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.14.3. Partitioning the disk"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.14.3. Partitioning the disk"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-create-partition_pre-caching" class="j-doc-nav__link ">
    16.14.3.1. Creating the partition
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-mount-partition_pre-caching" class="j-doc-nav__link ">
    16.14.3.2. Mounting the partition
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-downloading-images_pre-caching" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.14.4. Downloading the images
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.14.4. Downloading the images"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.14.4. Downloading the images"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-downloading-images-parallel-workers_pre-caching" class="j-doc-nav__link ">
    16.14.4.1. Downloading with parallel workers
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-preparing-ocp-images_pre-caching" class="j-doc-nav__link ">
    16.14.4.2. Preparing to download the OpenShift Container Platform images
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-downloading-ocp-images_pre-caching" class="j-doc-nav__link ">
    16.14.4.3. Downloading the OpenShift Container Platform images
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-downloading-operator-images_pre-caching" class="j-doc-nav__link ">
    16.14.4.4. Downloading the Operator images
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-custom-pre-caching-in-disconnected-environment_pre-caching" class="j-doc-nav__link ">
    16.14.4.5. Pre-caching custom images in disconnected environments
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-pre-caching-config-con_pre-caching" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.14.5. Pre-caching images in GitOps ZTP
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.14.5. Pre-caching images in GitOps ZTP"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.14.5. Pre-caching images in GitOps ZTP"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-pre-caching-config-clusters-ignitionconfigoverride_pre-caching" class="j-doc-nav__link ">
    16.14.5.1. Understanding the clusters.ignitionConfigOverride field
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-pre-caching-config-nodes-installerargs_pre-caching" class="j-doc-nav__link ">
    16.14.5.2. Understanding the nodes.installerArgs field
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-pre-caching-config-nodes-ignitionconfigoverride_pre-caching" class="j-doc-nav__link ">
    16.14.5.3. Understanding the nodes.ignitionConfigOverride field
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#ztp-pre-staging-troubleshooting_pre-caching" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.14.6. Troubleshooting
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.14.6. Troubleshooting"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.14.6. Troubleshooting"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#rendered-catalog-is-invalid" class="j-doc-nav__link ">
    16.14.6.1. Rendered catalog is invalid
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                    

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance#idm139735351482992" class="j-doc-nav__link ">
    Legal Notice
  </a>
  
          </li>
              </ol>
    
  </div>
</nav>


              </div>
            </div>
            <div class="j-options-container j-options-container--mobile">
              <button class="j-sidebar__menu-trigger j-sidebar__menu-trigger--options content-expander__trigger">
                <span class="j-sidebar__menu-trigger__open-headline">
                  Settings
                </span>
                <span class="j-sidebar__menu-trigger__close-text">Close</span>
              </button>
              

  <ul class="j-doc-options__list content-expander">
    <li class="j-doc-options__item">
          <label class="j-doc-option__label j-doc-option__label--language" for="j-doc-language">
        Language:
      </label>
      <select id="j-doc-language" class="j-doc-option__select">
                  <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance" selected=''>
            English
          </option>
                  <option value="/documentation/zh-cn/openshift_container_platform/4.13/html-single/scalability_and_performance" >
            简体中文
          </option>
                  <option value="/documentation/ko-kr/openshift_container_platform/4.13/html-single/scalability_and_performance" >
            한국어
          </option>
                  <option value="/documentation/ja-jp/openshift_container_platform/4.13/html-single/scalability_and_performance" >
            日本語
          </option>
              </select>

            <noscript>
        <div class="j-doc-option__label j-doc-option__label--language" id="j-doc-option__label--language--nojs">
          Language:
        </div>
        <ul aria-labelledby="j-doc-option__label--language--nojs" class="j-doc-option__languages-list">
                      <li>
              <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance">English</a>
            </li>
                      <li>
              <a href="/documentation/zh-cn/openshift_container_platform/4.13/html-single/scalability_and_performance">简体中文</a>
            </li>
                      <li>
              <a href="/documentation/ko-kr/openshift_container_platform/4.13/html-single/scalability_and_performance">한국어</a>
            </li>
                      <li>
              <a href="/documentation/ja-jp/openshift_container_platform/4.13/html-single/scalability_and_performance">日本語</a>
            </li>
                  </ul>
      </noscript>

      </li>

    <li class="j-doc-options__item">
    <label for="j-doc-mode" class="j-doc-option__label j-doc-option__label--format">
      Format:
    </label>
    <select id="j-doc-mode" class="j-doc-option__select">
              <option value="/documentation/en-us/openshift_container_platform/4.13/html/scalability_and_performance"  class="j-doc-options__option j-doc-options__option--multi-page">
          Multi-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance" selected='' class="j-doc-options__option j-doc-options__option--single-page">
          Single-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/pdf/scalability_and_performance/OpenShift_Container_Platform-4.13-Scalability_and_performance-en-US.pdf"  class="j-doc-options__option j-doc-options__option--pdf">
          PDF
        </option>
          </select>

        <noscript>
      <div class="j-doc-option__label j-doc-option__label--format" id="j-doc-option__label--format--nojs">
        Format:
      </div>
      <ul class="j-doc-option__format-list" aria-labelledby="j-doc-option__label--format--nojs">
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--multi-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html/scalability_and_performance">Multi-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--single-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance">Single-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--pdf"><a href="/documentation/en-us/openshift_container_platform/4.13/pdf/scalability_and_performance/OpenShift_Container_Platform-4.13-Scalability_and_performance-en-US.pdf">PDF</a></li>
              </ul>
    </noscript>
  </li>
</ul>


              </div>
          </div>
                <div class="pvof-doc__tertiary-sidebar j-doc__tertiary-sidebar">
          <div class="pvof-doc__tertiary-sidebar__inner j-doc__tertiary-sidebar__inner">
            <div class="j-doc__doc-options">
              <div class="j-options-container j-options-container--desktop">
                <button class="j-sidebar__menu-trigger j-sidebar__menu-trigger--tablet content-expander__trigger">
                  <span class="j-sidebar__menu-trigger-icon"></span>
                  <h2 class="visually-hidden">Language and Page Formatting Options</h2>
                </button>
                  

  <ul class="j-doc-options__list content-expander">
    <li class="j-doc-options__item">
          <label class="j-doc-option__label j-doc-option__label--language" for="j-doc-language--2">
        Language:
      </label>
      <select id="j-doc-language--2" class="j-doc-option__select">
                  <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance" selected=''>
            English
          </option>
                  <option value="/documentation/zh-cn/openshift_container_platform/4.13/html-single/scalability_and_performance" >
            简体中文
          </option>
                  <option value="/documentation/ko-kr/openshift_container_platform/4.13/html-single/scalability_and_performance" >
            한국어
          </option>
                  <option value="/documentation/ja-jp/openshift_container_platform/4.13/html-single/scalability_and_performance" >
            日本語
          </option>
              </select>

            <noscript>
        <div class="j-doc-option__label j-doc-option__label--language" id="j-doc-option__label--language--nojs">
          Language:
        </div>
        <ul aria-labelledby="j-doc-option__label--language--nojs" class="j-doc-option__languages-list">
                      <li>
              <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance">English</a>
            </li>
                      <li>
              <a href="/documentation/zh-cn/openshift_container_platform/4.13/html-single/scalability_and_performance">简体中文</a>
            </li>
                      <li>
              <a href="/documentation/ko-kr/openshift_container_platform/4.13/html-single/scalability_and_performance">한국어</a>
            </li>
                      <li>
              <a href="/documentation/ja-jp/openshift_container_platform/4.13/html-single/scalability_and_performance">日本語</a>
            </li>
                  </ul>
      </noscript>

      </li>

    <li class="j-doc-options__item">
    <label for="j-doc-mode--2" class="j-doc-option__label j-doc-option__label--format">
      Format:
    </label>
    <select id="j-doc-mode--2" class="j-doc-option__select">
              <option value="/documentation/en-us/openshift_container_platform/4.13/html/scalability_and_performance"  class="j-doc-options__option j-doc-options__option--multi-page">
          Multi-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance" selected='' class="j-doc-options__option j-doc-options__option--single-page">
          Single-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/pdf/scalability_and_performance/OpenShift_Container_Platform-4.13-Scalability_and_performance-en-US.pdf"  class="j-doc-options__option j-doc-options__option--pdf">
          PDF
        </option>
          </select>

        <noscript>
      <div class="j-doc-option__label j-doc-option__label--format" id="j-doc-option__label--format--nojs">
        Format:
      </div>
      <ul class="j-doc-option__format-list" aria-labelledby="j-doc-option__label--format--nojs">
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--multi-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html/scalability_and_performance">Multi-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--single-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance">Single-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--pdf"><a href="/documentation/en-us/openshift_container_platform/4.13/pdf/scalability_and_performance/OpenShift_Container_Platform-4.13-Scalability_and_performance-en-US.pdf">PDF</a></li>
              </ul>
    </noscript>
  </li>
</ul>


                </div>
              </div>
          </div>
        </div>

                  <div class="doc-wrapper pvof-doc__wrapper j-superdoc__content-wrapper" id="doc-wrapper">
            

  <div class="pane-page-title">
    <h1 class="title" itemprop="name">Scalability and performance</h1>
  </div>


  <div xml:lang="en-US" class="book" id="idm139735353265648"><div class="titlepage"><div><div class="producttitle"><span class="productname">OpenShift Container Platform</span> <span class="productnumber">4.13</span></div><div><h2 class="subtitle">Scaling your OpenShift Container Platform cluster and tuning performance in production environments</h2></div><div><div xml:lang="en-US" class="authorgroup"><span class="orgname">Red Hat OpenShift Documentation Team</span></div></div><div><a href="#idm139735351482992">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				This document provides instructions for scaling your cluster and optimizing the performance of your OpenShift Container Platform environment.
			</div></div></div></div><hr/></div><section class="chapter" id="recommended-performance-and-scalability-practices"><div class="titlepage"><div><div><h1 class="title">Chapter 1. Recommended performance and scalability practices</h1></div></div></div><section class="section" id="recommended-control-plane-practices"><div class="titlepage"><div><div><h2 class="title">1.1. Recommended control plane practices</h2></div></div></div><p>
				This topic provides recommended performance and scalability practices for control planes in OpenShift Container Platform.
			</p><section class="section" id="recommended-scale-practices_recommended-control-plane-practices"><div class="titlepage"><div><div><h3 class="title">1.1.1. Recommended practices for scaling the cluster</h3></div></div></div><p>
					The guidance in this section is only relevant for installations with cloud provider integration.
				</p><p>
					Apply the following best practices to scale the number of worker machines in your OpenShift Container Platform cluster. You scale the worker machines by increasing or decreasing the number of replicas that are defined in the worker machine set.
				</p><p>
					When scaling up the cluster to higher node counts:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Spread nodes across all of the available zones for higher availability.
						</li><li class="listitem">
							Scale up by no more than 25 to 50 machines at once.
						</li><li class="listitem">
							Consider creating new compute machine sets in each available zone with alternative instance types of similar size to help mitigate any periodic provider capacity constraints. For example, on AWS, use m5.large and m5d.large.
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Cloud providers might implement a quota for API services. Therefore, gradually scale the cluster.
					</p></div></div><p>
					The controller might not be able to create the machines if the replicas in the compute machine sets are set to higher numbers all at one time. The number of requests the cloud platform, which OpenShift Container Platform is deployed on top of, is able to handle impacts the process. The controller will start to query more while trying to create, check, and update the machines with the status. The cloud platform on which OpenShift Container Platform is deployed has API request limits; excessive queries might lead to machine creation failures due to cloud platform limitations.
				</p><p>
					Enable machine health checks when scaling to large node counts. In case of failures, the health checks monitor the condition and automatically repair unhealthy machines.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						When scaling large and dense clusters to lower node counts, it might take large amounts of time because the process involves draining or evicting the objects running on the nodes being terminated in parallel. Also, the client might start to throttle the requests if there are too many objects to evict. The default client queries per second (QPS) and burst rates are currently set to <code class="literal">5</code> and <code class="literal">10</code> respectively. These values cannot be modified in OpenShift Container Platform.
					</p></div></div></section><section class="section" id="master-node-sizing_recommended-control-plane-practices"><div class="titlepage"><div><div><h3 class="title">1.1.2. Control plane node sizing</h3></div></div></div><p>
					The control plane node resource requirements depend on the number and type of nodes and objects in the cluster. The following control plane node size recommendations are based on the results of a control plane density focused testing, or <span class="emphasis"><em>Cluster-density</em></span>. This test creates the following objects across a given number of namespaces:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							1 image stream
						</li><li class="listitem">
							1 build
						</li><li class="listitem">
							5 deployments, with 2 pod replicas in a <code class="literal">sleep</code> state, mounting 4 secrets, 4 config maps, and 1 downward API volume each
						</li><li class="listitem">
							5 services, each one pointing to the TCP/8080 and TCP/8443 ports of one of the previous deployments
						</li><li class="listitem">
							1 route pointing to the first of the previous services
						</li><li class="listitem">
							10 secrets containing 2048 random string characters
						</li><li class="listitem">
							10 config maps containing 2048 random string characters
						</li></ul></div><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735352494784" scope="col">Number of worker nodes</th><th align="left" valign="top" id="idm139735352493696" scope="col">Cluster-density (namespaces)</th><th align="left" valign="top" id="idm139735354408480" scope="col">CPU cores</th><th align="left" valign="top" id="idm139735354407392" scope="col">Memory (GB)</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735352494784"> <p>
									24
								</p>
								 </td><td align="left" valign="top" headers="idm139735352493696"> <p>
									500
								</p>
								 </td><td align="left" valign="top" headers="idm139735354408480"> <p>
									4
								</p>
								 </td><td align="left" valign="top" headers="idm139735354407392"> <p>
									16
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735352494784"> <p>
									120
								</p>
								 </td><td align="left" valign="top" headers="idm139735352493696"> <p>
									1000
								</p>
								 </td><td align="left" valign="top" headers="idm139735354408480"> <p>
									8
								</p>
								 </td><td align="left" valign="top" headers="idm139735354407392"> <p>
									32
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735352494784"> <p>
									252
								</p>
								 </td><td align="left" valign="top" headers="idm139735352493696"> <p>
									4000
								</p>
								 </td><td align="left" valign="top" headers="idm139735354408480"> <p>
									16, but 24 if using the OVN-Kubernetes network plug-in
								</p>
								 </td><td align="left" valign="top" headers="idm139735354407392"> <p>
									64, but 128 if using the OVN-Kubernetes network plug-in
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735352494784"> <p>
									501, but untested with the OVN-Kubernetes network plug-in
								</p>
								 </td><td align="left" valign="top" headers="idm139735352493696"> <p>
									4000
								</p>
								 </td><td align="left" valign="top" headers="idm139735354408480"> <p>
									16
								</p>
								 </td><td align="left" valign="top" headers="idm139735354407392"> <p>
									96
								</p>
								 </td></tr></tbody></table></div><p>
					The data from the table above is based on an OpenShift Container Platform running on top of AWS, using r5.4xlarge instances as control-plane nodes and m5.2xlarge instances as worker nodes.
				</p><p>
					On a large and dense cluster with three control plane nodes, the CPU and memory usage will spike up when one of the nodes is stopped, rebooted, or fails. The failures can be due to unexpected issues with power, network, underlying infrastructure, or intentional cases where the cluster is restarted after shutting it down to save costs. The remaining two control plane nodes must handle the load in order to be highly available, which leads to increase in the resource usage. This is also expected during upgrades because the control plane nodes are cordoned, drained, and rebooted serially to apply the operating system updates, as well as the control plane Operators update. To avoid cascading failures, keep the overall CPU and memory resource usage on the control plane nodes to at most 60% of all available capacity to handle the resource usage spikes. Increase the CPU and memory on the control plane nodes accordingly to avoid potential downtime due to lack of resources.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The node sizing varies depending on the number of nodes and object counts in the cluster. It also depends on whether the objects are actively being created on the cluster. During object creation, the control plane is more active in terms of resource usage compared to when the objects are in the <code class="literal">running</code> phase.
					</p></div></div><p>
					Operator Lifecycle Manager (OLM ) runs on the control plane nodes and its memory footprint depends on the number of namespaces and user installed operators that OLM needs to manage on the cluster. Control plane nodes need to be sized accordingly to avoid OOM kills. Following data points are based on the results from cluster maximums testing.
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735352431424" scope="col">Number of namespaces</th><th align="left" valign="top" id="idm139735352507328" scope="col">OLM memory at idle state (GB)</th><th align="left" valign="top" id="idm139735352506272" scope="col">OLM memory with 5 user operators installed (GB)</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735352431424"> <p>
									500
								</p>
								 </td><td align="left" valign="top" headers="idm139735352507328"> <p>
									0.823
								</p>
								 </td><td align="left" valign="top" headers="idm139735352506272"> <p>
									1.7
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735352431424"> <p>
									1000
								</p>
								 </td><td align="left" valign="top" headers="idm139735352507328"> <p>
									1.2
								</p>
								 </td><td align="left" valign="top" headers="idm139735352506272"> <p>
									2.5
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735352431424"> <p>
									1500
								</p>
								 </td><td align="left" valign="top" headers="idm139735352507328"> <p>
									1.7
								</p>
								 </td><td align="left" valign="top" headers="idm139735352506272"> <p>
									3.2
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735352431424"> <p>
									2000
								</p>
								 </td><td align="left" valign="top" headers="idm139735352507328"> <p>
									2
								</p>
								 </td><td align="left" valign="top" headers="idm139735352506272"> <p>
									4.4
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735352431424"> <p>
									3000
								</p>
								 </td><td align="left" valign="top" headers="idm139735352507328"> <p>
									2.7
								</p>
								 </td><td align="left" valign="top" headers="idm139735352506272"> <p>
									5.6
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735352431424"> <p>
									4000
								</p>
								 </td><td align="left" valign="top" headers="idm139735352507328"> <p>
									3.8
								</p>
								 </td><td align="left" valign="top" headers="idm139735352506272"> <p>
									7.6
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735352431424"> <p>
									5000
								</p>
								 </td><td align="left" valign="top" headers="idm139735352507328"> <p>
									4.2
								</p>
								 </td><td align="left" valign="top" headers="idm139735352506272"> <p>
									9.02
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735352431424"> <p>
									6000
								</p>
								 </td><td align="left" valign="top" headers="idm139735352507328"> <p>
									5.8
								</p>
								 </td><td align="left" valign="top" headers="idm139735352506272"> <p>
									11.3
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735352431424"> <p>
									7000
								</p>
								 </td><td align="left" valign="top" headers="idm139735352507328"> <p>
									6.6
								</p>
								 </td><td align="left" valign="top" headers="idm139735352506272"> <p>
									12.9
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735352431424"> <p>
									8000
								</p>
								 </td><td align="left" valign="top" headers="idm139735352507328"> <p>
									6.9
								</p>
								 </td><td align="left" valign="top" headers="idm139735352506272"> <p>
									14.8
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735352431424"> <p>
									9000
								</p>
								 </td><td align="left" valign="top" headers="idm139735352507328"> <p>
									8
								</p>
								 </td><td align="left" valign="top" headers="idm139735352506272"> <p>
									17.7
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735352431424"> <p>
									10,000
								</p>
								 </td><td align="left" valign="top" headers="idm139735352507328"> <p>
									9.9
								</p>
								 </td><td align="left" valign="top" headers="idm139735352506272"> <p>
									21.6
								</p>
								 </td></tr></tbody></table></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						You can modify the control plane node size in a running OpenShift Container Platform 4.13 cluster for the following configurations only:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Clusters installed with a user-provisioned installation method.
							</li><li class="listitem">
								AWS clusters installed with an installer-provisioned infrastructure installation method.
							</li><li class="listitem">
								Clusters that use a control plane machine set to manage control plane machines.
							</li></ul></div><p>
						For all other configurations, you must estimate your total node count and use the suggested control plane node size during installation.
					</p></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The recommendations are based on the data points captured on OpenShift Container Platform clusters with OpenShift SDN as the network plugin.
					</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						In OpenShift Container Platform 4.13, half of a CPU core (500 millicore) is now reserved by the system by default compared to OpenShift Container Platform 3.11 and previous versions. The sizes are determined taking that into consideration.
					</p></div></div><section class="section" id="increasing-aws-flavor-size_recommended-control-plane-practices"><div class="titlepage"><div><div><h4 class="title">1.1.2.1. Selecting a larger Amazon Web Services instance type for control plane machines</h4></div></div></div><p>
						If the control plane machines in an Amazon Web Services (AWS) cluster require more resources, you can select a larger AWS instance type for the control plane machines to use.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The procedure for clusters that use a control plane machine set is different from the procedure for clusters that do not use a control plane machine set.
						</p><p>
							If you are uncertain about the state of the <code class="literal">ControlPlaneMachineSet</code> CR in your cluster, you can <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-checking-status_cpmso-getting-started">verify the CR status</a>.
						</p></div></div><section class="section" id="cpms-changing-aws-instance-type_recommended-control-plane-practices"><div class="titlepage"><div><div><h5 class="title">1.1.2.1.1. Changing the Amazon Web Services instance type by using a control plane machine set</h5></div></div></div><p>
							You can change the Amazon Web Services (AWS) instance type that your control plane machines use by updating the specification in the control plane machine set custom resource (CR).
						</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
									Your AWS cluster uses a control plane machine set.
								</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
									Edit your control plane machine set CR by running the following command:
								</p><pre class="programlisting language-terminal">$ oc --namespace openshift-machine-api edit controlplanemachineset.machine.openshift.io cluster</pre></li><li class="listitem"><p class="simpara">
									Edit the following line under the <code class="literal">providerSpec</code> field:
								</p><pre class="programlisting language-yaml">providerSpec:
  value:
    ...
    instanceType: &lt;compatible_aws_instance_type&gt; <span id="CO1-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO1-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify a larger AWS instance type with the same base as the previous selection. For example, you can change <code class="literal">m6i.xlarge</code> to <code class="literal">m6i.2xlarge</code> or <code class="literal">m6i.4xlarge</code>.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Save your changes.
								</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											For clusters that use the default <code class="literal">RollingUpdate</code> update strategy, the Operator automatically propagates the changes to your control plane configuration.
										</li><li class="listitem">
											For clusters that are configured to use the <code class="literal">OnDelete</code> update strategy, you must replace your control plane machines manually.
										</li></ul></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-using">Managing control plane machines with control plane machine sets</a>
								</li></ul></div></section><section class="section" id="aws-console-changing-aws-instance-type_recommended-control-plane-practices"><div class="titlepage"><div><div><h5 class="title">1.1.2.1.2. Changing the Amazon Web Services instance type by using the AWS console</h5></div></div></div><p>
							You can change the Amazon Web Services (AWS) instance type that your control plane machines use by updating the instance type in the AWS console.
						</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
									You have access to the AWS console with the permissions required to modify the EC2 Instance for your cluster.
								</li><li class="listitem">
									You have access to the OpenShift Container Platform cluster as a user with the <code class="literal">cluster-admin</code> role.
								</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
									Open the AWS console and fetch the instances for the control plane machines.
								</li><li class="listitem"><p class="simpara">
									Choose one control plane machine instance.
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
											For the selected control plane machine, back up the etcd data by creating an etcd snapshot. For more information, see "Backing up etcd".
										</li><li class="listitem">
											In the AWS console, stop the control plane machine instance.
										</li><li class="listitem">
											Select the stopped instance, and click <span class="strong strong"><strong>Actions</strong></span> → <span class="strong strong"><strong>Instance Settings</strong></span> → <span class="strong strong"><strong>Change instance type</strong></span>.
										</li><li class="listitem">
											Change the instance to a larger type, ensuring that the type is the same base as the previous selection, and apply changes. For example, you can change <code class="literal">m6i.xlarge</code> to <code class="literal">m6i.2xlarge</code> or <code class="literal">m6i.4xlarge</code>.
										</li><li class="listitem">
											Start the instance.
										</li><li class="listitem">
											If your OpenShift Container Platform cluster has a corresponding <code class="literal">Machine</code> object for the instance, update the instance type of the object to match the instance type set in the AWS console.
										</li></ol></div></li><li class="listitem">
									Repeat this process for each control plane machine.
								</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/backup_and_restore/#backing-up-etcd">Backing up etcd</a>
								</li><li class="listitem">
									<a class="link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html">AWS documentation about changing the instance type</a>
								</li></ul></div></section></section></section></section><section class="section" id="recommended-infrastructure-practices"><div class="titlepage"><div><div><h2 class="title">1.2. Recommended infrastructure practices</h2></div></div></div><p>
				This topic provides recommended performance and scalability practices for infrastructure in OpenShift Container Platform.
			</p><section class="section" id="infrastructure-node-sizing_recommended-infrastructure-practices"><div class="titlepage"><div><div><h3 class="title">1.2.1. Infrastructure node sizing</h3></div></div></div><p>
					<span class="emphasis"><em>Infrastructure nodes</em></span> are nodes that are labeled to run pieces of the OpenShift Container Platform environment. The infrastructure node resource requirements depend on the cluster age, nodes, and objects in the cluster, as these factors can lead to an increase in the number of metrics or time series in Prometheus. The following infrastructure node size recommendations are based on the results observed in cluster-density testing detailed in the <span class="strong strong"><strong>Control plane node sizing</strong></span> section, where the monitoring stack and the default ingress-controller were moved to these nodes.
				</p><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735340224864" scope="col">Number of worker nodes</th><th align="left" valign="top" id="idm139735340223776" scope="col">Cluster density, or number of namespaces</th><th align="left" valign="top" id="idm139735339735408" scope="col">CPU cores</th><th align="left" valign="top" id="idm139735339734320" scope="col">Memory (GB)</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735340224864"> <p>
									27
								</p>
								 </td><td align="left" valign="top" headers="idm139735340223776"> <p>
									500
								</p>
								 </td><td align="left" valign="top" headers="idm139735339735408"> <p>
									4
								</p>
								 </td><td align="left" valign="top" headers="idm139735339734320"> <p>
									24
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735340224864"> <p>
									120
								</p>
								 </td><td align="left" valign="top" headers="idm139735340223776"> <p>
									1000
								</p>
								 </td><td align="left" valign="top" headers="idm139735339735408"> <p>
									8
								</p>
								 </td><td align="left" valign="top" headers="idm139735339734320"> <p>
									48
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735340224864"> <p>
									252
								</p>
								 </td><td align="left" valign="top" headers="idm139735340223776"> <p>
									4000
								</p>
								 </td><td align="left" valign="top" headers="idm139735339735408"> <p>
									16
								</p>
								 </td><td align="left" valign="top" headers="idm139735339734320"> <p>
									128
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735340224864"> <p>
									501
								</p>
								 </td><td align="left" valign="top" headers="idm139735340223776"> <p>
									4000
								</p>
								 </td><td align="left" valign="top" headers="idm139735339735408"> <p>
									32
								</p>
								 </td><td align="left" valign="top" headers="idm139735339734320"> <p>
									128
								</p>
								 </td></tr></tbody></table></div><p>
					In general, three infrastructure nodes are recommended per cluster.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						These sizing recommendations should be used as a guideline. Prometheus is a highly memory intensive application; the resource usage depends on various factors including the number of nodes, objects, the Prometheus metrics scraping interval, metrics or time series, and the age of the cluster. In addition, the router resource usage can also be affected by the number of routes and the amount/type of inbound requests.
					</p><p>
						These recommendations apply only to infrastructure nodes hosting Monitoring, Ingress and Registry infrastructure components installed during cluster creation.
					</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						In OpenShift Container Platform 4.13, half of a CPU core (500 millicore) is now reserved by the system by default compared to OpenShift Container Platform 3.11 and previous versions. This influences the stated sizing recommendations.
					</p></div></div></section><section class="section" id="scaling-cluster-monitoring-operator_recommended-infrastructure-practices"><div class="titlepage"><div><div><h3 class="title">1.2.2. Scaling the Cluster Monitoring Operator</h3></div></div></div><p>
					OpenShift Container Platform exposes metrics that the Cluster Monitoring Operator collects and stores in the Prometheus-based monitoring stack. As an administrator, you can view dashboards for system resources, containers, and components metrics in the OpenShift Container Platform web console by navigating to <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>Dashboards</strong></span>.
				</p></section><section class="section" id="prometheus-database-storage-requirements_recommended-infrastructure-practices"><div class="titlepage"><div><div><h3 class="title">1.2.3. Prometheus database storage requirements</h3></div></div></div><p>
					Red Hat performed various tests for different scale sizes.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The Prometheus storage requirements below are not prescriptive and should be used as a reference. Higher resource consumption might be observed in your cluster depending on workload activity and resource density, including the number of pods, containers, routes, or other resources exposing metrics collected by Prometheus.
					</p></div></div><div class="table" id="idm139735355130544"><p class="title"><strong>Table 1.1. Prometheus Database storage requirements based on number of nodes/pods in the cluster</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 20%; " class="col_3"><!--Empty--></col><col style="width: 20%; " class="col_4"><!--Empty--></col><col style="width: 20%; " class="col_5"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735351464544" scope="col">Number of Nodes</th><th align="left" valign="top" id="idm139735351463456" scope="col">Number of pods (2 containers per pod)</th><th align="left" valign="top" id="idm139735351462400" scope="col">Prometheus storage growth per day</th><th align="left" valign="top" id="idm139735351461344" scope="col">Prometheus storage growth per 15 days</th><th align="left" valign="top" id="idm139735342880144" scope="col">Network (per tsdb chunk)</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735351464544"> <p>
									50
								</p>
								 </td><td align="left" valign="top" headers="idm139735351463456"> <p>
									1800
								</p>
								 </td><td align="left" valign="top" headers="idm139735351462400"> <p>
									6.3 GB
								</p>
								 </td><td align="left" valign="top" headers="idm139735351461344"> <p>
									94 GB
								</p>
								 </td><td align="left" valign="top" headers="idm139735342880144"> <p>
									16 MB
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735351464544"> <p>
									100
								</p>
								 </td><td align="left" valign="top" headers="idm139735351463456"> <p>
									3600
								</p>
								 </td><td align="left" valign="top" headers="idm139735351462400"> <p>
									13 GB
								</p>
								 </td><td align="left" valign="top" headers="idm139735351461344"> <p>
									195 GB
								</p>
								 </td><td align="left" valign="top" headers="idm139735342880144"> <p>
									26 MB
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735351464544"> <p>
									150
								</p>
								 </td><td align="left" valign="top" headers="idm139735351463456"> <p>
									5400
								</p>
								 </td><td align="left" valign="top" headers="idm139735351462400"> <p>
									19 GB
								</p>
								 </td><td align="left" valign="top" headers="idm139735351461344"> <p>
									283 GB
								</p>
								 </td><td align="left" valign="top" headers="idm139735342880144"> <p>
									36 MB
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735351464544"> <p>
									200
								</p>
								 </td><td align="left" valign="top" headers="idm139735351463456"> <p>
									7200
								</p>
								 </td><td align="left" valign="top" headers="idm139735351462400"> <p>
									25 GB
								</p>
								 </td><td align="left" valign="top" headers="idm139735351461344"> <p>
									375 GB
								</p>
								 </td><td align="left" valign="top" headers="idm139735342880144"> <p>
									46 MB
								</p>
								 </td></tr></tbody></table></div></div><p>
					Approximately 20 percent of the expected size was added as overhead to ensure that the storage requirements do not exceed the calculated value.
				</p><p>
					The above calculation is for the default OpenShift Container Platform Cluster Monitoring Operator.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						CPU utilization has minor impact. The ratio is approximately 1 core out of 40 per 50 nodes and 1800 pods.
					</p></div></div><p>
					<span class="strong strong"><strong>Recommendations for OpenShift Container Platform</strong></span>
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Use at least two infrastructure (infra) nodes.
						</li><li class="listitem">
							Use at least three <span class="strong strong"><strong>openshift-container-storage</strong></span> nodes with non-volatile memory express (SSD or NVMe) drives.
						</li></ul></div></section><section class="section" id="configuring-cluster-monitoring_recommended-infrastructure-practices"><div class="titlepage"><div><div><h3 class="title">1.2.4. Configuring cluster monitoring</h3></div></div></div><p class="_abstract _abstract">
					You can increase the storage capacity for the Prometheus component in the cluster monitoring stack.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To increase the storage capacity for Prometheus:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a YAML configuration file, <code class="literal">cluster-monitoring-config.yaml</code>. For example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
data:
  config.yaml: |
    prometheusK8s:
      retention: {{PROMETHEUS_RETENTION_PERIOD}} <span id="CO2-1"><!--Empty--></span><span class="callout">1</span>
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      volumeClaimTemplate:
        spec:
          storageClassName: {{STORAGE_CLASS}} <span id="CO2-2"><!--Empty--></span><span class="callout">2</span>
          resources:
            requests:
              storage: {{PROMETHEUS_STORAGE_SIZE}} <span id="CO2-3"><!--Empty--></span><span class="callout">3</span>
    alertmanagerMain:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      volumeClaimTemplate:
        spec:
          storageClassName: {{STORAGE_CLASS}} <span id="CO2-4"><!--Empty--></span><span class="callout">4</span>
          resources:
            requests:
              storage: {{ALERTMANAGER_STORAGE_SIZE}} <span id="CO2-5"><!--Empty--></span><span class="callout">5</span>
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO2-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The default value of Prometheus retention is <code class="literal">PROMETHEUS_RETENTION_PERIOD=15d</code>. Units are measured in time using one of these suffixes: s, m, h, d.
								</div></dd><dt><a href="#CO2-2"><span class="callout">2</span></a> <a href="#CO2-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The storage class for your cluster.
								</div></dd><dt><a href="#CO2-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									A typical value is <code class="literal">PROMETHEUS_STORAGE_SIZE=2000Gi</code>. Storage values can be a plain integer or a fixed-point integer using one of these suffixes: E, P, T, G, M, K. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki.
								</div></dd><dt><a href="#CO2-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									A typical value is <code class="literal">ALERTMANAGER_STORAGE_SIZE=20Gi</code>. Storage values can be a plain integer or a fixed-point integer using one of these suffixes: E, P, T, G, M, K. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki.
								</div></dd></dl></div></li><li class="listitem">
							Add values for the retention period, storage class, and storage sizes.
						</li><li class="listitem">
							Save the file.
						</li><li class="listitem"><p class="simpara">
							Apply the changes by running:
						</p><pre class="programlisting language-terminal">$ oc create -f cluster-monitoring-config.yaml</pre></li></ol></div></section><section class="section _additional-resources" id="additional-resources"><div class="titlepage"><div><div><h3 class="title">1.2.5. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/solutions/5034771">Infrastructure Nodes in OpenShift 4</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#planning-your-environment-according-to-object-maximums">OpenShift Container Platform cluster maximums</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#creating-infrastructure-machinesets">Creating infrastructure machine sets</a>
						</li></ul></div></section></section><section class="section" id="recommended-etcd-practices"><div class="titlepage"><div><div><h2 class="title">1.3. Recommended etcd practices</h2></div></div></div><p>
				This topic provides recommended performance and scalability practices for etcd in OpenShift Container Platform.
			</p><section class="section" id="recommended-etcd-practices_recommended-etcd-practices"><div class="titlepage"><div><div><h3 class="title">1.3.1. Recommended etcd practices</h3></div></div></div><p>
					Because etcd writes data to disk and persists proposals on disk, its performance depends on disk performance. Although etcd is not particularly I/O intensive, it requires a low latency block device for optimal performance and stability. Because etcd’s consensus protocol depends on persistently storing metadata to a log (WAL), etcd is sensitive to disk-write latency. Slow disks and disk activity from other processes can cause long fsync latencies.
				</p><p>
					Those latencies can cause etcd to miss heartbeats, not commit new proposals to the disk on time, and ultimately experience request timeouts and temporary leader loss. High write latencies also lead to an OpenShift API slowness, which affects cluster performance. Because of these reasons, avoid colocating other workloads on the control-plane nodes that are I/O sensitive or intensive and share the same underlying I/O infrastructure.
				</p><p>
					In terms of latency, run etcd on top of a block device that can write at least 50 IOPS of 8000 bytes long sequentially. That is, with a latency of 10ms, keep in mind that uses fdatasync to synchronize each write in the WAL. For heavy loaded clusters, sequential 500 IOPS of 8000 bytes (2 ms) are recommended. To measure those numbers, you can use a benchmarking tool, such as fio.
				</p><p>
					To achieve such performance, run etcd on machines that are backed by SSD or NVMe disks with low latency and high throughput. Consider single-level cell (SLC) solid-state drives (SSDs), which provide 1 bit per memory cell, are durable and reliable, and are ideal for write-intensive workloads.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The load on etcd arises from static factors, such as the number of nodes and pods, and dynamic factors, including changes in endpoints due to pod autoscaling, pod restarts, job executions, and other workload-related events. To accurately size your etcd setup, you must analyze the specific requirements of your workload. Consider the number of nodes, pods, and other relevant factors that impact the load on etcd.
					</p></div></div><p>
					The following hard disk features provide optimal etcd performance:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Low latency to support fast read operation.
						</li><li class="listitem">
							High-bandwidth writes for faster compactions and defragmentation.
						</li><li class="listitem">
							High-bandwidth reads for faster recovery from failures.
						</li><li class="listitem">
							Solid state drives as a minimum selection, however NVMe drives are preferred.
						</li><li class="listitem">
							Server-grade hardware from various manufacturers for increased reliability.
						</li><li class="listitem">
							RAID 0 technology for increased performance.
						</li><li class="listitem">
							Dedicated etcd drives. Do not place log files or other heavy workloads on etcd drives.
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Avoid NAS or SAN setups and spinning drives. Ceph Rados Block Device (RBD) and other types of network-attached storage can result in unpredictable network latency. To provide fast storage to etcd nodes at scale, use PCI passthrough to pass NVM devices directly to the nodes.
					</p></div></div><p>
					Always benchmark by using utilities such as fio. You can use such utilities to continuously monitor the cluster performance as it increases.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Avoid using the Network File System (NFS) protocol or other network based file systems.
					</p></div></div><p>
					Some key metrics to monitor on a deployed OpenShift Container Platform cluster are p99 of etcd disk write ahead log duration and the number of etcd leader changes. Use Prometheus to track these metrics.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The etcd member database sizes can vary in a cluster during normal operations. This difference does not affect cluster upgrades, even if the leader size is different from the other members.
					</p></div></div><p>
					To validate the hardware for etcd before or after you create the OpenShift Container Platform cluster, you can use fio.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Container runtimes such as Podman or Docker are installed on the machine that you’re testing.
						</li><li class="listitem">
							Data is written to the <code class="literal">/var/lib/etcd</code> path.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Run fio and analyze the results:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p class="simpara">
									If you use Podman, run this command:
								</p><pre class="programlisting language-terminal">$ sudo podman run --volume /var/lib/etcd:/var/lib/etcd:Z quay.io/openshift-scale/etcd-perf</pre></li><li class="listitem"><p class="simpara">
									If you use Docker, run this command:
								</p><pre class="programlisting language-terminal">$ sudo docker run --volume /var/lib/etcd:/var/lib/etcd:Z quay.io/openshift-scale/etcd-perf</pre></li></ul></div></li></ul></div><p>
					The output reports whether the disk is fast enough to host etcd by comparing the 99th percentile of the fsync metric captured from the run to see if it is less than 10 ms. A few of the most important etcd metrics that might affected by I/O performance are as follow:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">etcd_disk_wal_fsync_duration_seconds_bucket</code> metric reports the etcd’s WAL fsync duration
						</li><li class="listitem">
							<code class="literal">etcd_disk_backend_commit_duration_seconds_bucket</code> metric reports the etcd backend commit latency duration
						</li><li class="listitem">
							<code class="literal">etcd_server_leader_changes_seen_total</code> metric reports the leader changes
						</li></ul></div><p>
					Because etcd replicates the requests among all the members, its performance strongly depends on network input/output (I/O) latency. High network latencies result in etcd heartbeats taking longer than the election timeout, which results in leader elections that are disruptive to the cluster. A key metric to monitor on a deployed OpenShift Container Platform cluster is the 99th percentile of etcd network peer latency on each etcd cluster member. Use Prometheus to track the metric.
				</p><p>
					The <code class="literal">histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket[2m]))</code> metric reports the round trip time for etcd to finish replicating the client requests between the members. Ensure that it is less than 50 ms.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/solutions/4885641">How to use <code class="literal">fio</code> to check etcd disk performance in OpenShift Container Platform</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/articles/6271341">etcd performance troubleshooting guide for OpenShift Container Platform</a>
						</li></ul></div></section><section class="section" id="move-etcd-different-disk_recommended-etcd-practices"><div class="titlepage"><div><div><h3 class="title">1.3.2. Moving etcd to a different disk</h3></div></div></div><p>
					You can move etcd from a shared disk to a separate disk to prevent or resolve performance issues.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The <code class="literal">MachineConfigPool</code> must match <code class="literal">metadata.labels[machineconfiguration.openshift.io/role]</code>. This applies to a controller, worker, or a custom pool.
						</li><li class="listitem">
							The node’s auxiliary storage device, such as <code class="literal">/dev/sdb</code>, must match the sdb. Change this reference in all places in the file.
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						This procedure does not move parts of the root file system, such as <code class="literal">/var/</code>, to another disk or partition on an installed node.
					</p></div></div><p>
					The Machine Config Operator (MCO) is responsible for mounting a secondary disk for an OpenShift Container Platform 4.13 container storage.
				</p><p>
					Use the following steps to move etcd to a different device:
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">machineconfig</code> YAML file named <code class="literal">etcd-mc.yml</code> and add the following information:
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 98-var-lib-etcd
spec:
  config:
    ignition:
      version: 3.2.0
    systemd:
      units:
      - contents: |
          [Unit]
          Description=Make File System on /dev/sdb
          DefaultDependencies=no
          BindsTo=dev-sdb.device
          After=dev-sdb.device var.mount
          Before=systemd-fsck@dev-sdb.service

          [Service]
          Type=oneshot
          RemainAfterExit=yes
          ExecStart=/usr/lib/systemd/systemd-makefs xfs /dev/sdb
          TimeoutSec=0

          [Install]
          WantedBy=var-lib-containers.mount
        enabled: true
        name: systemd-mkfs@dev-sdb.service
      - contents: |
          [Unit]
          Description=Mount /dev/sdb to /var/lib/etcd
          Before=local-fs.target
          Requires=systemd-mkfs@dev-sdb.service
          After=systemd-mkfs@dev-sdb.service var.mount

          [Mount]
          What=/dev/sdb
          Where=/var/lib/etcd
          Type=xfs
          Options=defaults,prjquota

          [Install]
          WantedBy=local-fs.target
        enabled: true
        name: var-lib-etcd.mount
      - contents: |
          [Unit]
          Description=Sync etcd data if new mount is empty
          DefaultDependencies=no
          After=var-lib-etcd.mount var.mount
          Before=crio.service

          [Service]
          Type=oneshot
          RemainAfterExit=yes
          ExecCondition=/usr/bin/test ! -d /var/lib/etcd/member
          ExecStart=/usr/sbin/setenforce 0
          ExecStart=/bin/rsync -ar /sysroot/ostree/deploy/rhcos/var/lib/etcd/ /var/lib/etcd/
          ExecStart=/usr/sbin/setenforce 1
          TimeoutSec=0

          [Install]
          WantedBy=multi-user.target graphical.target
        enabled: true
        name: sync-var-lib-etcd-to-etcd.service
      - contents: |
          [Unit]
          Description=Restore recursive SELinux security contexts
          DefaultDependencies=no
          After=var-lib-etcd.mount
          Before=crio.service

          [Service]
          Type=oneshot
          RemainAfterExit=yes
          ExecStart=/sbin/restorecon -R /var/lib/etcd/
          TimeoutSec=0

          [Install]
          WantedBy=multi-user.target graphical.target
        enabled: true
        name: restorecon-var-lib-etcd.service</pre></li><li class="listitem"><p class="simpara">
							Create the machine configuration by entering the following commands:
						</p><pre class="programlisting language-terminal">$ oc login -u ${ADMIN} -p ${ADMINPASSWORD} ${API}
... output omitted ...</pre><pre class="programlisting language-terminal">$ oc create -f etcd-mc.yml
machineconfig.machineconfiguration.openshift.io/98-var-lib-etcd created</pre><pre class="programlisting language-terminal">$ oc login -u ${ADMIN} -p ${ADMINPASSWORD} ${API}
 [... output omitted ...]</pre><pre class="programlisting language-terminal">$ oc create -f etcd-mc.yml machineconfig.machineconfiguration.openshift.io/98-var-lib-etcd created</pre><p class="simpara">
							The nodes are updated and rebooted. After the reboot completes, the following events occur:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									An XFS file system is created on the specified disk.
								</li><li class="listitem">
									The disk mounts to <code class="literal">/var/lib/etc</code>.
								</li><li class="listitem">
									The content from <code class="literal">/sysroot/ostree/deploy/rhcos/var/lib/etcd</code> syncs to <code class="literal">/var/lib/etcd</code>.
								</li><li class="listitem">
									A restore of <code class="literal">SELinux</code> labels is forced for <code class="literal">/var/lib/etcd</code>.
								</li><li class="listitem">
									The old content is not removed.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							After the nodes are on a separate disk, update the machine configuration file, <code class="literal">etcd-mc.yml</code> with the following information:
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 98-var-lib-etcd
spec:
  config:
    ignition:
      version: 3.2.0
    systemd:
      units:
      - contents: |
          [Unit]
          Description=Mount /dev/sdb to /var/lib/etcd
          Before=local-fs.target
          Requires=systemd-mkfs@dev-sdb.service
          After=systemd-mkfs@dev-sdb.service var.mount

          [Mount]
          What=/dev/sdb
          Where=/var/lib/etcd
          Type=xfs
          Options=defaults,prjquota

          [Install]
          WantedBy=local-fs.target
        enabled: true
        name: var-lib-etcd.mount</pre></li><li class="listitem"><p class="simpara">
							Apply the modified version that removes the logic for creating and syncing the device by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc replace -f etcd-mc.yml</pre><p class="simpara">
							The previous step prevents the nodes from rebooting.
						</p></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://docs.openshift.com/container-platform/4.11/architecture/architecture-rhcos.html">Red Hat Enterprise Linux CoreOS (RHCOS)</a>
						</li></ul></div></section><section class="section" id="etcd-defrag_recommended-etcd-practices"><div class="titlepage"><div><div><h3 class="title">1.3.3. Defragmenting etcd data</h3></div></div></div><p>
					For large and dense clusters, etcd can suffer from poor performance if the keyspace grows too large and exceeds the space quota. Periodically maintain and defragment etcd to free up space in the data store. Monitor Prometheus for etcd metrics and defragment it when required; otherwise, etcd can raise a cluster-wide alarm that puts the cluster into a maintenance mode that accepts only key reads and deletes.
				</p><p>
					Monitor these key metrics:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">etcd_server_quota_backend_bytes</code>, which is the current quota limit
						</li><li class="listitem">
							<code class="literal">etcd_mvcc_db_total_size_in_use_in_bytes</code>, which indicates the actual database usage after a history compaction
						</li><li class="listitem">
							<code class="literal">etcd_mvcc_db_total_size_in_bytes</code>, which shows the database size, including free space waiting for defragmentation
						</li></ul></div><p>
					Defragment etcd data to reclaim disk space after events that cause disk fragmentation, such as etcd history compaction.
				</p><p>
					History compaction is performed automatically every five minutes and leaves gaps in the back-end database. This fragmented space is available for use by etcd, but is not available to the host file system. You must defragment etcd to make this space available to the host file system.
				</p><p>
					Defragmentation occurs automatically, but you can also trigger it manually.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Automatic defragmentation is good for most cases, because the etcd operator uses cluster information to determine the most efficient operation for the user.
					</p></div></div><section class="section" id="automatic-defrag-etcd-data_recommended-etcd-practices"><div class="titlepage"><div><div><h4 class="title">1.3.3.1. Automatic defragmentation</h4></div></div></div><p>
						The etcd Operator automatically defragments disks. No manual intervention is needed.
					</p><p>
						Verify that the defragmentation process is successful by viewing one of these logs:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								etcd logs
							</li><li class="listitem">
								cluster-etcd-operator pod
							</li><li class="listitem">
								operator status error log
							</li></ul></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
							Automatic defragmentation can cause leader election failure in various OpenShift core components, such as the Kubernetes controller manager, which triggers a restart of the failing component. The restart is harmless and either triggers failover to the next running instance or the component resumes work again after the restart.
						</p></div></div><div class="formalpara"><p class="title"><strong>Example log output for successful defragmentation</strong></p><p>
							
<pre class="programlisting language-terminal">etcd member has been defragmented: <span class="emphasis"><em>&lt;member_name&gt;</em></span>, memberID: <span class="emphasis"><em>&lt;member_id&gt;</em></span></pre>

						</p></div><div class="formalpara"><p class="title"><strong>Example log output for unsuccessful defragmentation</strong></p><p>
							
<pre class="programlisting language-terminal">failed defrag on member: <span class="emphasis"><em>&lt;member_name&gt;</em></span>, memberID: <span class="emphasis"><em>&lt;member_id&gt;</em></span>: <span class="emphasis"><em>&lt;error_message&gt;</em></span></pre>

						</p></div></section><section class="section" id="manual-defrag-etcd-data_recommended-etcd-practices"><div class="titlepage"><div><div><h4 class="title">1.3.3.2. Manual defragmentation</h4></div></div></div><p>
						A Prometheus alert indicates when you need to use manual defragmentation. The alert is displayed in two cases:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								When etcd uses more than 50% of its available space for more than 10 minutes
							</li><li class="listitem">
								When etcd is actively using less than 50% of its total database size for more than 10 minutes
							</li></ul></div><p>
						You can also determine whether defragmentation is needed by checking the etcd database size in MB that will be freed by defragmentation with the PromQL expression: <code class="literal">(etcd_mvcc_db_total_size_in_bytes - etcd_mvcc_db_total_size_in_use_in_bytes)/1024/1024</code>
					</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
							Defragmenting etcd is a blocking action. The etcd member will not respond until defragmentation is complete. For this reason, wait at least one minute between defragmentation actions on each of the pods to allow the cluster to recover.
						</p></div></div><p>
						Follow this procedure to defragment etcd data on each etcd member.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Determine which etcd member is the leader, because the leader should be defragmented last.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Get the list of etcd pods:
									</p><pre class="programlisting language-terminal">$ oc -n openshift-etcd get pods -l k8s-app=etcd -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">etcd-ip-10-0-159-225.example.redhat.com                3/3     Running     0          175m   10.0.159.225   ip-10-0-159-225.example.redhat.com   &lt;none&gt;           &lt;none&gt;
etcd-ip-10-0-191-37.example.redhat.com                 3/3     Running     0          173m   10.0.191.37    ip-10-0-191-37.example.redhat.com    &lt;none&gt;           &lt;none&gt;
etcd-ip-10-0-199-170.example.redhat.com                3/3     Running     0          176m   10.0.199.170   ip-10-0-199-170.example.redhat.com   &lt;none&gt;           &lt;none&gt;</pre>

										</p></div></li><li class="listitem"><p class="simpara">
										Choose a pod and run the following command to determine which etcd member is the leader:
									</p><pre class="programlisting language-terminal">$ oc rsh -n openshift-etcd etcd-ip-10-0-159-225.example.redhat.com etcdctl endpoint status --cluster -w table</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">Defaulting container name to etcdctl.
Use 'oc describe pod/etcd-ip-10-0-159-225.example.redhat.com -n openshift-etcd' to see all of the containers in this pod.
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|  https://10.0.191.37:2379 | 251cd44483d811c3 |   3.5.9 |  104 MB |     false |      false |         7 |      91624 |              91624 |        |
| https://10.0.159.225:2379 | 264c7c58ecbdabee |   3.5.9 |  104 MB |     false |      false |         7 |      91624 |              91624 |        |
| https://10.0.199.170:2379 | 9ac311f93915cc79 |   3.5.9 |  104 MB |      true |      false |         7 |      91624 |              91624 |        |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</pre>

										</p></div><p class="simpara">
										Based on the <code class="literal">IS LEADER</code> column of this output, the <code class="literal">https://10.0.199.170:2379</code> endpoint is the leader. Matching this endpoint with the output of the previous step, the pod name of the leader is <code class="literal">etcd-ip-10-0-199-170.example.redhat.com</code>.
									</p></li></ol></div></li><li class="listitem"><p class="simpara">
								Defragment an etcd member.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Connect to the running etcd container, passing in the name of a pod that is <span class="emphasis"><em>not</em></span> the leader:
									</p><pre class="programlisting language-terminal">$ oc rsh -n openshift-etcd etcd-ip-10-0-159-225.example.redhat.com</pre></li><li class="listitem"><p class="simpara">
										Unset the <code class="literal">ETCDCTL_ENDPOINTS</code> environment variable:
									</p><pre class="programlisting language-terminal">sh-4.4# unset ETCDCTL_ENDPOINTS</pre></li><li class="listitem"><p class="simpara">
										Defragment the etcd member:
									</p><pre class="programlisting language-terminal">sh-4.4# etcdctl --command-timeout=30s --endpoints=https://localhost:2379 defrag</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">Finished defragmenting etcd member[https://localhost:2379]</pre>

										</p></div><p class="simpara">
										If a timeout error occurs, increase the value for <code class="literal">--command-timeout</code> until the command succeeds.
									</p></li><li class="listitem"><p class="simpara">
										Verify that the database size was reduced:
									</p><pre class="programlisting language-terminal">sh-4.4# etcdctl endpoint status -w table --cluster</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|  https://10.0.191.37:2379 | 251cd44483d811c3 |   3.5.9 |  104 MB |     false |      false |         7 |      91624 |              91624 |        |
| https://10.0.159.225:2379 | 264c7c58ecbdabee |   3.5.9 |   41 MB |     false |      false |         7 |      91624 |              91624 |        | <span id="CO3-1"><!--Empty--></span><span class="callout">1</span>
| https://10.0.199.170:2379 | 9ac311f93915cc79 |   3.5.9 |  104 MB |      true |      false |         7 |      91624 |              91624 |        |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</pre>

										</p></div><p class="simpara">
										This example shows that the database size for this etcd member is now 41 MB as opposed to the starting size of 104 MB.
									</p></li><li class="listitem"><p class="simpara">
										Repeat these steps to connect to each of the other etcd members and defragment them. Always defragment the leader last.
									</p><p class="simpara">
										Wait at least one minute between defragmentation actions to allow the etcd pod to recover. Until the etcd pod recovers, the etcd member will not respond.
									</p></li></ol></div></li><li class="listitem"><p class="simpara">
								If any <code class="literal">NOSPACE</code> alarms were triggered due to the space quota being exceeded, clear them.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Check if there are any <code class="literal">NOSPACE</code> alarms:
									</p><pre class="programlisting language-terminal">sh-4.4# etcdctl alarm list</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">memberID:12345678912345678912 alarm:NOSPACE</pre>

										</p></div></li><li class="listitem"><p class="simpara">
										Clear the alarms:
									</p><pre class="programlisting language-terminal">sh-4.4# etcdctl alarm disarm</pre></li></ol></div></li></ol></div></section></section></section></section><section class="chapter" id="planning-your-environment-according-to-object-maximums"><div class="titlepage"><div><div><h1 class="title">Chapter 2. Planning your environment according to object maximums</h1></div></div></div><p>
			Consider the following tested object maximums when you plan your OpenShift Container Platform cluster.
		</p><p>
			These guidelines are based on the largest possible cluster. For smaller clusters, the maximums are lower. There are many factors that influence the stated thresholds, including the etcd version or storage data format.
		</p><p>
			In most cases, exceeding these numbers results in lower overall performance. It does not necessarily mean that the cluster will fail.
		</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
				Clusters that experience rapid change, such as those with many starting and stopping pods, can have a lower practical maximum size than documented.
			</p></div></div><section class="section" id="cluster-maximums-major-releases_object-limits"><div class="titlepage"><div><div><h2 class="title">2.1. OpenShift Container Platform tested cluster maximums for major releases</h2></div></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Red Hat does not provide direct guidance on sizing your OpenShift Container Platform cluster. This is because determining whether your cluster is within the supported bounds of OpenShift Container Platform requires careful consideration of all the multidimensional factors that limit the cluster scale.
				</p></div></div><p>
				OpenShift Container Platform supports tested cluster maximums rather than absolute cluster maximums. Not every combination of OpenShift Container Platform version, control plane workload, and network plugin are tested, so the following table does not represent an absolute expectation of scale for all deployments. It might not be possible to scale to a maximum on all dimensions simultaneously. The table contains tested maximums for specific workload and deployment configurations, and serves as a scale guide as to what can be expected with similar deployments.
			</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735342308112" scope="col">Maximum type</th><th align="left" valign="top" id="idm139735342307024" scope="col">4.x tested maximum</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735342308112"> <p>
								Number of nodes
							</p>
							 </td><td align="left" valign="top" headers="idm139735342307024"> <p>
								2,000 <sup>[1]</sup>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342308112"> <p>
								Number of pods <sup>[2]</sup>
							</p>
							 </td><td align="left" valign="top" headers="idm139735342307024"> <p>
								150,000
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342308112"> <p>
								Number of pods per node
							</p>
							 </td><td align="left" valign="top" headers="idm139735342307024"> <p>
								2,500 <sup>[3][4]</sup>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342308112"> <p>
								Number of pods per core
							</p>
							 </td><td align="left" valign="top" headers="idm139735342307024"> <p>
								There is no default value.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342308112"> <p>
								Number of namespaces <sup>[5]</sup>
							</p>
							 </td><td align="left" valign="top" headers="idm139735342307024"> <p>
								10,000
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342308112"> <p>
								Number of builds
							</p>
							 </td><td align="left" valign="top" headers="idm139735342307024"> <p>
								10,000 (Default pod RAM 512 Mi) - Source-to-Image (S2I) build strategy
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342308112"> <p>
								Number of pods per namespace <sup>[6]</sup>
							</p>
							 </td><td align="left" valign="top" headers="idm139735342307024"> <p>
								25,000
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342308112"> <p>
								Number of routes and back ends per Ingress Controller
							</p>
							 </td><td align="left" valign="top" headers="idm139735342307024"> <p>
								2,000 per router
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342308112"> <p>
								Number of secrets
							</p>
							 </td><td align="left" valign="top" headers="idm139735342307024"> <p>
								80,000
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342308112"> <p>
								Number of config maps
							</p>
							 </td><td align="left" valign="top" headers="idm139735342307024"> <p>
								90,000
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342308112"> <p>
								Number of services <sup>[7]</sup>
							</p>
							 </td><td align="left" valign="top" headers="idm139735342307024"> <p>
								10,000
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342308112"> <p>
								Number of services per namespace
							</p>
							 </td><td align="left" valign="top" headers="idm139735342307024"> <p>
								5,000
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342308112"> <p>
								Number of back-ends per service
							</p>
							 </td><td align="left" valign="top" headers="idm139735342307024"> <p>
								5,000
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342308112"> <p>
								Number of deployments per namespace <sup>[6]</sup>
							</p>
							 </td><td align="left" valign="top" headers="idm139735342307024"> <p>
								2,000
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342308112"> <p>
								Number of build configs
							</p>
							 </td><td align="left" valign="top" headers="idm139735342307024"> <p>
								12,000
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342308112"> <p>
								Number of custom resource definitions (CRD)
							</p>
							 </td><td align="left" valign="top" headers="idm139735342307024"> <p>
								512 <sup>[8]</sup>
							</p>
							 </td></tr></tbody></table></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Pause pods were deployed to stress the control plane components of OpenShift Container Platform at 2000 node scale. The ability to scale to similar numbers will vary depending upon specific deployment and workload parameters.
					</li><li class="listitem">
						The pod count displayed here is the number of test pods. The actual number of pods depends on the application’s memory, CPU, and storage requirements.
					</li><li class="listitem">
						This was tested on a cluster with 31 servers: 3 control planes, 2 infrastructure nodes, and 26 worker nodes. If you need 2,500 user pods, you need both a <code class="literal">hostPrefix</code> of <code class="literal">20</code>, which allocates a network large enough for each node to contain more than 2000 pods, and a custom kubelet config with <code class="literal">maxPods</code> set to <code class="literal">2500</code>. For more information, see <a class="link" href="https://cloud.redhat.com/blog/running-2500-pods-per-node-on-ocp-4.13">Running 2500 pods per node on OCP 4.13</a>.
					</li><li class="listitem">
						The maximum tested pods per node is 2,500 for clusters using the <code class="literal">OVNKubernetes</code> network plugin. The maximum tested pods per node for the <code class="literal">OpenShiftSDN</code> network plugin is 500 pods.
					</li><li class="listitem">
						When there are a large number of active projects, etcd might suffer from poor performance if the keyspace grows excessively large and exceeds the space quota. Periodic maintenance of etcd, including defragmentation, is highly recommended to free etcd storage.
					</li><li class="listitem">
						There are several control loops in the system that must iterate over all objects in a given namespace as a reaction to some changes in state. Having a large number of objects of a given type in a single namespace can make those loops expensive and slow down processing given state changes. The limit assumes that the system has enough CPU, memory, and disk to satisfy the application requirements.
					</li><li class="listitem">
						Each service port and each service back-end has a corresponding entry in <code class="literal">iptables</code>. The number of back-ends of a given service impact the size of the <code class="literal">Endpoints</code> objects, which impacts the size of data that is being sent all over the system.
					</li><li class="listitem">
						OpenShift Container Platform has a limit of 512 total custom resource definitions (CRD), including those installed by OpenShift Container Platform, products integrating with OpenShift Container Platform and user-created CRDs. If there are more than 512 CRDs created, then there is a possibility that <code class="literal">oc</code> command requests might be throttled.
					</li></ol></div><section class="section" id="cluster-maximums-major-releases-example-scenario_object-limits"><div class="titlepage"><div><div><h3 class="title">2.1.1. Example scenario</h3></div></div></div><p>
					As an example, 500 worker nodes (m5.2xl) were tested, and are supported, using OpenShift Container Platform 4.13, the OVN-Kubernetes network plugin, and the following workload objects:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							200 namespaces, in addition to the defaults
						</li><li class="listitem">
							60 pods per node; 30 server and 30 client pods (30k total)
						</li><li class="listitem">
							57 image streams/ns (11.4k total)
						</li><li class="listitem">
							15 services/ns backed by the server pods (3k total)
						</li><li class="listitem">
							15 routes/ns backed by the previous services (3k total)
						</li><li class="listitem">
							20 secrets/ns (4k total)
						</li><li class="listitem">
							10 config maps/ns (2k total)
						</li><li class="listitem">
							6 network policies/ns, including deny-all, allow-from ingress and intra-namespace rules
						</li><li class="listitem">
							57 builds/ns
						</li></ul></div><p>
					The following factors are known to affect cluster workload scaling, positively or negatively, and should be factored into the scale numbers when planning a deployment. For additional information and guidance, contact your sales representative or <a class="link" href="https://access.redhat.com/support/">Red Hat support</a>.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Number of pods per node
						</li><li class="listitem">
							Number of containers per pod
						</li><li class="listitem">
							Type of probes used (for example, liveness/readiness, exec/http)
						</li><li class="listitem">
							Number of network policies
						</li><li class="listitem">
							Number of projects, or namespaces
						</li><li class="listitem">
							Number of image streams per project
						</li><li class="listitem">
							Number of builds per project
						</li><li class="listitem">
							Number of services/endpoints and type
						</li><li class="listitem">
							Number of routes
						</li><li class="listitem">
							Number of shards
						</li><li class="listitem">
							Number of secrets
						</li><li class="listitem">
							Number of config maps
						</li><li class="listitem"><p class="simpara">
							Rate of API calls, or the cluster “churn”, which is an estimation of how quickly things change in the cluster configuration.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									Prometheus query for pod creation requests per second over 5 minute windows: <code class="literal">sum(irate(apiserver_request_count{resource="pods",verb="POST"}[5m]))</code>
								</li><li class="listitem">
									Prometheus query for all API requests per second over 5 minute windows: <code class="literal">sum(irate(apiserver_request_count{}[5m]))</code>
								</li></ul></div></li><li class="listitem">
							Cluster node resource consumption of CPU
						</li><li class="listitem">
							Cluster node resource consumption of memory
						</li></ul></div></section></section><section class="section" id="cluster-maximums-environment_object-limits"><div class="titlepage"><div><div><h2 class="title">2.2. OpenShift Container Platform environment and configuration on which the cluster maximums are tested</h2></div></div></div><section class="section" id="aws-cloud-platform"><div class="titlepage"><div><div><h3 class="title">2.2.1. AWS cloud platform</h3></div></div></div><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 13%; " class="col_1"><!--Empty--></col><col style="width: 13%; " class="col_2"><!--Empty--></col><col style="width: 13%; " class="col_3"><!--Empty--></col><col style="width: 13%; " class="col_4"><!--Empty--></col><col style="width: 13%; " class="col_5"><!--Empty--></col><col style="width: 13%; " class="col_6"><!--Empty--></col><col style="width: 13%; " class="col_7"><!--Empty--></col><col style="width: 13%; " class="col_8"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735322963648" scope="col">Node</th><th align="left" valign="top" id="idm139735322962560" scope="col">Flavor</th><th align="left" valign="top" id="idm139735322961472" scope="col">vCPU</th><th align="left" valign="top" id="idm139735322960384" scope="col">RAM(GiB)</th><th align="left" valign="top" id="idm139735341795824" scope="col">Disk type</th><th align="left" valign="top" id="idm139735341794736" scope="col">Disk size(GiB)/IOS</th><th align="left" valign="top" id="idm139735341793648" scope="col">Count</th><th align="left" valign="top" id="idm139735341792560" scope="col">Region</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735322963648"> <p>
									Control plane/etcd <sup>[1]</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm139735322962560"> <p>
									r5.4xlarge
								</p>
								 </td><td align="left" valign="top" headers="idm139735322961472"> <p>
									16
								</p>
								 </td><td align="left" valign="top" headers="idm139735322960384"> <p>
									128
								</p>
								 </td><td align="left" valign="top" headers="idm139735341795824"> <p>
									gp3
								</p>
								 </td><td align="left" valign="top" headers="idm139735341794736"> <p>
									220
								</p>
								 </td><td align="left" valign="top" headers="idm139735341793648"> <p>
									3
								</p>
								 </td><td align="left" valign="top" headers="idm139735341792560"> <p>
									us-west-2
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735322963648"> <p>
									Infra <sup>[2]</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm139735322962560"> <p>
									m5.12xlarge
								</p>
								 </td><td align="left" valign="top" headers="idm139735322961472"> <p>
									48
								</p>
								 </td><td align="left" valign="top" headers="idm139735322960384"> <p>
									192
								</p>
								 </td><td align="left" valign="top" headers="idm139735341795824"> <p>
									gp3
								</p>
								 </td><td align="left" valign="top" headers="idm139735341794736"> <p>
									100
								</p>
								 </td><td align="left" valign="top" headers="idm139735341793648"> <p>
									3
								</p>
								 </td><td align="left" valign="top" headers="idm139735341792560"> <p>
									us-west-2
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735322963648"> <p>
									Workload <sup>[3]</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm139735322962560"> <p>
									m5.4xlarge
								</p>
								 </td><td align="left" valign="top" headers="idm139735322961472"> <p>
									16
								</p>
								 </td><td align="left" valign="top" headers="idm139735322960384"> <p>
									64
								</p>
								 </td><td align="left" valign="top" headers="idm139735341795824"> <p>
									gp3
								</p>
								 </td><td align="left" valign="top" headers="idm139735341794736"> <p>
									500 <sup>[4]</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm139735341793648"> <p>
									1
								</p>
								 </td><td align="left" valign="top" headers="idm139735341792560"> <p>
									us-west-2
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735322963648"> <p>
									Compute
								</p>
								 </td><td align="left" valign="top" headers="idm139735322962560"> <p>
									m5.2xlarge
								</p>
								 </td><td align="left" valign="top" headers="idm139735322961472"> <p>
									8
								</p>
								 </td><td align="left" valign="top" headers="idm139735322960384"> <p>
									32
								</p>
								 </td><td align="left" valign="top" headers="idm139735341795824"> <p>
									gp3
								</p>
								 </td><td align="left" valign="top" headers="idm139735341794736"> <p>
									100
								</p>
								 </td><td align="left" valign="top" headers="idm139735341793648"> <p>
									3/25/250/500 <sup>[5]</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm139735341792560"> <p>
									us-west-2
								</p>
								 </td></tr></tbody></table></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							gp3 disks with a baseline performance of 3000 IOPS and 125 MiB per second are used for control plane/etcd nodes because etcd is latency sensitive. gp3 volumes do not use burst performance.
						</li><li class="listitem">
							Infra nodes are used to host Monitoring, Ingress, and Registry components to ensure they have enough resources to run at large scale.
						</li><li class="listitem">
							Workload node is dedicated to run performance and scalability workload generators.
						</li><li class="listitem">
							Larger disk size is used so that there is enough space to store the large amounts of data that is collected during the performance and scalability test run.
						</li><li class="listitem">
							Cluster is scaled in iterations and performance and scalability tests are executed at the specified node counts.
						</li></ol></div></section><section class="section" id="ibm-power-platform"><div class="titlepage"><div><div><h3 class="title">2.2.2. IBM Power platform</h3></div></div></div><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 17%; " class="col_1"><!--Empty--></col><col style="width: 17%; " class="col_2"><!--Empty--></col><col style="width: 17%; " class="col_3"><!--Empty--></col><col style="width: 17%; " class="col_4"><!--Empty--></col><col style="width: 17%; " class="col_5"><!--Empty--></col><col style="width: 17%; " class="col_6"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735347267152" scope="col">Node</th><th align="left" valign="top" id="idm139735347266064" scope="col">vCPU</th><th align="left" valign="top" id="idm139735346016016" scope="col">RAM(GiB)</th><th align="left" valign="top" id="idm139735346014928" scope="col">Disk type</th><th align="left" valign="top" id="idm139735346013840" scope="col">Disk size(GiB)/IOS</th><th align="left" valign="top" id="idm139735346012752" scope="col">Count</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735347267152"> <p>
									Control plane/etcd <sup>[1]</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm139735347266064"> <p>
									16
								</p>
								 </td><td align="left" valign="top" headers="idm139735346016016"> <p>
									32
								</p>
								 </td><td align="left" valign="top" headers="idm139735346014928"> <p>
									io1
								</p>
								 </td><td align="left" valign="top" headers="idm139735346013840"> <p>
									120 / 10 IOPS per GiB
								</p>
								 </td><td align="left" valign="top" headers="idm139735346012752"> <p>
									3
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735347267152"> <p>
									Infra <sup>[2]</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm139735347266064"> <p>
									16
								</p>
								 </td><td align="left" valign="top" headers="idm139735346016016"> <p>
									64
								</p>
								 </td><td align="left" valign="top" headers="idm139735346014928"> <p>
									gp2
								</p>
								 </td><td align="left" valign="top" headers="idm139735346013840"> <p>
									120
								</p>
								 </td><td align="left" valign="top" headers="idm139735346012752"> <p>
									2
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735347267152"> <p>
									Workload <sup>[3]</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm139735347266064"> <p>
									16
								</p>
								 </td><td align="left" valign="top" headers="idm139735346016016"> <p>
									256
								</p>
								 </td><td align="left" valign="top" headers="idm139735346014928"> <p>
									gp2
								</p>
								 </td><td align="left" valign="top" headers="idm139735346013840"> <p>
									120 <sup>[4]</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm139735346012752"> <p>
									1
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735347267152"> <p>
									Compute
								</p>
								 </td><td align="left" valign="top" headers="idm139735347266064"> <p>
									16
								</p>
								 </td><td align="left" valign="top" headers="idm139735346016016"> <p>
									64
								</p>
								 </td><td align="left" valign="top" headers="idm139735346014928"> <p>
									gp2
								</p>
								 </td><td align="left" valign="top" headers="idm139735346013840"> <p>
									120
								</p>
								 </td><td align="left" valign="top" headers="idm139735346012752"> <p>
									2 to 100 <sup>[5]</sup>
								</p>
								 </td></tr></tbody></table></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							io1 disks with 120 / 10 IOPS per GiB are used for control plane/etcd nodes as etcd is I/O intensive and latency sensitive.
						</li><li class="listitem">
							Infra nodes are used to host Monitoring, Ingress, and Registry components to ensure they have enough resources to run at large scale.
						</li><li class="listitem">
							Workload node is dedicated to run performance and scalability workload generators.
						</li><li class="listitem">
							Larger disk size is used so that there is enough space to store the large amounts of data that is collected during the performance and scalability test run.
						</li><li class="listitem">
							Cluster is scaled in iterations.
						</li></ol></div></section><section class="section" id="ibm-z-platform"><div class="titlepage"><div><div><h3 class="title">2.2.3. IBM Z platform</h3></div></div></div><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 17%; " class="col_1"><!--Empty--></col><col style="width: 17%; " class="col_2"><!--Empty--></col><col style="width: 17%; " class="col_3"><!--Empty--></col><col style="width: 17%; " class="col_4"><!--Empty--></col><col style="width: 17%; " class="col_5"><!--Empty--></col><col style="width: 17%; " class="col_6"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735356141520" scope="col">Node</th><th align="left" valign="top" id="idm139735356140432" scope="col">vCPU <sup>[4]</sup></th><th align="left" valign="top" id="idm139735356139024" scope="col">RAM(GiB)<sup>[5]</sup></th><th align="left" valign="top" id="idm139735340554816" scope="col">Disk type</th><th align="left" valign="top" id="idm139735340553728" scope="col">Disk size(GiB)/IOS</th><th align="left" valign="top" id="idm139735340552640" scope="col">Count</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735356141520"> <p>
									Control plane/etcd <sup>[1,2]</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm139735356140432"> <p>
									8
								</p>
								 </td><td align="left" valign="top" headers="idm139735356139024"> <p>
									32
								</p>
								 </td><td align="left" valign="top" headers="idm139735340554816"> <p>
									ds8k
								</p>
								 </td><td align="left" valign="top" headers="idm139735340553728"> <p>
									300 / LCU 1
								</p>
								 </td><td align="left" valign="top" headers="idm139735340552640"> <p>
									3
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735356141520"> <p>
									Compute <sup>[1,3]</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm139735356140432"> <p>
									8
								</p>
								 </td><td align="left" valign="top" headers="idm139735356139024"> <p>
									32
								</p>
								 </td><td align="left" valign="top" headers="idm139735340554816"> <p>
									ds8k
								</p>
								 </td><td align="left" valign="top" headers="idm139735340553728"> <p>
									150 / LCU 2
								</p>
								 </td><td align="left" valign="top" headers="idm139735340552640"> <p>
									4 nodes (scaled to 100/250/500 pods per node)
								</p>
								 </td></tr></tbody></table></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Nodes are distributed between two logical control units (LCUs) to optimize disk I/O load of the control plane/etcd nodes as etcd is I/O intensive and latency sensitive. Etcd I/O demand should not interfere with other workloads.
						</li><li class="listitem">
							Four compute nodes are used for the tests running several iterations with 100/250/500 pods at the same time. First, idling pods were used to evaluate if pods can be instanced. Next, a network and CPU demanding client/server workload were used to evaluate the stability of the system under stress. Client and server pods were pairwise deployed and each pair was spread over two compute nodes.
						</li><li class="listitem">
							No separate workload node was used. The workload simulates a microservice workload between two compute nodes.
						</li><li class="listitem">
							Physical number of processors used is six Integrated Facilities for Linux (IFLs).
						</li><li class="listitem">
							Total physical memory used is 512 GiB.
						</li></ol></div></section></section><section class="section" id="how-to-plan-according-to-cluster-maximums_object-limits"><div class="titlepage"><div><div><h2 class="title">2.3. How to plan your environment according to tested cluster maximums</h2></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Oversubscribing the physical resources on a node affects resource guarantees the Kubernetes scheduler makes during pod placement. Learn what measures you can take to avoid memory swapping.
				</p><p>
					Some of the tested maximums are stretched only in a single dimension. They will vary when many objects are running on the cluster.
				</p><p>
					The numbers noted in this documentation are based on Red Hat’s test methodology, setup, configuration, and tunings. These numbers can vary based on your own individual setup and environments.
				</p></div></div><p>
				While planning your environment, determine how many pods are expected to fit per node:
			</p><pre class="screen">required pods per cluster / pods per node = total number of nodes needed</pre><p>
				The default maximum number of pods per node is 250. However, the number of pods that fit on a node is dependent on the application itself. Consider the application’s memory, CPU, and storage requirements, as described in "How to plan your environment according to application requirements".
			</p><div class="formalpara"><p class="title"><strong>Example scenario</strong></p><p>
					If you want to scope your cluster for 2200 pods per cluster, you would need at least five nodes, assuming that there are 500 maximum pods per node:
				</p></div><pre class="screen">2200 / 500 = 4.4</pre><p>
				If you increase the number of nodes to 20, then the pod distribution changes to 110 pods per node:
			</p><pre class="screen">2200 / 20 = 110</pre><p>
				Where:
			</p><pre class="screen">required pods per cluster / total number of nodes = expected pods per node</pre><p>
				OpenShift Container Platform comes with several system pods, such as SDN, DNS, Operators, and others, which run across every worker node by default. Therefore, the result of the above formula can vary.
			</p></section><section class="section" id="how-to-plan-according-to-application-requirements_object-limits"><div class="titlepage"><div><div><h2 class="title">2.4. How to plan your environment according to application requirements</h2></div></div></div><p>
				Consider an example application environment:
			</p><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 20%; " class="col_3"><!--Empty--></col><col style="width: 20%; " class="col_4"><!--Empty--></col><col style="width: 20%; " class="col_5"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735355919632" scope="col">Pod type</th><th align="left" valign="top" id="idm139735355918544" scope="col">Pod quantity</th><th align="left" valign="top" id="idm139735355917456" scope="col">Max memory</th><th align="left" valign="top" id="idm139735344894800" scope="col">CPU cores</th><th align="left" valign="top" id="idm139735344893712" scope="col">Persistent storage</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735355919632"> <p>
								apache
							</p>
							 </td><td align="left" valign="top" headers="idm139735355918544"> <p>
								100
							</p>
							 </td><td align="left" valign="top" headers="idm139735355917456"> <p>
								500 MB
							</p>
							 </td><td align="left" valign="top" headers="idm139735344894800"> <p>
								0.5
							</p>
							 </td><td align="left" valign="top" headers="idm139735344893712"> <p>
								1 GB
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735355919632"> <p>
								node.js
							</p>
							 </td><td align="left" valign="top" headers="idm139735355918544"> <p>
								200
							</p>
							 </td><td align="left" valign="top" headers="idm139735355917456"> <p>
								1 GB
							</p>
							 </td><td align="left" valign="top" headers="idm139735344894800"> <p>
								1
							</p>
							 </td><td align="left" valign="top" headers="idm139735344893712"> <p>
								1 GB
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735355919632"> <p>
								postgresql
							</p>
							 </td><td align="left" valign="top" headers="idm139735355918544"> <p>
								100
							</p>
							 </td><td align="left" valign="top" headers="idm139735355917456"> <p>
								1 GB
							</p>
							 </td><td align="left" valign="top" headers="idm139735344894800"> <p>
								2
							</p>
							 </td><td align="left" valign="top" headers="idm139735344893712"> <p>
								10 GB
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735355919632"> <p>
								JBoss EAP
							</p>
							 </td><td align="left" valign="top" headers="idm139735355918544"> <p>
								100
							</p>
							 </td><td align="left" valign="top" headers="idm139735355917456"> <p>
								1 GB
							</p>
							 </td><td align="left" valign="top" headers="idm139735344894800"> <p>
								1
							</p>
							 </td><td align="left" valign="top" headers="idm139735344893712"> <p>
								1 GB
							</p>
							 </td></tr></tbody></table></div><p>
				Extrapolated requirements: 550 CPU cores, 450GB RAM, and 1.4TB storage.
			</p><p>
				Instance size for nodes can be modulated up or down, depending on your preference. Nodes are often resource overcommitted. In this deployment scenario, you can choose to run additional smaller nodes or fewer larger nodes to provide the same amount of resources. Factors such as operational agility and cost-per-instance should be considered.
			</p><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735341317184" scope="col">Node type</th><th align="left" valign="top" id="idm139735341316096" scope="col">Quantity</th><th align="left" valign="top" id="idm139735341315008" scope="col">CPUs</th><th align="left" valign="top" id="idm139735341313920" scope="col">RAM (GB)</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735341317184"> <p>
								Nodes (option 1)
							</p>
							 </td><td align="left" valign="top" headers="idm139735341316096"> <p>
								100
							</p>
							 </td><td align="left" valign="top" headers="idm139735341315008"> <p>
								4
							</p>
							 </td><td align="left" valign="top" headers="idm139735341313920"> <p>
								16
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735341317184"> <p>
								Nodes (option 2)
							</p>
							 </td><td align="left" valign="top" headers="idm139735341316096"> <p>
								50
							</p>
							 </td><td align="left" valign="top" headers="idm139735341315008"> <p>
								8
							</p>
							 </td><td align="left" valign="top" headers="idm139735341313920"> <p>
								32
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735341317184"> <p>
								Nodes (option 3)
							</p>
							 </td><td align="left" valign="top" headers="idm139735341316096"> <p>
								25
							</p>
							 </td><td align="left" valign="top" headers="idm139735341315008"> <p>
								16
							</p>
							 </td><td align="left" valign="top" headers="idm139735341313920"> <p>
								64
							</p>
							 </td></tr></tbody></table></div><p>
				Some applications lend themselves well to overcommitted environments, and some do not. Most Java applications and applications that use huge pages are examples of applications that would not allow for overcommitment. That memory can not be used for other applications. In the example above, the environment would be roughly 30 percent overcommitted, a common ratio.
			</p><p>
				The application pods can access a service either by using environment variables or DNS. If using environment variables, for each active service the variables are injected by the kubelet when a pod is run on a node. A cluster-aware DNS server watches the Kubernetes API for new services and creates a set of DNS records for each one. If DNS is enabled throughout your cluster, then all pods should automatically be able to resolve services by their DNS name. Service discovery using DNS can be used in case you must go beyond 5000 services. When using environment variables for service discovery, the argument list exceeds the allowed length after 5000 services in a namespace, then the pods and deployments will start failing. Disable the service links in the deployment’s service specification file to overcome this:
			</p><pre class="programlisting language-yaml">---
apiVersion: template.openshift.io/v1
kind: Template
metadata:
  name: deployment-config-template
  creationTimestamp:
  annotations:
    description: This template will create a deploymentConfig with 1 replica, 4 env vars and a service.
    tags: ''
objects:
- apiVersion: apps.openshift.io/v1
  kind: DeploymentConfig
  metadata:
    name: deploymentconfig${IDENTIFIER}
  spec:
    template:
      metadata:
        labels:
          name: replicationcontroller${IDENTIFIER}
      spec:
        enableServiceLinks: false
        containers:
        - name: pause${IDENTIFIER}
          image: "${IMAGE}"
          ports:
          - containerPort: 8080
            protocol: TCP
          env:
          - name: ENVVAR1_${IDENTIFIER}
            value: "${ENV_VALUE}"
          - name: ENVVAR2_${IDENTIFIER}
            value: "${ENV_VALUE}"
          - name: ENVVAR3_${IDENTIFIER}
            value: "${ENV_VALUE}"
          - name: ENVVAR4_${IDENTIFIER}
            value: "${ENV_VALUE}"
          resources: {}
          imagePullPolicy: IfNotPresent
          capabilities: {}
          securityContext:
            capabilities: {}
            privileged: false
        restartPolicy: Always
        serviceAccount: ''
    replicas: 1
    selector:
      name: replicationcontroller${IDENTIFIER}
    triggers:
    - type: ConfigChange
    strategy:
      type: Rolling
- apiVersion: v1
  kind: Service
  metadata:
    name: service${IDENTIFIER}
  spec:
    selector:
      name: replicationcontroller${IDENTIFIER}
    ports:
    - name: serviceport${IDENTIFIER}
      protocol: TCP
      port: 80
      targetPort: 8080
    clusterIP: ''
    type: ClusterIP
    sessionAffinity: None
  status:
    loadBalancer: {}
parameters:
- name: IDENTIFIER
  description: Number to append to the name of resources
  value: '1'
  required: true
- name: IMAGE
  description: Image to use for deploymentConfig
  value: gcr.io/google-containers/pause-amd64:3.0
  required: false
- name: ENV_VALUE
  description: Value to use for environment variables
  generate: expression
  from: "[A-Za-z0-9]{255}"
  required: false
labels:
  template: deployment-config-template</pre><p>
				The number of application pods that can run in a namespace is dependent on the number of services and the length of the service name when the environment variables are used for service discovery. <code class="literal">ARG_MAX</code> on the system defines the maximum argument length for a new process and it is set to 2097152 bytes (2 MiB) by default. The Kubelet injects environment variables in to each pod scheduled to run in the namespace including:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">&lt;SERVICE_NAME&gt;_SERVICE_HOST=&lt;IP&gt;</code>
					</li><li class="listitem">
						<code class="literal">&lt;SERVICE_NAME&gt;_SERVICE_PORT=&lt;PORT&gt;</code>
					</li><li class="listitem">
						<code class="literal">&lt;SERVICE_NAME&gt;_PORT=tcp://&lt;IP&gt;:&lt;PORT&gt;</code>
					</li><li class="listitem">
						<code class="literal">&lt;SERVICE_NAME&gt;_PORT_&lt;PORT&gt;_TCP=tcp://&lt;IP&gt;:&lt;PORT&gt;</code>
					</li><li class="listitem">
						<code class="literal">&lt;SERVICE_NAME&gt;_PORT_&lt;PORT&gt;_TCP_PROTO=tcp</code>
					</li><li class="listitem">
						<code class="literal">&lt;SERVICE_NAME&gt;_PORT_&lt;PORT&gt;_TCP_PORT=&lt;PORT&gt;</code>
					</li><li class="listitem">
						<code class="literal">&lt;SERVICE_NAME&gt;_PORT_&lt;PORT&gt;_TCP_ADDR=&lt;ADDR&gt;</code>
					</li></ul></div><p>
				The pods in the namespace will start to fail if the argument length exceeds the allowed value and the number of characters in a service name impacts it. For example, in a namespace with 5000 services, the limit on the service name is 33 characters, which enables you to run 5000 pods in the namespace.
			</p></section></section><section class="chapter" id="ibm-z-recommended-host-practices"><div class="titlepage"><div><div><h1 class="title">Chapter 3. Recommended host practices for IBM Z &amp; IBM(R) LinuxONE environments</h1></div></div></div><p>
			This topic provides recommended host practices for OpenShift Container Platform on IBM Z and IBM® LinuxONE.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				The s390x architecture is unique in many aspects. Therefore, some recommendations made here might not apply to other platforms.
			</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				Unless stated otherwise, these practices apply to both z/VM and Red Hat Enterprise Linux (RHEL) KVM installations on IBM Z and IBM® LinuxONE.
			</p></div></div><section class="section" id="ibm-z-managing-cpu-overcommitment_ibm-z-recommended-host-practices"><div class="titlepage"><div><div><h2 class="title">3.1. Managing CPU overcommitment</h2></div></div></div><p>
				In a highly virtualized IBM Z environment, you must carefully plan the infrastructure setup and sizing. One of the most important features of virtualization is the capability to do resource overcommitment, allocating more resources to the virtual machines than actually available at the hypervisor level. This is very workload dependent and there is no golden rule that can be applied to all setups.
			</p><p>
				Depending on your setup, consider these best practices regarding CPU overcommitment:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						At LPAR level (PR/SM hypervisor), avoid assigning all available physical cores (IFLs) to each LPAR. For example, with four physical IFLs available, you should not define three LPARs with four logical IFLs each.
					</li><li class="listitem">
						Check and understand LPAR shares and weights.
					</li><li class="listitem">
						An excessive number of virtual CPUs can adversely affect performance. Do not define more virtual processors to a guest than logical processors are defined to the LPAR.
					</li><li class="listitem">
						Configure the number of virtual processors per guest for peak workload, not more.
					</li><li class="listitem">
						Start small and monitor the workload. Increase the vCPU number incrementally if necessary.
					</li><li class="listitem">
						Not all workloads are suitable for high overcommitment ratios. If the workload is CPU intensive, you will probably not be able to achieve high ratios without performance problems. Workloads that are more I/O intensive can keep consistent performance even with high overcommitment ratios.
					</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://www.vm.ibm.com/perf/tips/prgcom.html">z/VM Common Performance Problems and Solutions</a>
					</li><li class="listitem">
						<a class="link" href="https://www.ibm.com/docs/en/linux-on-systems?topic=overcommitment-considerations">z/VM overcommitment considerations</a>
					</li><li class="listitem">
						<a class="link" href="https://www.ibm.com/docs/en/zos/2.2.0?topic=director-lpar-cpu-management">LPAR CPU management</a>
					</li></ul></div></section><section class="section" id="ibm-z-disable-thp_ibm-z-recommended-host-practices"><div class="titlepage"><div><div><h2 class="title">3.2. Disable Transparent Huge Pages</h2></div></div></div><p>
				Transparent Huge Pages (THP) attempt to automate most aspects of creating, managing, and using huge pages. Since THP automatically manages the huge pages, this is not always handled optimally for all types of workloads. THP can lead to performance regressions, since many applications handle huge pages on their own. Therefore, consider disabling THP.
			</p></section><section class="section" id="ibm-z-boost-networking-performance-with-rfs_ibm-z-recommended-host-practices"><div class="titlepage"><div><div><h2 class="title">3.3. Boost networking performance with Receive Flow Steering</h2></div></div></div><p>
				Receive Flow Steering (RFS) extends Receive Packet Steering (RPS) by further reducing network latency. RFS is technically based on RPS, and improves the efficiency of packet processing by increasing the CPU cache hit rate. RFS achieves this, and in addition considers queue length, by determining the most convenient CPU for computation so that cache hits are more likely to occur within the CPU. Thus, the CPU cache is invalidated less and requires fewer cycles to rebuild the cache. This can help reduce packet processing run time.
			</p><section class="section" id="use-the-mco-to-activate-rfs_ibm-z-recommended-host-practices"><div class="titlepage"><div><div><h3 class="title">3.3.1. Use the Machine Config Operator (MCO) to activate RFS</h3></div></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Copy the following MCO sample profile into a YAML file. For example, <code class="literal">enable-rfs.yaml</code>:
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 50-enable-rfs
spec:
  config:
    ignition:
      version: 2.2.0
    storage:
      files:
      - contents:
          source: data:text/plain;charset=US-ASCII,%23%20turn%20on%20Receive%20Flow%20Steering%20%28RFS%29%20for%20all%20network%20interfaces%0ASUBSYSTEM%3D%3D%22net%22%2C%20ACTION%3D%3D%22add%22%2C%20RUN%7Bprogram%7D%2B%3D%22/bin/bash%20-c%20%27for%20x%20in%20/sys/%24DEVPATH/queues/rx-%2A%3B%20do%20echo%208192%20%3E%20%24x/rps_flow_cnt%3B%20%20done%27%22%0A
        filesystem: root
        mode: 0644
        path: /etc/udev/rules.d/70-persistent-net.rules
      - contents:
          source: data:text/plain;charset=US-ASCII,%23%20define%20sock%20flow%20enbtried%20for%20%20Receive%20Flow%20Steering%20%28RFS%29%0Anet.core.rps_sock_flow_entries%3D8192%0A
        filesystem: root
        mode: 0644
        path: /etc/sysctl.d/95-enable-rps.conf</pre></li><li class="listitem"><p class="simpara">
							Create the MCO profile:
						</p><pre class="programlisting language-terminal">$ oc create -f enable-rfs.yaml</pre></li><li class="listitem"><p class="simpara">
							Verify that an entry named <code class="literal">50-enable-rfs</code> is listed:
						</p><pre class="programlisting language-terminal">$ oc get mc</pre></li><li class="listitem"><p class="simpara">
							To deactivate, enter:
						</p><pre class="programlisting language-terminal">$ oc delete mc 50-enable-rfs</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://developer.ibm.com/tutorials/red-hat-openshift-on-ibm-z-tune-your-network-performance-with-rfs/">OpenShift Container Platform on IBM Z: Tune your network performance with RFS</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-networking-configuration_tools#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Configuration_tools-Configuring_Receive_Flow_Steering_RFS">Configuring Receive Flow Steering (RFS)</a>
						</li><li class="listitem">
							<a class="link" href="https://www.kernel.org/doc/Documentation/networking/scaling.txt">Scaling in the Linux Networking Stack</a>
						</li></ul></div></section></section><section class="section" id="ibm-z-choose-networking-setup_ibm-z-recommended-host-practices"><div class="titlepage"><div><div><h2 class="title">3.4. Choose your networking setup</h2></div></div></div><p>
				The networking stack is one of the most important components for a Kubernetes-based product like OpenShift Container Platform. For IBM Z setups, the networking setup depends on the hypervisor of your choice. Depending on the workload and the application, the best fit usually changes with the use case and the traffic pattern.
			</p><p>
				Depending on your setup, consider these best practices:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Consider all options regarding networking devices to optimize your traffic pattern. Explore the advantages of OSA-Express, RoCE Express, HiperSockets, z/VM VSwitch, Linux Bridge (KVM), and others to decide which option leads to the greatest benefit for your setup.
					</li><li class="listitem">
						Always use the latest available NIC version. For example, OSA Express 7S 10 GbE shows great improvement compared to OSA Express 6S 10 GbE with transactional workload types, although both are 10 GbE adapters.
					</li><li class="listitem">
						Each virtual switch adds an additional layer of latency.
					</li><li class="listitem">
						The load balancer plays an important role for network communication outside the cluster. Consider using a production-grade hardware load balancer if this is critical for your application.
					</li><li class="listitem">
						OpenShift Container Platform SDN introduces flows and rules, which impact the networking performance. Make sure to consider pod affinities and placements, to benefit from the locality of services where communication is critical.
					</li><li class="listitem">
						Balance the trade-off between performance and functionality.
					</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://www.ibm.com/docs/en/linux-on-systems?topic=openshift-performance#openshift_perf__ocp_eval">OpenShift Container Platform on IBM Z - Performance Experiences, Hints and Tips</a>
					</li><li class="listitem">
						<a class="link" href="https://www.ibm.com/docs/en/linux-on-systems?topic=openshift-performance#openshift_perf__ocp_net">OpenShift Container Platform on IBM Z Networking Performance</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#controlling-pod-placement-on-nodes-using-node-affinity-rules">Controlling pod placement on nodes using node affinity rules</a>
					</li></ul></div></section><section class="section" id="ibm-z-ensure-high-disk-performance-hyperpav_ibm-z-recommended-host-practices"><div class="titlepage"><div><div><h2 class="title">3.5. Ensure high disk performance with HyperPAV on z/VM</h2></div></div></div><p>
				DASD and ECKD devices are commonly used disk types in IBM Z environments. In a typical OpenShift Container Platform setup in z/VM environments, DASD disks are commonly used to support the local storage for the nodes. You can set up HyperPAV alias devices to provide more throughput and overall better I/O performance for the DASD disks that support the z/VM guests.
			</p><p>
				Using HyperPAV for the local storage devices leads to a significant performance benefit. However, you must be aware that there is a trade-off between throughput and CPU costs.
			</p><section class="section" id="use-the-mco-to-activate-hyperpav-aliases-in-nodes-using-zvm-full-pack-minidisks_ibm-z-recommended-host-practices"><div class="titlepage"><div><div><h3 class="title">3.5.1. Use the Machine Config Operator (MCO) to activate HyperPAV aliases in nodes using z/VM full-pack minidisks</h3></div></div></div><p>
					For z/VM-based OpenShift Container Platform setups that use full-pack minidisks, you can leverage the advantage of MCO profiles by activating HyperPAV aliases in all of the nodes. You must add YAML configurations for both control plane and compute nodes.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Copy the following MCO sample profile into a YAML file for the control plane node. For example, <code class="literal">05-master-kernelarg-hpav.yaml</code>:
						</p><pre class="programlisting language-terminal">$ cat 05-master-kernelarg-hpav.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 05-master-kernelarg-hpav
spec:
  config:
    ignition:
      version: 3.1.0
  kernelArguments:
    - rd.dasd=800-805</pre></li><li class="listitem"><p class="simpara">
							Copy the following MCO sample profile into a YAML file for the compute node. For example, <code class="literal">05-worker-kernelarg-hpav.yaml</code>:
						</p><pre class="programlisting language-terminal">$ cat 05-worker-kernelarg-hpav.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 05-worker-kernelarg-hpav
spec:
  config:
    ignition:
      version: 3.1.0
  kernelArguments:
    - rd.dasd=800-805</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								You must modify the <code class="literal">rd.dasd</code> arguments to fit the device IDs.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Create the MCO profiles:
						</p><pre class="programlisting language-terminal">$ oc create -f 05-master-kernelarg-hpav.yaml</pre><pre class="programlisting language-terminal">$ oc create -f 05-worker-kernelarg-hpav.yaml</pre></li><li class="listitem"><p class="simpara">
							To deactivate, enter:
						</p><pre class="programlisting language-terminal">$ oc delete -f 05-master-kernelarg-hpav.yaml</pre><pre class="programlisting language-terminal">$ oc delete -f 05-worker-kernelarg-hpav.yaml</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://www.ibm.com/docs/en/linux-on-systems?topic=io-using-hyperpav-eckd-dasd">Using HyperPAV for ECKD DASD</a>
						</li><li class="listitem">
							<a class="link" href="https://public.dhe.ibm.com/software/dw/linux390/perf/zvm_hpav00.pdf">Scaling HyperPAV alias devices on Linux guests on z/VM</a>
						</li></ul></div></section></section><section class="section" id="ibm-z-rhel-kvm-host-recommendations_ibm-z-recommended-host-practices"><div class="titlepage"><div><div><h2 class="title">3.6. RHEL KVM on IBM Z host recommendations</h2></div></div></div><p>
				Optimizing a KVM virtual server environment strongly depends on the workloads of the virtual servers and on the available resources. The same action that enhances performance in one environment can have adverse effects in another. Finding the best balance for a particular setting can be a challenge and often involves experimentation.
			</p><p>
				The following section introduces some best practices when using OpenShift Container Platform with RHEL KVM on IBM Z and IBM® LinuxONE environments.
			</p><section class="section" id="use-multiple-queues-for-your-virtio-network-interfaces_ibm-z-recommended-host-practices"><div class="titlepage"><div><div><h3 class="title">3.6.1. Use multiple queues for your VirtIO network interfaces</h3></div></div></div><p>
					With multiple virtual CPUs, you can transfer packages in parallel if you provide multiple queues for incoming and outgoing packets. Use the <code class="literal">queues</code> attribute of the <code class="literal">driver</code> element to configure multiple queues. Specify an integer of at least 2 that does not exceed the number of virtual CPUs of the virtual server.
				</p><p>
					The following example specification configures two input and output queues for a network interface:
				</p><pre class="programlisting language-xml">&lt;interface type="direct"&gt;
    &lt;source network="net01"/&gt;
    &lt;model type="virtio"/&gt;
    &lt;driver ... queues="2"/&gt;
&lt;/interface&gt;</pre><p>
					Multiple queues are designed to provide enhanced performance for a network interface, but they also use memory and CPU resources. Start with defining two queues for busy interfaces. Next, try two queues for interfaces with less traffic or more than two queues for busy interfaces.
				</p></section><section class="section" id="use-io-threads-for-your-virtual-block-devices_ibm-z-recommended-host-practices"><div class="titlepage"><div><div><h3 class="title">3.6.2. Use I/O threads for your virtual block devices</h3></div></div></div><p>
					To make virtual block devices use I/O threads, you must configure one or more I/O threads for the virtual server and each virtual block device to use one of these I/O threads.
				</p><p>
					The following example specifies <code class="literal">&lt;iothreads&gt;3&lt;/iothreads&gt;</code> to configure three I/O threads, with consecutive decimal thread IDs 1, 2, and 3. The <code class="literal">iothread="2"</code> parameter specifies the driver element of the disk device to use the I/O thread with ID 2.
				</p><div class="formalpara"><p class="title"><strong>Sample I/O thread specification</strong></p><p>
						
<pre class="programlisting language-xml">...
&lt;domain&gt;
 	&lt;iothreads&gt;3&lt;/iothreads&gt;<span id="CO3-2"><!--Empty--></span><span class="callout">1</span>
  	 ...
    	&lt;devices&gt;
       ...
          &lt;disk type="block" device="disk"&gt;<span id="CO3-3"><!--Empty--></span><span class="callout">2</span>
&lt;driver ... iothread="2"/&gt;
    &lt;/disk&gt;
       ...
    	&lt;/devices&gt;
   ...
&lt;/domain&gt;</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO3-1"><span class="callout">1</span></a> <a href="#CO3-2"><span class="callout">1</span></a> </dt><dd><div class="para">
							The number of I/O threads.
						</div></dd><dt><a href="#CO3-3"><span class="callout">2</span></a> </dt><dd><div class="para">
							The driver element of the disk device.
						</div></dd></dl></div><p>
					Threads can increase the performance of I/O operations for disk devices, but they also use memory and CPU resources. You can configure multiple devices to use the same thread. The best mapping of threads to devices depends on the available resources and the workload.
				</p><p>
					Start with a small number of I/O threads. Often, a single I/O thread for all disk devices is sufficient. Do not configure more threads than the number of virtual CPUs, and do not configure idle threads.
				</p><p>
					You can use the <code class="literal">virsh iothreadadd</code> command to add I/O threads with specific thread IDs to a running virtual server.
				</p></section><section class="section" id="avoid-virtual-scsi-devices_ibm-z-recommended-host-practices"><div class="titlepage"><div><div><h3 class="title">3.6.3. Avoid virtual SCSI devices</h3></div></div></div><p>
					Configure virtual SCSI devices only if you need to address the device through SCSI-specific interfaces. Configure disk space as virtual block devices rather than virtual SCSI devices, regardless of the backing on the host.
				</p><p>
					However, you might need SCSI-specific interfaces for:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							A LUN for a SCSI-attached tape drive on the host.
						</li><li class="listitem">
							A DVD ISO file on the host file system that is mounted on a virtual DVD drive.
						</li></ul></div></section><section class="section" id="configure-guest-caching-for-disk_ibm-z-recommended-host-practices"><div class="titlepage"><div><div><h3 class="title">3.6.4. Configure guest caching for disk</h3></div></div></div><p>
					Configure your disk devices to do caching by the guest and not by the host.
				</p><p>
					Ensure that the driver element of the disk device includes the <code class="literal">cache="none"</code> and <code class="literal">io="native"</code> parameters.
				</p><pre class="programlisting language-xml">&lt;disk type="block" device="disk"&gt;
    &lt;driver name="qemu" type="raw" cache="none" io="native" iothread="1"/&gt;
...
&lt;/disk&gt;</pre></section><section class="section" id="exclude-the-memory-ballon-device_ibm-z-recommended-host-practices"><div class="titlepage"><div><div><h3 class="title">3.6.5. Exclude the memory balloon device</h3></div></div></div><p>
					Unless you need a dynamic memory size, do not define a memory balloon device and ensure that libvirt does not create one for you. Include the <code class="literal">memballoon</code> parameter as a child of the devices element in your domain configuration XML file.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Check the list of active profiles:
						</p><pre class="programlisting language-xml">&lt;memballoon model="none"/&gt;</pre></li></ul></div></section><section class="section" id="tune-the-cpu-migration-algorithm-of-the-host-scheduler_ibm-z-recommended-host-practices"><div class="titlepage"><div><div><h3 class="title">3.6.6. Tune the CPU migration algorithm of the host scheduler</h3></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Do not change the scheduler settings unless you are an expert who understands the implications. Do not apply changes to production systems without testing them and confirming that they have the intended effect.
					</p></div></div><p>
					The <code class="literal">kernel.sched_migration_cost_ns</code> parameter specifies a time interval in nanoseconds. After the last execution of a task, the CPU cache is considered to have useful content until this interval expires. Increasing this interval results in fewer task migrations. The default value is 500000 ns.
				</p><p>
					If the CPU idle time is higher than expected when there are runnable processes, try reducing this interval. If tasks bounce between CPUs or nodes too often, try increasing it.
				</p><p>
					To dynamically set the interval to 60000 ns, enter the following command:
				</p><pre class="programlisting language-terminal"># sysctl kernel.sched_migration_cost_ns=60000</pre><p>
					To persistently change the value to 60000 ns, add the following entry to <code class="literal">/etc/sysctl.conf</code>:
				</p><pre class="programlisting language-config">kernel.sched_migration_cost_ns=60000</pre></section><section class="section" id="disable-the-cpuset-cgroup-controller_ibm-z-recommended-host-practices"><div class="titlepage"><div><div><h3 class="title">3.6.7. Disable the cpuset cgroup controller</h3></div></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						This setting applies only to KVM hosts with cgroups version 1. To enable CPU hotplug on the host, disable the cgroup controller.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Open <code class="literal">/etc/libvirt/qemu.conf</code> with an editor of your choice.
						</li><li class="listitem">
							Go to the <code class="literal">cgroup_controllers</code> line.
						</li><li class="listitem">
							Duplicate the entire line and remove the leading number sign (#) from the copy.
						</li><li class="listitem"><p class="simpara">
							Remove the <code class="literal">cpuset</code> entry, as follows:
						</p><pre class="programlisting language-config">cgroup_controllers = [ "cpu", "devices", "memory", "blkio", "cpuacct" ]</pre></li><li class="listitem"><p class="simpara">
							For the new setting to take effect, you must restart the libvirtd daemon:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Stop all virtual machines.
								</li><li class="listitem"><p class="simpara">
									Run the following command:
								</p><pre class="programlisting language-terminal"># systemctl restart libvirtd</pre></li><li class="listitem">
									Restart the virtual machines.
								</li></ol></div></li></ol></div><p>
					This setting persists across host reboots.
				</p></section><section class="section" id="tune-the-polling-period-for-idle-virtual-cpus_ibm-z-recommended-host-practices"><div class="titlepage"><div><div><h3 class="title">3.6.8. Tune the polling period for idle virtual CPUs</h3></div></div></div><p>
					When a virtual CPU becomes idle, KVM polls for wakeup conditions for the virtual CPU before allocating the host resource. You can specify the time interval, during which polling takes place in sysfs at <code class="literal">/sys/module/kvm/parameters/halt_poll_ns</code>. During the specified time, polling reduces the wakeup latency for the virtual CPU at the expense of resource usage. Depending on the workload, a longer or shorter time for polling can be beneficial. The time interval is specified in nanoseconds. The default is 50000 ns.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To optimize for low CPU consumption, enter a small value or write 0 to disable polling:
						</p><pre class="programlisting language-terminal"># echo 0 &gt; /sys/module/kvm/parameters/halt_poll_ns</pre></li><li class="listitem"><p class="simpara">
							To optimize for low latency, for example for transactional workloads, enter a large value:
						</p><pre class="programlisting language-terminal"># echo 80000 &gt; /sys/module/kvm/parameters/halt_poll_ns</pre></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://www.ibm.com/docs/en/linux-on-systems?topic=v-kvm">Linux on IBM Z Performance Tuning for KVM</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_virtualization/getting-started-with-virtualization-in-rhel-8-on-ibm-z_configuring-and-managing-virtualization">Getting started with virtualization on IBM Z</a>
						</li></ul></div></section></section></section><section class="chapter" id="using-node-tuning-operator"><div class="titlepage"><div><div><h1 class="title">Chapter 4. Using the Node Tuning Operator</h1></div></div></div><p>
			Learn about the Node Tuning Operator and how you can use it to manage node-level tuning by orchestrating the tuned daemon.
		</p><section class="section" id="about-node-tuning-operator_node-tuning-operator"><div class="titlepage"><div><div><h2 class="title">4.1. About the Node Tuning Operator</h2></div></div></div><p>
				The Node Tuning Operator helps you manage node-level tuning by orchestrating the TuneD daemon and achieves low latency performance by using the Performance Profile controller. The majority of high-performance applications require some level of kernel tuning. The Node Tuning Operator provides a unified management interface to users of node-level sysctls and more flexibility to add custom tuning specified by user needs.
			</p><p>
				The Operator manages the containerized TuneD daemon for OpenShift Container Platform as a Kubernetes daemon set. It ensures the custom tuning specification is passed to all containerized TuneD daemons running in the cluster in the format that the daemons understand. The daemons run on all nodes in the cluster, one per node.
			</p><p>
				Node-level settings applied by the containerized TuneD daemon are rolled back on an event that triggers a profile change or when the containerized TuneD daemon is terminated gracefully by receiving and handling a termination signal.
			</p><p>
				The Node Tuning Operator uses the Performance Profile controller to implement automatic tuning to achieve low latency performance for OpenShift Container Platform applications.
			</p><p>
				The cluster administrator configures a performance profile to define node-level settings such as the following:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Updating the kernel to kernel-rt.
					</li><li class="listitem">
						Choosing CPUs for housekeeping.
					</li><li class="listitem">
						Choosing CPUs for running workloads.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Currently, disabling CPU load balancing is not supported by cgroup v2. As a result, you might not get the desired behavior from performance profiles if you have cgroup v2 enabled. Enabling cgroup v2 is not recommended if you are using performance profiles.
				</p></div></div><p>
				The Node Tuning Operator is part of a standard OpenShift Container Platform installation in version 4.1 and later.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					In earlier versions of OpenShift Container Platform, the Performance Addon Operator was used to implement automatic tuning to achieve low latency performance for OpenShift applications. In OpenShift Container Platform 4.11 and later, this functionality is part of the Node Tuning Operator.
				</p></div></div></section><section class="section" id="accessing-an-example-node-tuning-operator-specification_node-tuning-operator"><div class="titlepage"><div><div><h2 class="title">4.2. Accessing an example Node Tuning Operator specification</h2></div></div></div><p>
				Use this process to access an example Node Tuning Operator specification.
			</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Run the following command to access an example Node Tuning Operator specification:
					</p><pre class="programlisting language-terminal">$ oc get Tuned/default -o yaml -n openshift-cluster-node-tuning-operator</pre></li></ul></div><p>
				The default CR is meant for delivering standard node-level tuning for the OpenShift Container Platform platform and it can only be modified to set the Operator Management state. Any other custom changes to the default CR will be overwritten by the Operator. For custom tuning, create your own Tuned CRs. Newly created CRs will be combined with the default CR and custom tuning applied to OpenShift Container Platform nodes based on node or pod labels and profile priorities.
			</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
					While in certain situations the support for pod labels can be a convenient way of automatically delivering required tuning, this practice is discouraged and strongly advised against, especially in large-scale clusters. The default Tuned CR ships without pod label matching. If a custom profile is created with pod label matching, then the functionality will be enabled at that time. The pod label functionality will be deprecated in future versions of the Node Tuning Operator.
				</p></div></div></section><section class="section" id="custom-tuning-default-profiles-set_node-tuning-operator"><div class="titlepage"><div><div><h2 class="title">4.3. Default profiles set on a cluster</h2></div></div></div><p>
				The following are the default profiles set on a cluster.
			</p><pre class="programlisting language-yaml">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: default
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=Optimize systems running OpenShift (provider specific parent profile)
      include=-provider-${f:exec:cat:/var/lib/tuned/provider},openshift
    name: openshift
  recommend:
  - profile: openshift-control-plane
    priority: 30
    match:
    - label: node-role.kubernetes.io/master
    - label: node-role.kubernetes.io/infra
  - profile: openshift-node
    priority: 40</pre><p>
				Starting with OpenShift Container Platform 4.9, all OpenShift TuneD profiles are shipped with the TuneD package. You can use the <code class="literal">oc exec</code> command to view the contents of these profiles:
			</p><pre class="programlisting language-terminal">$ oc exec $tuned_pod -n openshift-cluster-node-tuning-operator -- find /usr/lib/tuned/openshift{,-control-plane,-node} -name tuned.conf -exec grep -H ^ {} \;</pre></section><section class="section" id="verifying-tuned-profiles-are-applied_node-tuning-operator"><div class="titlepage"><div><div><h2 class="title">4.4. Verifying that the TuneD profiles are applied</h2></div></div></div><p>
				Verify the TuneD profiles that are applied to your cluster node.
			</p><pre class="programlisting language-terminal">$ oc get profile -n openshift-cluster-node-tuning-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
					
<pre class="programlisting language-terminal">NAME             TUNED                     APPLIED   DEGRADED   AGE
master-0         openshift-control-plane   True      False      6h33m
master-1         openshift-control-plane   True      False      6h33m
master-2         openshift-control-plane   True      False      6h33m
worker-a         openshift-node            True      False      6h28m
worker-b         openshift-node            True      False      6h28m</pre>

				</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">NAME</code>: Name of the Profile object. There is one Profile object per node and their names match.
					</li><li class="listitem">
						<code class="literal">TUNED</code>: Name of the desired TuneD profile to apply.
					</li><li class="listitem">
						<code class="literal">APPLIED</code>: <code class="literal">True</code> if the TuneD daemon applied the desired profile. (<code class="literal">True/False/Unknown</code>).
					</li><li class="listitem">
						<code class="literal">DEGRADED</code>: <code class="literal">True</code> if any errors were reported during application of the TuneD profile (<code class="literal">True/False/Unknown</code>).
					</li><li class="listitem">
						<code class="literal">AGE</code>: Time elapsed since the creation of Profile object.
					</li></ul></div><p>
				The <code class="literal">ClusterOperator/node-tuning</code> object also contains useful information about the Operator and its node agents' health. For example, Operator misconfiguration is reported by <code class="literal">ClusterOperator/node-tuning</code> status messages.
			</p><p>
				To get status information about the <code class="literal">ClusterOperator/node-tuning</code> object, run the following command:
			</p><pre class="programlisting language-terminal">$ oc get co/node-tuning -n openshift-cluster-node-tuning-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
					
<pre class="programlisting language-terminal">NAME          VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
node-tuning   4.13.1    True        False         True       60m     1/5 Profiles with bootcmdline conflict</pre>

				</p></div><p>
				If either the <code class="literal">ClusterOperator/node-tuning</code> or a profile object’s status is <code class="literal">DEGRADED</code>, additional information is provided in the Operator or operand logs.
			</p></section><section class="section" id="custom-tuning-specification_node-tuning-operator"><div class="titlepage"><div><div><h2 class="title">4.5. Custom tuning specification</h2></div></div></div><p>
				The custom resource (CR) for the Operator has two major sections. The first section, <code class="literal">profile:</code>, is a list of TuneD profiles and their names. The second, <code class="literal">recommend:</code>, defines the profile selection logic.
			</p><p>
				Multiple custom tuning specifications can co-exist as multiple CRs in the Operator’s namespace. The existence of new CRs or the deletion of old CRs is detected by the Operator. All existing custom tuning specifications are merged and appropriate objects for the containerized TuneD daemons are updated.
			</p><p>
				<span class="strong strong"><strong>Management state</strong></span>
			</p><p>
				The Operator Management state is set by adjusting the default Tuned CR. By default, the Operator is in the Managed state and the <code class="literal">spec.managementState</code> field is not present in the default Tuned CR. Valid values for the Operator Management state are as follows:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Managed: the Operator will update its operands as configuration resources are updated
					</li><li class="listitem">
						Unmanaged: the Operator will ignore changes to the configuration resources
					</li><li class="listitem">
						Removed: the Operator will remove its operands and resources the Operator provisioned
					</li></ul></div><p>
				<span class="strong strong"><strong>Profile data</strong></span>
			</p><p>
				The <code class="literal">profile:</code> section lists TuneD profiles and their names.
			</p><pre class="programlisting language-yaml">profile:
- name: tuned_profile_1
  data: |
    # TuneD profile specification
    [main]
    summary=Description of tuned_profile_1 profile

    [sysctl]
    net.ipv4.ip_forward=1
    # ... other sysctl's or other TuneD daemon plugins supported by the containerized TuneD

# ...

- name: tuned_profile_n
  data: |
    # TuneD profile specification
    [main]
    summary=Description of tuned_profile_n profile

    # tuned_profile_n profile settings</pre><p>
				<span class="strong strong"><strong>Recommended profiles</strong></span>
			</p><p>
				The <code class="literal">profile:</code> selection logic is defined by the <code class="literal">recommend:</code> section of the CR. The <code class="literal">recommend:</code> section is a list of items to recommend the profiles based on a selection criteria.
			</p><pre class="programlisting language-yaml">recommend:
&lt;recommend-item-1&gt;
# ...
&lt;recommend-item-n&gt;</pre><p>
				The individual items of the list:
			</p><pre class="programlisting language-yaml">- machineConfigLabels: <span id="CO4-1"><!--Empty--></span><span class="callout">1</span>
    &lt;mcLabels&gt; <span id="CO4-2"><!--Empty--></span><span class="callout">2</span>
  match: <span id="CO4-3"><!--Empty--></span><span class="callout">3</span>
    &lt;match&gt; <span id="CO4-4"><!--Empty--></span><span class="callout">4</span>
  priority: &lt;priority&gt; <span id="CO4-5"><!--Empty--></span><span class="callout">5</span>
  profile: &lt;tuned_profile_name&gt; <span id="CO4-6"><!--Empty--></span><span class="callout">6</span>
  operand: <span id="CO4-7"><!--Empty--></span><span class="callout">7</span>
    debug: &lt;bool&gt; <span id="CO4-8"><!--Empty--></span><span class="callout">8</span>
    tunedConfig:
      reapply_sysctl: &lt;bool&gt; <span id="CO4-9"><!--Empty--></span><span class="callout">9</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO4-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						Optional.
					</div></dd><dt><a href="#CO4-2"><span class="callout">2</span></a> </dt><dd><div class="para">
						A dictionary of key/value <code class="literal">MachineConfig</code> labels. The keys must be unique.
					</div></dd><dt><a href="#CO4-3"><span class="callout">3</span></a> </dt><dd><div class="para">
						If omitted, profile match is assumed unless a profile with a higher priority matches first or <code class="literal">machineConfigLabels</code> is set.
					</div></dd><dt><a href="#CO4-4"><span class="callout">4</span></a> </dt><dd><div class="para">
						An optional list.
					</div></dd><dt><a href="#CO4-5"><span class="callout">5</span></a> </dt><dd><div class="para">
						Profile ordering priority. Lower numbers mean higher priority (<code class="literal">0</code> is the highest priority).
					</div></dd><dt><a href="#CO4-6"><span class="callout">6</span></a> </dt><dd><div class="para">
						A TuneD profile to apply on a match. For example <code class="literal">tuned_profile_1</code>.
					</div></dd><dt><a href="#CO4-7"><span class="callout">7</span></a> </dt><dd><div class="para">
						Optional operand configuration.
					</div></dd><dt><a href="#CO4-8"><span class="callout">8</span></a> </dt><dd><div class="para">
						Turn debugging on or off for the TuneD daemon. Options are <code class="literal">true</code> for on or <code class="literal">false</code> for off. The default is <code class="literal">false</code>.
					</div></dd><dt><a href="#CO4-9"><span class="callout">9</span></a> </dt><dd><div class="para">
						Turn <code class="literal">reapply_sysctl</code> functionality on or off for the TuneD daemon. Options are <code class="literal">true</code> for on and <code class="literal">false</code> for off.
					</div></dd></dl></div><p>
				<code class="literal">&lt;match&gt;</code> is an optional list recursively defined as follows:
			</p><pre class="programlisting language-yaml">- label: &lt;label_name&gt; <span id="CO5-1"><!--Empty--></span><span class="callout">1</span>
  value: &lt;label_value&gt; <span id="CO5-2"><!--Empty--></span><span class="callout">2</span>
  type: &lt;label_type&gt; <span id="CO5-3"><!--Empty--></span><span class="callout">3</span>
    &lt;match&gt; <span id="CO5-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO5-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						Node or pod label name.
					</div></dd><dt><a href="#CO5-2"><span class="callout">2</span></a> </dt><dd><div class="para">
						Optional node or pod label value. If omitted, the presence of <code class="literal">&lt;label_name&gt;</code> is enough to match.
					</div></dd><dt><a href="#CO5-3"><span class="callout">3</span></a> </dt><dd><div class="para">
						Optional object type (<code class="literal">node</code> or <code class="literal">pod</code>). If omitted, <code class="literal">node</code> is assumed.
					</div></dd><dt><a href="#CO5-4"><span class="callout">4</span></a> </dt><dd><div class="para">
						An optional <code class="literal">&lt;match&gt;</code> list.
					</div></dd></dl></div><p>
				If <code class="literal">&lt;match&gt;</code> is not omitted, all nested <code class="literal">&lt;match&gt;</code> sections must also evaluate to <code class="literal">true</code>. Otherwise, <code class="literal">false</code> is assumed and the profile with the respective <code class="literal">&lt;match&gt;</code> section will not be applied or recommended. Therefore, the nesting (child <code class="literal">&lt;match&gt;</code> sections) works as logical AND operator. Conversely, if any item of the <code class="literal">&lt;match&gt;</code> list matches, the entire <code class="literal">&lt;match&gt;</code> list evaluates to <code class="literal">true</code>. Therefore, the list acts as logical OR operator.
			</p><p>
				If <code class="literal">machineConfigLabels</code> is defined, machine config pool based matching is turned on for the given <code class="literal">recommend:</code> list item. <code class="literal">&lt;mcLabels&gt;</code> specifies the labels for a machine config. The machine config is created automatically to apply host settings, such as kernel boot parameters, for the profile <code class="literal">&lt;tuned_profile_name&gt;</code>. This involves finding all machine config pools with machine config selector matching <code class="literal">&lt;mcLabels&gt;</code> and setting the profile <code class="literal">&lt;tuned_profile_name&gt;</code> on all nodes that are assigned the found machine config pools. To target nodes that have both master and worker roles, you must use the master role.
			</p><p>
				The list items <code class="literal">match</code> and <code class="literal">machineConfigLabels</code> are connected by the logical OR operator. The <code class="literal">match</code> item is evaluated first in a short-circuit manner. Therefore, if it evaluates to <code class="literal">true</code>, the <code class="literal">machineConfigLabels</code> item is not considered.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					When using machine config pool based matching, it is advised to group nodes with the same hardware configuration into the same machine config pool. Not following this practice might result in TuneD operands calculating conflicting kernel parameters for two or more nodes sharing the same machine config pool.
				</p></div></div><div class="formalpara"><p class="title"><strong>Example: node or pod label based matching</strong></p><p>
					
<pre class="programlisting language-yaml">- match:
  - label: tuned.openshift.io/elasticsearch
    match:
    - label: node-role.kubernetes.io/master
    - label: node-role.kubernetes.io/infra
    type: pod
  priority: 10
  profile: openshift-control-plane-es
- match:
  - label: node-role.kubernetes.io/master
  - label: node-role.kubernetes.io/infra
  priority: 20
  profile: openshift-control-plane
- priority: 30
  profile: openshift-node</pre>

				</p></div><p>
				The CR above is translated for the containerized TuneD daemon into its <code class="literal">recommend.conf</code> file based on the profile priorities. The profile with the highest priority (<code class="literal">10</code>) is <code class="literal">openshift-control-plane-es</code> and, therefore, it is considered first. The containerized TuneD daemon running on a given node looks to see if there is a pod running on the same node with the <code class="literal">tuned.openshift.io/elasticsearch</code> label set. If not, the entire <code class="literal">&lt;match&gt;</code> section evaluates as <code class="literal">false</code>. If there is such a pod with the label, in order for the <code class="literal">&lt;match&gt;</code> section to evaluate to <code class="literal">true</code>, the node label also needs to be <code class="literal">node-role.kubernetes.io/master</code> or <code class="literal">node-role.kubernetes.io/infra</code>.
			</p><p>
				If the labels for the profile with priority <code class="literal">10</code> matched, <code class="literal">openshift-control-plane-es</code> profile is applied and no other profile is considered. If the node/pod label combination did not match, the second highest priority profile (<code class="literal">openshift-control-plane</code>) is considered. This profile is applied if the containerized TuneD pod runs on a node with labels <code class="literal">node-role.kubernetes.io/master</code> or <code class="literal">node-role.kubernetes.io/infra</code>.
			</p><p>
				Finally, the profile <code class="literal">openshift-node</code> has the lowest priority of <code class="literal">30</code>. It lacks the <code class="literal">&lt;match&gt;</code> section and, therefore, will always match. It acts as a profile catch-all to set <code class="literal">openshift-node</code> profile, if no other profile with higher priority matches on a given node.
			</p><div class="informalfigure"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Scalability_and_performance-en-US/images/b350c395f7c7262cec5e5d9d7404ce73/node-tuning-operator-workflow-revised.png" alt="Decision workflow"/></div></div><div class="formalpara"><p class="title"><strong>Example: machine config pool based matching</strong></p><p>
					
<pre class="programlisting language-yaml">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: openshift-node-custom
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=Custom OpenShift node profile with an additional kernel parameter
      include=openshift-node
      [bootloader]
      cmdline_openshift_node_custom=+skew_tick=1
    name: openshift-node-custom

  recommend:
  - machineConfigLabels:
      machineconfiguration.openshift.io/role: "worker-custom"
    priority: 20
    profile: openshift-node-custom</pre>

				</p></div><p>
				To minimize node reboots, label the target nodes with a label the machine config pool’s node selector will match, then create the Tuned CR above and finally create the custom machine config pool itself.
			</p><p>
				<span class="strong strong"><strong>Cloud provider-specific TuneD profiles</strong></span>
			</p><p>
				With this functionality, all Cloud provider-specific nodes can conveniently be assigned a TuneD profile specifically tailored to a given Cloud provider on a OpenShift Container Platform cluster. This can be accomplished without adding additional node labels or grouping nodes into machine config pools.
			</p><p>
				This functionality takes advantage of <code class="literal">spec.providerID</code> node object values in the form of <code class="literal">&lt;cloud-provider&gt;://&lt;cloud-provider-specific-id&gt;</code> and writes the file <code class="literal">/var/lib/tuned/provider</code> with the value <code class="literal">&lt;cloud-provider&gt;</code> in NTO operand containers. The content of this file is then used by TuneD to load <code class="literal">provider-&lt;cloud-provider&gt;</code> profile if such profile exists.
			</p><p>
				The <code class="literal">openshift</code> profile that both <code class="literal">openshift-control-plane</code> and <code class="literal">openshift-node</code> profiles inherit settings from is now updated to use this functionality through the use of conditional profile loading. Neither NTO nor TuneD currently include any Cloud provider-specific profiles. However, it is possible to create a custom profile <code class="literal">provider-&lt;cloud-provider&gt;</code> that will be applied to all Cloud provider-specific cluster nodes.
			</p><div class="formalpara"><p class="title"><strong>Example GCE Cloud provider profile</strong></p><p>
					
<pre class="programlisting language-yaml">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: provider-gce
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=GCE Cloud provider-specific profile
      # Your tuning for GCE Cloud provider goes here.
    name: provider-gce</pre>

				</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Due to profile inheritance, any setting specified in the <code class="literal">provider-&lt;cloud-provider&gt;</code> profile will be overwritten by the <code class="literal">openshift</code> profile and its child profiles.
				</p></div></div></section><section class="section" id="custom-tuning-example_node-tuning-operator"><div class="titlepage"><div><div><h2 class="title">4.6. Custom tuning examples</h2></div></div></div><p>
				<span class="strong strong"><strong>Using TuneD profiles from the default CR</strong></span>
			</p><p>
				The following CR applies custom node-level tuning for OpenShift Container Platform nodes with label <code class="literal">tuned.openshift.io/ingress-node-label</code> set to any value.
			</p><div class="formalpara"><p class="title"><strong>Example: custom tuning using the openshift-control-plane TuneD profile</strong></p><p>
					
<pre class="programlisting language-yaml">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: ingress
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=A custom OpenShift ingress profile
      include=openshift-control-plane
      [sysctl]
      net.ipv4.ip_local_port_range="1024 65535"
      net.ipv4.tcp_tw_reuse=1
    name: openshift-ingress
  recommend:
  - match:
    - label: tuned.openshift.io/ingress-node-label
    priority: 10
    profile: openshift-ingress</pre>

				</p></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Custom profile writers are strongly encouraged to include the default TuneD daemon profiles shipped within the default Tuned CR. The example above uses the default <code class="literal">openshift-control-plane</code> profile to accomplish this.
				</p></div></div><p>
				<span class="strong strong"><strong>Using built-in TuneD profiles</strong></span>
			</p><p>
				Given the successful rollout of the NTO-managed daemon set, the TuneD operands all manage the same version of the TuneD daemon. To list the built-in TuneD profiles supported by the daemon, query any TuneD pod in the following way:
			</p><pre class="programlisting language-terminal">$ oc exec $tuned_pod -n openshift-cluster-node-tuning-operator -- find /usr/lib/tuned/ -name tuned.conf -printf '%h\n' | sed 's|^.*/||'</pre><p>
				You can use the profile names retrieved by this in your custom tuning specification.
			</p><div class="formalpara"><p class="title"><strong>Example: using built-in hpc-compute TuneD profile</strong></p><p>
					
<pre class="programlisting language-yaml">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: openshift-node-hpc-compute
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=Custom OpenShift node profile for HPC compute workloads
      include=openshift-node,hpc-compute
    name: openshift-node-hpc-compute

  recommend:
  - match:
    - label: tuned.openshift.io/openshift-node-hpc-compute
    priority: 20
    profile: openshift-node-hpc-compute</pre>

				</p></div><p>
				In addition to the built-in <code class="literal">hpc-compute</code> profile, the example above includes the <code class="literal">openshift-node</code> TuneD daemon profile shipped within the default Tuned CR to use OpenShift-specific tuning for compute nodes.
			</p><p>
				<span class="strong strong"><strong>Overriding host-level sysctls</strong></span>
			</p><p>
				Various kernel parameters can be changed at runtime by using <code class="literal">/run/sysctl.d/</code>, <code class="literal">/etc/sysctl.d/</code>, and <code class="literal">/etc/sysctl.conf</code> host configuration files. OpenShift Container Platform adds several host configuration files which set kernel parameters at runtime; for example, <code class="literal">net.ipv[4-6].</code>, <code class="literal">fs.inotify.</code>, and <code class="literal">vm.max_map_count</code>. These runtime parameters provide basic functional tuning for the system prior to the kubelet and the Operator start.
			</p><p>
				The Operator does not override these settings unless the <code class="literal">reapply_sysctl</code> option is set to <code class="literal">false</code>. Setting this option to <code class="literal">false</code> results in <code class="literal">TuneD</code> not applying the settings from the host configuration files after it applies its custom profile.
			</p><div class="formalpara"><p class="title"><strong>Example: overriding host-level sysctls</strong></p><p>
					
<pre class="programlisting language-yaml">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: openshift-no-reapply-sysctl
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=Custom OpenShift profile
      include=openshift-node
      [sysctl]
      vm.max_map_count=&gt;524288
    name: openshift-no-reapply-sysctl
  recommend:
  - match:
    - label: tuned.openshift.io/openshift-no-reapply-sysctl
    priority: 15
    profile: openshift-no-reapply-sysctl
    operand:
      tunedConfig:
        reapply_sysctl: false</pre>

				</p></div></section><section class="section" id="supported-tuned-daemon-plug-ins_node-tuning-operator"><div class="titlepage"><div><div><h2 class="title">4.7. Supported TuneD daemon plugins</h2></div></div></div><p>
				Excluding the <code class="literal">[main]</code> section, the following TuneD plugins are supported when using custom profiles defined in the <code class="literal">profile:</code> section of the Tuned CR:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						audio
					</li><li class="listitem">
						cpu
					</li><li class="listitem">
						disk
					</li><li class="listitem">
						eeepc_she
					</li><li class="listitem">
						modules
					</li><li class="listitem">
						mounts
					</li><li class="listitem">
						net
					</li><li class="listitem">
						scheduler
					</li><li class="listitem">
						scsi_host
					</li><li class="listitem">
						selinux
					</li><li class="listitem">
						sysctl
					</li><li class="listitem">
						sysfs
					</li><li class="listitem">
						usb
					</li><li class="listitem">
						video
					</li><li class="listitem">
						vm
					</li><li class="listitem">
						bootloader
					</li></ul></div><p>
				There is some dynamic tuning functionality provided by some of these plugins that is not supported. The following TuneD plugins are currently not supported:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						script
					</li><li class="listitem">
						systemd
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The TuneD bootloader plugin only supports Red Hat Enterprise Linux CoreOS (RHCOS) worker nodes.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/customizing-tuned-profiles_monitoring-and-managing-system-status-and-performance#available-tuned-plug-ins_customizing-tuned-profiles">Available TuneD Plugins</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/getting-started-with-tuned_monitoring-and-managing-system-status-and-performance">Getting Started with TuneD</a>
					</li></ul></div></section><section class="section" id="node-tuning-hosted-cluster_node-tuning-operator"><div class="titlepage"><div><div><h2 class="title">4.8. Configuring node tuning in a hosted cluster</h2></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Hosted control planes is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
				</p><p>
					For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
				</p></div></div><p>
				To set node-level tuning on the nodes in your hosted cluster, you can use the Node Tuning Operator. In hosted control planes, you can configure node tuning by creating config maps that contain <code class="literal">Tuned</code> objects and referencing those config maps in your node pools.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a config map that contains a valid tuned manifest, and reference the manifest in a node pool. In the following example, a <code class="literal">Tuned</code> manifest defines a profile that sets <code class="literal">vm.dirty_ratio</code> to 55 on nodes that contain the <code class="literal">tuned-1-node-label</code> node label with any value. Save the following <code class="literal">ConfigMap</code> manifest in a file named <code class="literal">tuned-1.yaml</code>:
					</p><pre class="programlisting language-yaml">    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: tuned-1
      namespace: clusters
    data:
      tuning: |
        apiVersion: tuned.openshift.io/v1
        kind: Tuned
        metadata:
          name: tuned-1
          namespace: openshift-cluster-node-tuning-operator
        spec:
          profile:
          - data: |
              [main]
              summary=Custom OpenShift profile
              include=openshift-node
              [sysctl]
              vm.dirty_ratio="55"
            name: tuned-1-profile
          recommend:
          - priority: 20
            profile: tuned-1-profile</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If you do not add any labels to an entry in the <code class="literal">spec.recommend</code> section of the Tuned spec, node-pool-based matching is assumed, so the highest priority profile in the <code class="literal">spec.recommend</code> section is applied to nodes in the pool. Although you can achieve more fine-grained node-label-based matching by setting a label value in the Tuned <code class="literal">.spec.recommend.match</code> section, node labels will not persist during an upgrade unless you set the <code class="literal">.spec.management.upgradeType</code> value of the node pool to <code class="literal">InPlace</code>.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Create the <code class="literal">ConfigMap</code> object in the management cluster:
					</p><pre class="programlisting language-terminal">$ oc --kubeconfig="$MGMT_KUBECONFIG" create -f tuned-1.yaml</pre></li><li class="listitem"><p class="simpara">
						Reference the <code class="literal">ConfigMap</code> object in the <code class="literal">spec.tuningConfig</code> field of the node pool, either by editing a node pool or creating one. In this example, assume that you have only one <code class="literal">NodePool</code>, named <code class="literal">nodepool-1</code>, which contains 2 nodes.
					</p><pre class="programlisting language-yaml">    apiVersion: hypershift.openshift.io/v1alpha1
    kind: NodePool
    metadata:
      ...
      name: nodepool-1
      namespace: clusters
    ...
    spec:
      ...
      tuningConfig:
      - name: tuned-1
    status:
    ...</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You can reference the same config map in multiple node pools. In hosted control planes, the Node Tuning Operator appends a hash of the node pool name and namespace to the name of the Tuned CRs to distinguish them. Outside of this case, do not create multiple TuneD profiles of the same name in different Tuned CRs for the same hosted cluster.
						</p></div></div></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
					Now that you have created the <code class="literal">ConfigMap</code> object that contains a <code class="literal">Tuned</code> manifest and referenced it in a <code class="literal">NodePool</code>, the Node Tuning Operator syncs the <code class="literal">Tuned</code> objects into the hosted cluster. You can verify which <code class="literal">Tuned</code> objects are defined and which TuneD profiles are applied to each node.
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						List the <code class="literal">Tuned</code> objects in the hosted cluster:
					</p><pre class="programlisting language-terminal">$ oc --kubeconfig="$HC_KUBECONFIG" get Tuneds -n openshift-cluster-node-tuning-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME       AGE
default    7m36s
rendered   7m36s
tuned-1    65s</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						List the <code class="literal">Profile</code> objects in the hosted cluster:
					</p><pre class="programlisting language-terminal">$ oc --kubeconfig="$HC_KUBECONFIG" get Profiles -n openshift-cluster-node-tuning-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                           TUNED            APPLIED   DEGRADED   AGE
nodepool-1-worker-1            tuned-1-profile  True      False      7m43s
nodepool-1-worker-2            tuned-1-profile  True      False      7m14s</pre>

						</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If no custom profiles are created, the <code class="literal">openshift-node</code> profile is applied by default.
						</p></div></div></li><li class="listitem"><p class="simpara">
						To confirm that the tuning was applied correctly, start a debug shell on a node and check the sysctl values:
					</p><pre class="programlisting language-terminal">$ oc --kubeconfig="$HC_KUBECONFIG" debug node/nodepool-1-worker-1 -- chroot /host sysctl vm.dirty_ratio</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">vm.dirty_ratio = 55</pre>

						</p></div></li></ol></div></section><section class="section" id="advanced-node-tuning-hosted-cluster_node-tuning-operator"><div class="titlepage"><div><div><h2 class="title">4.9. Advanced node tuning for hosted clusters by setting kernel boot parameters</h2></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Hosted control planes is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
				</p><p>
					For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
				</p></div></div><p>
				For more advanced tuning in hosted control planes, which requires setting kernel boot parameters, you can also use the Node Tuning Operator. The following example shows how you can create a node pool with huge pages reserved.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a <code class="literal">ConfigMap</code> object that contains a <code class="literal">Tuned</code> object manifest for creating 10 huge pages that are 2 MB in size. Save this <code class="literal">ConfigMap</code> manifest in a file named <code class="literal">tuned-hugepages.yaml</code>:
					</p><pre class="programlisting language-yaml">    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: tuned-hugepages
      namespace: clusters
    data:
      tuning: |
        apiVersion: tuned.openshift.io/v1
        kind: Tuned
        metadata:
          name: hugepages
          namespace: openshift-cluster-node-tuning-operator
        spec:
          profile:
          - data: |
              [main]
              summary=Boot time configuration for hugepages
              include=openshift-node
              [bootloader]
              cmdline_openshift_node_hugepages=hugepagesz=2M hugepages=50
            name: openshift-node-hugepages
          recommend:
          - priority: 20
            profile: openshift-node-hugepages</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The <code class="literal">.spec.recommend.match</code> field is intentionally left blank. In this case, this <code class="literal">Tuned</code> object is applied to all nodes in the node pool where this <code class="literal">ConfigMap</code> object is referenced. Group nodes with the same hardware configuration into the same node pool. Otherwise, TuneD operands can calculate conflicting kernel parameters for two or more nodes that share the same node pool.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Create the <code class="literal">ConfigMap</code> object in the management cluster:
					</p><pre class="programlisting language-terminal">$ oc --kubeconfig="$MGMT_KUBECONFIG" create -f tuned-hugepages.yaml</pre></li><li class="listitem"><p class="simpara">
						Create a <code class="literal">NodePool</code> manifest YAML file, customize the upgrade type of the <code class="literal">NodePool</code>, and reference the <code class="literal">ConfigMap</code> object that you created in the <code class="literal">spec.tuningConfig</code> section. Create the <code class="literal">NodePool</code> manifest and save it in a file named <code class="literal">hugepages-nodepool.yaml</code> by using the <code class="literal">hypershift</code> CLI:
					</p><pre class="programlisting language-yaml">    NODEPOOL_NAME=hugepages-example
    INSTANCE_TYPE=m5.2xlarge
    NODEPOOL_REPLICAS=2

    hypershift create nodepool aws \
      --cluster-name $CLUSTER_NAME \
      --name $NODEPOOL_NAME \
      --node-count $NODEPOOL_REPLICAS \
      --instance-type $INSTANCE_TYPE \
      --render &gt; hugepages-nodepool.yaml</pre></li><li class="listitem"><p class="simpara">
						In the <code class="literal">hugepages-nodepool.yaml</code> file, set <code class="literal">.spec.management.upgradeType</code> to <code class="literal">InPlace</code>, and set <code class="literal">.spec.tuningConfig</code> to reference the <code class="literal">tuned-hugepages</code> <code class="literal">ConfigMap</code> object that you created.
					</p><pre class="programlisting language-yaml">    apiVersion: hypershift.openshift.io/v1alpha1
    kind: NodePool
    metadata:
      name: hugepages-nodepool
      namespace: clusters
      ...
    spec:
      management:
        ...
        upgradeType: InPlace
      ...
      tuningConfig:
      - name: tuned-hugepages</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							To avoid the unnecessary re-creation of nodes when you apply the new <code class="literal">MachineConfig</code> objects, set <code class="literal">.spec.management.upgradeType</code> to <code class="literal">InPlace</code>. If you use the <code class="literal">Replace</code> upgrade type, nodes are fully deleted and new nodes can replace them when you apply the new kernel boot parameters that the TuneD operand calculated.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Create the <code class="literal">NodePool</code> in the management cluster:
					</p><pre class="programlisting language-terminal">$ oc --kubeconfig="$MGMT_KUBECONFIG" create -f hugepages-nodepool.yaml</pre></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
					After the nodes are available, the containerized TuneD daemon calculates the required kernel boot parameters based on the applied TuneD profile. After the nodes are ready and reboot once to apply the generated <code class="literal">MachineConfig</code> object, you can verify that the TuneD profile is applied and that the kernel boot parameters are set.
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						List the <code class="literal">Tuned</code> objects in the hosted cluster:
					</p><pre class="programlisting language-terminal">$ oc --kubeconfig="$HC_KUBECONFIG" get Tuneds -n openshift-cluster-node-tuning-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                 AGE
default              123m
hugepages-8dfb1fed   1m23s
rendered             123m</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						List the <code class="literal">Profile</code> objects in the hosted cluster:
					</p><pre class="programlisting language-terminal">$ oc --kubeconfig="$HC_KUBECONFIG" get Profiles -n openshift-cluster-node-tuning-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                           TUNED                      APPLIED   DEGRADED   AGE
nodepool-1-worker-1            openshift-node             True      False      132m
nodepool-1-worker-2            openshift-node             True      False      131m
hugepages-nodepool-worker-1    openshift-node-hugepages   True      False      4m8s
hugepages-nodepool-worker-2    openshift-node-hugepages   True      False      3m57s</pre>

						</p></div><p class="simpara">
						Both of the worker nodes in the new <code class="literal">NodePool</code> have the <code class="literal">openshift-node-hugepages</code> profile applied.
					</p></li><li class="listitem"><p class="simpara">
						To confirm that the tuning was applied correctly, start a debug shell on a node and check <code class="literal">/proc/cmdline</code>.
					</p><pre class="programlisting language-terminal">$ oc --kubeconfig="$HC_KUBECONFIG" debug node/nodepool-1-worker-1 -- chroot /host cat /proc/cmdline</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">BOOT_IMAGE=(hd0,gpt3)/ostree/rhcos-... hugepagesz=2M hugepages=50</pre>

						</p></div></li></ol></div><div class="_additional-resources _additional-resources"><p class="title"><strong>Additional resources</strong></p><p>
					For more information about hosted control planes, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.8/html/clusters/cluster_mce_overview#hosted-control-planes-intro">Hosted control planes (Technology Preview)</a>.
				</p></div></section></section><section class="chapter" id="using-cpu-manager"><div class="titlepage"><div><div><h1 class="title">Chapter 5. Using CPU Manager and Topology Manager</h1></div></div></div><p>
			CPU Manager manages groups of CPUs and constrains workloads to specific CPUs.
		</p><p>
			CPU Manager is useful for workloads that have some of these attributes:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Require as much CPU time as possible.
				</li><li class="listitem">
					Are sensitive to processor cache misses.
				</li><li class="listitem">
					Are low-latency network applications.
				</li><li class="listitem">
					Coordinate with other processes and benefit from sharing a single processor cache.
				</li></ul></div><p>
			Topology Manager collects hints from the CPU Manager, Device Manager, and other Hint Providers to align pod resources, such as CPU, SR-IOV VFs, and other device resources, for all Quality of Service (QoS) classes on the same non-uniform memory access (NUMA) node.
		</p><p>
			Topology Manager uses topology information from the collected hints to decide if a pod can be accepted or rejected on a node, based on the configured Topology Manager policy and pod resources requested.
		</p><p>
			Topology Manager is useful for workloads that use hardware accelerators to support latency-critical execution and high throughput parallel computation.
		</p><p>
			To use Topology Manager you must configure CPU Manager with the <code class="literal">static</code> policy.
		</p><section class="section" id="seting_up_cpu_manager_using-cpu-manager-and-topology_manager"><div class="titlepage"><div><div><h2 class="title">5.1. Setting up CPU Manager</h2></div></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Optional: Label a node:
					</p><pre class="programlisting language-terminal"># oc label node perf-node.example.com cpumanager=true</pre></li><li class="listitem"><p class="simpara">
						Edit the <code class="literal">MachineConfigPool</code> of the nodes where CPU Manager should be enabled. In this example, all workers have CPU Manager enabled:
					</p><pre class="programlisting language-terminal"># oc edit machineconfigpool worker</pre></li><li class="listitem"><p class="simpara">
						Add a label to the worker machine config pool:
					</p><pre class="programlisting language-yaml">metadata:
  creationTimestamp: 2020-xx-xxx
  generation: 3
  labels:
    custom-kubelet: cpumanager-enabled</pre></li><li class="listitem"><p class="simpara">
						Create a <code class="literal">KubeletConfig</code>, <code class="literal">cpumanager-kubeletconfig.yaml</code>, custom resource (CR). Refer to the label created in the previous step to have the correct nodes updated with the new kubelet config. See the <code class="literal">machineConfigPoolSelector</code> section:
					</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: cpumanager-enabled
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: cpumanager-enabled
  kubeletConfig:
     cpuManagerPolicy: static <span id="CO6-1"><!--Empty--></span><span class="callout">1</span>
     cpuManagerReconcilePeriod: 5s <span id="CO6-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO6-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specify a policy:
							</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">none</code>. This policy explicitly enables the existing default CPU affinity scheme, providing no affinity beyond what the scheduler does automatically. This is the default policy.
									</li><li class="listitem">
										<code class="literal">static</code>. This policy allows containers in guaranteed pods with integer CPU requests. It also limits access to exclusive CPUs on the node. If <code class="literal">static</code>, you must use a lowercase <code class="literal">s</code>.
									</li></ul></div></dd><dt><a href="#CO6-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Optional. Specify the CPU Manager reconcile frequency. The default is <code class="literal">5s</code>.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Create the dynamic kubelet config:
					</p><pre class="programlisting language-terminal"># oc create -f cpumanager-kubeletconfig.yaml</pre><p class="simpara">
						This adds the CPU Manager feature to the kubelet config and, if needed, the Machine Config Operator (MCO) reboots the node. To enable CPU Manager, a reboot is not needed.
					</p></li><li class="listitem"><p class="simpara">
						Check for the merged kubelet config:
					</p><pre class="programlisting language-terminal"># oc get machineconfig 99-worker-XXXXXX-XXXXX-XXXX-XXXXX-kubelet -o json | grep ownerReference -A7</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-json">       "ownerReferences": [
            {
                "apiVersion": "machineconfiguration.openshift.io/v1",
                "kind": "KubeletConfig",
                "name": "cpumanager-enabled",
                "uid": "7ed5616d-6b72-11e9-aae1-021e1ce18878"
            }
        ]</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Check the worker for the updated <code class="literal">kubelet.conf</code>:
					</p><pre class="programlisting language-terminal"># oc debug node/perf-node.example.com
sh-4.2# cat /host/etc/kubernetes/kubelet.conf | grep cpuManager</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">cpuManagerPolicy: static        <span id="CO7-1"><!--Empty--></span><span class="callout">1</span>
cpuManagerReconcilePeriod: 5s   <span id="CO7-2"><!--Empty--></span><span class="callout">2</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO7-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								<code class="literal">cpuManagerPolicy</code> is defined when you create the <code class="literal">KubeletConfig</code> CR.
							</div></dd><dt><a href="#CO7-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								<code class="literal">cpuManagerReconcilePeriod</code> is defined when you create the <code class="literal">KubeletConfig</code> CR.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Create a pod that requests a core or multiple cores. Both limits and requests must have their CPU value set to a whole integer. That is the number of cores that will be dedicated to this pod:
					</p><pre class="programlisting language-terminal"># cat cpumanager-pod.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  generateName: cpumanager-
spec:
  containers:
  - name: cpumanager
    image: gcr.io/google_containers/pause-amd64:3.0
    resources:
      requests:
        cpu: 1
        memory: "1G"
      limits:
        cpu: 1
        memory: "1G"
  nodeSelector:
    cpumanager: "true"</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Create the pod:
					</p><pre class="programlisting language-terminal"># oc create -f cpumanager-pod.yaml</pre></li><li class="listitem"><p class="simpara">
						Verify that the pod is scheduled to the node that you labeled:
					</p><pre class="programlisting language-terminal"># oc describe pod cpumanager</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">Name:               cpumanager-6cqz7
Namespace:          default
Priority:           0
PriorityClassName:  &lt;none&gt;
Node:  perf-node.example.com/xxx.xx.xx.xxx
...
 Limits:
      cpu:     1
      memory:  1G
    Requests:
      cpu:        1
      memory:     1G
...
QoS Class:       Guaranteed
Node-Selectors:  cpumanager=true</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Verify that the <code class="literal">cgroups</code> are set up correctly. Get the process ID (PID) of the <code class="literal">pause</code> process:
					</p><pre class="programlisting language-terminal"># ├─init.scope
│ └─1 /usr/lib/systemd/systemd --switched-root --system --deserialize 17
└─kubepods.slice
  ├─kubepods-pod69c01f8e_6b74_11e9_ac0f_0a2b62178a22.slice
  │ ├─crio-b5437308f1a574c542bdf08563b865c0345c8f8c0b0a655612c.scope
  │ └─32706 /pause</pre><p class="simpara">
						Pods of quality of service (QoS) tier <code class="literal">Guaranteed</code> are placed within the <code class="literal">kubepods.slice</code>. Pods of other QoS tiers end up in child <code class="literal">cgroups</code> of <code class="literal">kubepods</code>:
					</p><pre class="programlisting language-terminal"># cd /sys/fs/cgroup/cpuset/kubepods.slice/kubepods-pod69c01f8e_6b74_11e9_ac0f_0a2b62178a22.slice/crio-b5437308f1ad1a7db0574c542bdf08563b865c0345c86e9585f8c0b0a655612c.scope
# for i in `ls cpuset.cpus tasks` ; do echo -n "$i "; cat $i ; done</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">cpuset.cpus 1
tasks 32706</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Check the allowed CPU list for the task:
					</p><pre class="programlisting language-terminal"># grep ^Cpus_allowed_list /proc/32706/status</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal"> Cpus_allowed_list:    1</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Verify that another pod (in this case, the pod in the <code class="literal">burstable</code> QoS tier) on the system cannot run on the core allocated for the <code class="literal">Guaranteed</code> pod:
					</p><pre class="programlisting language-terminal"># cat /sys/fs/cgroup/cpuset/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-podc494a073_6b77_11e9_98c0_06bba5c387ea.slice/crio-c56982f57b75a2420947f0afc6cafe7534c5734efc34157525fa9abbf99e3849.scope/cpuset.cpus
0
# oc describe node perf-node.example.com</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">...
Capacity:
 attachable-volumes-aws-ebs:  39
 cpu:                         2
 ephemeral-storage:           124768236Ki
 hugepages-1Gi:               0
 hugepages-2Mi:               0
 memory:                      8162900Ki
 pods:                        250
Allocatable:
 attachable-volumes-aws-ebs:  39
 cpu:                         1500m
 ephemeral-storage:           124768236Ki
 hugepages-1Gi:               0
 hugepages-2Mi:               0
 memory:                      7548500Ki
 pods:                        250
-------                               ----                           ------------  ----------  ---------------  -------------  ---
  default                                 cpumanager-6cqz7               1 (66%)       1 (66%)     1G (12%)         1G (12%)       29m

Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource                    Requests          Limits
  --------                    --------          ------
  cpu                         1440m (96%)       1 (66%)</pre>

						</p></div><p class="simpara">
						This VM has two CPU cores. The <code class="literal">system-reserved</code> setting reserves 500 millicores, meaning that half of one core is subtracted from the total capacity of the node to arrive at the <code class="literal">Node Allocatable</code> amount. You can see that <code class="literal">Allocatable CPU</code> is 1500 millicores. This means you can run one of the CPU Manager pods since each will take one whole core. A whole core is equivalent to 1000 millicores. If you try to schedule a second pod, the system will accept the pod, but it will never be scheduled:
					</p><pre class="programlisting language-terminal">NAME                    READY   STATUS    RESTARTS   AGE
cpumanager-6cqz7        1/1     Running   0          33m
cpumanager-7qc2t        0/1     Pending   0          11s</pre></li></ol></div></section><section class="section" id="topology_manager_policies_using-cpu-manager-and-topology_manager"><div class="titlepage"><div><div><h2 class="title">5.2. Topology Manager policies</h2></div></div></div><p>
				Topology Manager aligns <code class="literal">Pod</code> resources of all Quality of Service (QoS) classes by collecting topology hints from Hint Providers, such as CPU Manager and Device Manager, and using the collected hints to align the <code class="literal">Pod</code> resources.
			</p><p>
				Topology Manager supports four allocation policies, which you assign in the <code class="literal">KubeletConfig</code> custom resource (CR) named <code class="literal">cpumanager-enabled</code>:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">none</code> policy</span></dt><dd>
							This is the default policy and does not perform any topology alignment.
						</dd><dt><span class="term"><code class="literal">best-effort</code> policy</span></dt><dd>
							For each container in a pod with the <code class="literal">best-effort</code> topology management policy, kubelet calls each Hint Provider to discover their resource availability. Using this information, the Topology Manager stores the preferred NUMA Node affinity for that container. If the affinity is not preferred, Topology Manager stores this and admits the pod to the node.
						</dd><dt><span class="term"><code class="literal">restricted</code> policy</span></dt><dd>
							For each container in a pod with the <code class="literal">restricted</code> topology management policy, kubelet calls each Hint Provider to discover their resource availability. Using this information, the Topology Manager stores the preferred NUMA Node affinity for that container. If the affinity is not preferred, Topology Manager rejects this pod from the node, resulting in a pod in a <code class="literal">Terminated</code> state with a pod admission failure.
						</dd><dt><span class="term"><code class="literal">single-numa-node</code> policy</span></dt><dd>
							For each container in a pod with the <code class="literal">single-numa-node</code> topology management policy, kubelet calls each Hint Provider to discover their resource availability. Using this information, the Topology Manager determines if a single NUMA Node affinity is possible. If it is, the pod is admitted to the node. If a single NUMA Node affinity is not possible, the Topology Manager rejects the pod from the node. This results in a pod in a Terminated state with a pod admission failure.
						</dd></dl></div></section><section class="section" id="seting_up_topology_manager_using-cpu-manager-and-topology_manager"><div class="titlepage"><div><div><h2 class="title">5.3. Setting up Topology Manager</h2></div></div></div><p>
				To use Topology Manager, you must configure an allocation policy in the <code class="literal">KubeletConfig</code> custom resource (CR) named <code class="literal">cpumanager-enabled</code>. This file might exist if you have set up CPU Manager. If the file does not exist, you can create the file.
			</p><div class="itemizedlist"><p class="title"><strong>Prequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Configure the CPU Manager policy to be <code class="literal">static</code>.
					</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					To activate Topololgy Manager:
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Configure the Topology Manager allocation policy in the custom resource.
					</p><pre class="programlisting language-terminal">$ oc edit KubeletConfig cpumanager-enabled</pre><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: cpumanager-enabled
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: cpumanager-enabled
  kubeletConfig:
     cpuManagerPolicy: static <span id="CO8-1"><!--Empty--></span><span class="callout">1</span>
     cpuManagerReconcilePeriod: 5s
     topologyManagerPolicy: single-numa-node <span id="CO8-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO8-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								This parameter must be <code class="literal">static</code> with a lowercase <code class="literal">s</code>.
							</div></dd><dt><a href="#CO8-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specify your selected Topology Manager allocation policy. Here, the policy is <code class="literal">single-numa-node</code>. Acceptable values are: <code class="literal">default</code>, <code class="literal">best-effort</code>, <code class="literal">restricted</code>, <code class="literal">single-numa-node</code>.
							</div></dd></dl></div></li></ol></div></section><section class="section" id="pod-interactions-with-topology-manager_using-cpu-manager-and-topology_manager"><div class="titlepage"><div><div><h2 class="title">5.4. Pod interactions with Topology Manager policies</h2></div></div></div><p>
				The example <code class="literal">Pod</code> specs below help illustrate pod interactions with Topology Manager.
			</p><p>
				The following pod runs in the <code class="literal">BestEffort</code> QoS class because no resource requests or limits are specified.
			</p><pre class="programlisting language-yaml">spec:
  containers:
  - name: nginx
    image: nginx</pre><p>
				The next pod runs in the <code class="literal">Burstable</code> QoS class because requests are less than limits.
			</p><pre class="programlisting language-yaml">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"</pre><p>
				If the selected policy is anything other than <code class="literal">none</code>, Topology Manager would not consider either of these <code class="literal">Pod</code> specifications.
			</p><p>
				The last example pod below runs in the Guaranteed QoS class because requests are equal to limits.
			</p><pre class="programlisting language-yaml">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "2"
        example.com/device: "1"
      requests:
        memory: "200Mi"
        cpu: "2"
        example.com/device: "1"</pre><p>
				Topology Manager would consider this pod. The Topology Manager would consult the hint providers, which are CPU Manager and Device Manager, to get topology hints for the pod.
			</p><p>
				Topology Manager will use this information to store the best topology for this container. In the case of this pod, CPU Manager and Device Manager will use this stored information at the resource allocation stage.
			</p></section></section><section class="chapter" id="cnf-numa-aware-scheduling"><div class="titlepage"><div><div><h1 class="title">Chapter 6. Scheduling NUMA-aware workloads</h1></div></div></div><p>
			Learn about NUMA-aware scheduling and how you can use it to deploy high performance workloads in an OpenShift Container Platform cluster.
		</p><p>
			The NUMA Resources Operator allows you to schedule high-performance workloads in the same NUMA zone. It deploys a node resources exporting agent that reports on available cluster node NUMA resources, and a secondary scheduler that manages the workloads.
		</p><section class="section" id="cnf-about-numa-aware-scheduling_numa-aware"><div class="titlepage"><div><div><h2 class="title">6.1. About NUMA-aware scheduling</h2></div></div></div><p>
				Non-Uniform Memory Access (NUMA) is a compute platform architecture that allows different CPUs to access different regions of memory at different speeds. NUMA resource topology refers to the locations of CPUs, memory, and PCI devices relative to each other in the compute node. Co-located resources are said to be in the same <span class="emphasis"><em>NUMA zone</em></span>. For high-performance applications, the cluster needs to process pod workloads in a single NUMA zone.
			</p><p>
				NUMA architecture allows a CPU with multiple memory controllers to use any available memory across CPU complexes, regardless of where the memory is located. This allows for increased flexibility at the expense of performance. A CPU processing a workload using memory that is outside its NUMA zone is slower than a workload processed in a single NUMA zone. Also, for I/O-constrained workloads, the network interface on a distant NUMA zone slows down how quickly information can reach the application. High-performance workloads, such as telecommunications workloads, cannot operate to specification under these conditions. NUMA-aware scheduling aligns the requested cluster compute resources (CPUs, memory, devices) in the same NUMA zone to process latency-sensitive or high-performance workloads efficiently. NUMA-aware scheduling also improves pod density per compute node for greater resource efficiency.
			</p><p>
				By integrating the Node Tuning Operator’s performance profile with NUMA-aware scheduling, you can further configure CPU affinity to optimize performance for latency-sensitive workloads.
			</p><p>
				The default OpenShift Container Platform pod scheduler scheduling logic considers the available resources of the entire compute node, not individual NUMA zones. If the most restrictive resource alignment is requested in the kubelet topology manager, error conditions can occur when admitting the pod to a node. Conversely, if the most restrictive resource alignment is not requested, the pod can be admitted to the node without proper resource alignment, leading to worse or unpredictable performance. For example, runaway pod creation with <code class="literal">Topology Affinity Error</code> statuses can occur when the pod scheduler makes suboptimal scheduling decisions for guaranteed pod workloads by not knowing if the pod’s requested resources are available. Scheduling mismatch decisions can cause indefinite pod startup delays. Also, depending on the cluster state and resource allocation, poor pod scheduling decisions can cause extra load on the cluster because of failed startup attempts.
			</p><p>
				The NUMA Resources Operator deploys a custom NUMA resources secondary scheduler and other resources to mitigate against the shortcomings of the default OpenShift Container Platform pod scheduler. The following diagram provides a high-level overview of NUMA-aware pod scheduling.
			</p><div class="figure" id="idm139735352051456"><p class="title"><strong>Figure 6.1. NUMA-aware scheduling overview</strong></p><div class="figure-contents"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Scalability_and_performance-en-US/images/4874f2bc876b7aa9f25e9fb2e11c12e9/216_OpenShift_Topology-aware_Scheduling_0222.png" alt="Diagram of NUMA-aware scheduling that shows how the various components interact with each other in the cluster"/></div></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">NodeResourceTopology API</span></dt><dd>
							The <code class="literal">NodeResourceTopology</code> API describes the available NUMA zone resources in each compute node.
						</dd><dt><span class="term">NUMA-aware scheduler</span></dt><dd>
							The NUMA-aware secondary scheduler receives information about the available NUMA zones from the <code class="literal">NodeResourceTopology</code> API and schedules high-performance workloads on a node where it can be optimally processed.
						</dd><dt><span class="term">Node topology exporter</span></dt><dd>
							The node topology exporter exposes the available NUMA zone resources for each compute node to the <code class="literal">NodeResourceTopology</code> API. The node topology exporter daemon tracks the resource allocation from the kubelet by using the <code class="literal">PodResources</code> API.
						</dd><dt><span class="term">PodResources API</span></dt><dd><p class="simpara">
							The <code class="literal">PodResources</code> API is local to each node and exposes the resource topology and available resources to the kubelet.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The <code class="literal">List</code> endpoint of the <code class="literal">PodResources</code> API exposes exclusive CPUs allocated to a particular container. The API does not expose CPUs that belong to a shared pool.
							</p><p>
								The <code class="literal">GetAllocatableResources</code> endpoint exposes allocatable resources available on a node.
							</p></div></div></dd></dl></div><div class="itemizedlist"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						For more information about running secondary pod schedulers in your cluster and how to deploy pods with a secondary pod scheduler, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#secondary-scheduler-configuring">Scheduling pods using a secondary scheduler</a>.
					</li></ul></div></section><section class="section" id="installing-the-numa-resources-operator_numa-aware"><div class="titlepage"><div><div><h2 class="title">6.2. Installing the NUMA Resources Operator</h2></div></div></div><p>
				NUMA Resources Operator deploys resources that allow you to schedule NUMA-aware workloads and deployments. You can install the NUMA Resources Operator using the OpenShift Container Platform CLI or the web console.
			</p><section class="section" id="cnf-installing-numa-resources-operator-cli_numa-aware"><div class="titlepage"><div><div><h3 class="title">6.2.1. Installing the NUMA Resources Operator using the CLI</h3></div></div></div><p>
					As a cluster administrator, you can install the Operator using the CLI.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a namespace for the NUMA Resources Operator:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Save the following YAML in the <code class="literal">nro-namespace.yaml</code> file:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:
  name: openshift-numaresources</pre></li><li class="listitem"><p class="simpara">
									Create the <code class="literal">Namespace</code> CR by running the following command:
								</p><pre class="programlisting language-terminal">$ oc create -f nro-namespace.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create the Operator group for the NUMA Resources Operator:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Save the following YAML in the <code class="literal">nro-operatorgroup.yaml</code> file:
								</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: numaresources-operator
  namespace: openshift-numaresources
spec:
  targetNamespaces:
  - openshift-numaresources</pre></li><li class="listitem"><p class="simpara">
									Create the <code class="literal">OperatorGroup</code> CR by running the following command:
								</p><pre class="programlisting language-terminal">$ oc create -f nro-operatorgroup.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create the subscription for the NUMA Resources Operator:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Save the following YAML in the <code class="literal">nro-sub.yaml</code> file:
								</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1
kind: Subscription
metadata:
  name: numaresources-operator
  namespace: openshift-numaresources
spec:
  channel: "4.13"
  name: numaresources-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace</pre></li><li class="listitem"><p class="simpara">
									Create the <code class="literal">Subscription</code> CR by running the following command:
								</p><pre class="programlisting language-terminal">$ oc create -f nro-sub.yaml</pre></li></ol></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Verify that the installation succeeded by inspecting the CSV resource in the <code class="literal">openshift-numaresources</code> namespace. Run the following command:
						</p><pre class="programlisting language-terminal">$ oc get csv -n openshift-numaresources</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                             DISPLAY                  VERSION   REPLACES   PHASE
numaresources-operator.v4.13.2   numaresources-operator   4.13.2               Succeeded</pre>

							</p></div></li></ol></div></section><section class="section" id="cnf-installing-numa-resources-operator-console_numa-aware"><div class="titlepage"><div><div><h3 class="title">6.2.2. Installing the NUMA Resources Operator using the web console</h3></div></div></div><p>
					As a cluster administrator, you can install the NUMA Resources Operator using the web console.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Install the NUMA Resources Operator using the OpenShift Container Platform web console:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									In the OpenShift Container Platform web console, click <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span>.
								</li><li class="listitem">
									Choose <span class="strong strong"><strong>NUMA Resources Operator</strong></span> from the list of available Operators, and then click <span class="strong strong"><strong>Install</strong></span>.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Optional: Verify that the NUMA Resources Operator installed successfully:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Switch to the <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span> page.
								</li><li class="listitem"><p class="simpara">
									Ensure that <span class="strong strong"><strong>NUMA Resources Operator</strong></span> is listed in the <span class="strong strong"><strong>default</strong></span> project with a <span class="strong strong"><strong>Status</strong></span> of <span class="strong strong"><strong>InstallSucceeded</strong></span>.
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										During installation an Operator might display a <span class="strong strong"><strong>Failed</strong></span> status. If the installation later succeeds with an <span class="strong strong"><strong>InstallSucceeded</strong></span> message, you can ignore the <span class="strong strong"><strong>Failed</strong></span> message.
									</p></div></div><p class="simpara">
									If the Operator does not appear as installed, to troubleshoot further:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											Go to the <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span> page and inspect the <span class="strong strong"><strong>Operator Subscriptions</strong></span> and <span class="strong strong"><strong>Install Plans</strong></span> tabs for any failure or errors under <span class="strong strong"><strong>Status</strong></span>.
										</li><li class="listitem">
											Go to the <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Pods</strong></span> page and check the logs for pods in the <code class="literal">default</code> project.
										</li></ul></div></li></ol></div></li></ol></div></section></section><section class="section" id="cnf-scheduling-numa-aware-workloads-overview_numa-aware"><div class="titlepage"><div><div><h2 class="title">6.3. Scheduling NUMA-aware workloads</h2></div></div></div><p>
				Clusters running latency-sensitive workloads typically feature performance profiles that help to minimize workload latency and optimize performance. The NUMA-aware scheduler deploys workloads based on available node NUMA resources and with respect to any performance profile settings applied to the node. The combination of NUMA-aware deployments, and the performance profile of the workload, ensures that workloads are scheduled in a way that maximizes performance.
			</p><section class="section" id="cnf-creating-nrop-cr_numa-aware"><div class="titlepage"><div><div><h3 class="title">6.3.1. Creating the NUMAResourcesOperator custom resource</h3></div></div></div><p>
					When you have installed the NUMA Resources Operator, then create the <code class="literal">NUMAResourcesOperator</code> custom resource (CR) that instructs the NUMA Resources Operator to install all the cluster infrastructure needed to support the NUMA-aware scheduler, including daemon sets and APIs.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							Install the NUMA Resources Operator.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the <code class="literal">NUMAResourcesOperator</code> custom resource:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Save the following YAML in the <code class="literal">nrop.yaml</code> file:
								</p><pre class="programlisting language-yaml">apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesOperator
metadata:
  name: numaresourcesoperator
spec:
  nodeGroups:
  - machineConfigPoolSelector:
      matchLabels:
        pools.operator.machineconfiguration.openshift.io/worker: ""</pre></li><li class="listitem"><p class="simpara">
									Create the <code class="literal">NUMAResourcesOperator</code> CR by running the following command:
								</p><pre class="programlisting language-terminal">$ oc create -f nrop.yaml</pre></li></ol></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Verify that the NUMA Resources Operator deployed successfully by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get numaresourcesoperators.nodetopology.openshift.io</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                    AGE
numaresourcesoperator   10m</pre>

							</p></div></li></ul></div></section><section class="section" id="cnf-deploying-the-numa-aware-scheduler_numa-aware"><div class="titlepage"><div><div><h3 class="title">6.3.2. Deploying the NUMA-aware secondary pod scheduler</h3></div></div></div><p>
					After you install the NUMA Resources Operator, do the following to deploy the NUMA-aware secondary pod scheduler:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Configure the performance profile.
						</li><li class="listitem">
							Deploy the NUMA-aware secondary scheduler.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							Create the required machine config pool.
						</li><li class="listitem">
							Install the NUMA Resources Operator.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the <code class="literal">PerformanceProfile</code> custom resource (CR):
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Save the following YAML in the <code class="literal">nro-perfprof.yaml</code> file:
								</p><pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: perfprof-nrop
spec:
  cpu: <span id="CO9-1"><!--Empty--></span><span class="callout">1</span>
    isolated: "4-51,56-103"
    reserved: "0,1,2,3,52,53,54,55"
  nodeSelector:
    node-role.kubernetes.io/worker: ""
  numa:
    topologyPolicy: single-numa-node</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO9-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The <code class="literal">cpu.isolated</code> and <code class="literal">cpu.reserved</code> specifications define ranges for isolated and reserved CPUs. Enter valid values for your CPU configuration. See the <span class="emphasis"><em>Additional resources</em></span> section for more information about configuring a performance profile.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Create the <code class="literal">PerformanceProfile</code> CR by running the following command:
								</p><pre class="programlisting language-terminal">$ oc create -f nro-perfprof.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">performanceprofile.performance.openshift.io/perfprof-nrop created</pre>

									</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">NUMAResourcesScheduler</code> custom resource that deploys the NUMA-aware custom pod scheduler:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Save the following YAML in the <code class="literal">nro-scheduler.yaml</code> file:
								</p><pre class="programlisting language-yaml">apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesScheduler
metadata:
  name: numaresourcesscheduler
spec:
  imageSpec: "registry.redhat.io/openshift4/noderesourcetopology-scheduler-container-rhel8:v4.13"
  cacheResyncPeriod: "5s" <span id="CO10-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO10-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Enter an interval value in seconds for synchronization of the scheduler cache. A value of <code class="literal">5s</code> is typical for most implementations.
										</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												Enable the <code class="literal">cacheResyncPeriod</code> specification to help the NUMA Resource Operator report more exact resource availability by monitoring pending resources on nodes and synchronizing this information in the scheduler cache at a defined interval. This also helps to minimize <code class="literal">Topology Affinity Error</code> errors because of sub-optimal scheduling decisions. The lower the interval the greater the network load. The <code class="literal">cacheResyncPeriod</code> specification is disabled by default.
											</li><li class="listitem">
												Setting a value of <code class="literal">Enabled</code> for the <code class="literal">podsFingerprinting</code> specification in the <code class="literal">NUMAResourcesOperator</code> CR is a requirement for the implementation of the <code class="literal">cacheResyncPeriod</code> specification.
											</li></ul></div></div></div></li><li class="listitem"><p class="simpara">
									Create the <code class="literal">NUMAResourcesScheduler</code> CR by running the following command:
								</p><pre class="programlisting language-terminal">$ oc create -f nro-scheduler.yaml</pre></li></ol></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Verify that the performance profile was applied by running the following command:
						</p><pre class="programlisting language-terminal">$ oc describe performanceprofile &lt;performance-profile-name&gt;</pre></li><li class="listitem"><p class="simpara">
							Verify that the required resources deployed successfully by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get all -n openshift-numaresources</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                                    READY   STATUS    RESTARTS   AGE
pod/numaresources-controller-manager-7575848485-bns4s   1/1     Running   0          13m
pod/numaresourcesoperator-worker-dvj4n                  2/2     Running   0          16m
pod/numaresourcesoperator-worker-lcg4t                  2/2     Running   0          16m
pod/secondary-scheduler-56994cf6cf-7qf4q                1/1     Running   0          16m
NAME                                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                     AGE
daemonset.apps/numaresourcesoperator-worker   2         2         2       2            2           node-role.kubernetes.io/worker=   16m
NAME                                               READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/numaresources-controller-manager   1/1     1            1           13m
deployment.apps/secondary-scheduler                1/1     1            1           16m
NAME                                                          DESIRED   CURRENT   READY   AGE
replicaset.apps/numaresources-controller-manager-7575848485   1         1         1       13m
replicaset.apps/secondary-scheduler-56994cf6cf                1         1         1       16m</pre>

							</p></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#cnf-about-the-profile-creator-tool_cnf-create-performance-profiles">About the Performance Profile Creator</a>.
						</li></ul></div></section><section class="section" id="cnf-scheduling-numa-aware-workloads_numa-aware"><div class="titlepage"><div><div><h3 class="title">6.3.3. Scheduling workloads with the NUMA-aware scheduler</h3></div></div></div><p>
					You can schedule workloads with the NUMA-aware scheduler using <code class="literal">Deployment</code> CRs that specify the minimum required resources to process the workload.
				</p><p>
					The following example deployment uses NUMA-aware scheduling for a sample workload.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							Install the NUMA Resources Operator and deploy the NUMA-aware secondary scheduler.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Get the name of the NUMA-aware scheduler that is deployed in the cluster by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get numaresourcesschedulers.nodetopology.openshift.io numaresourcesscheduler -o json | jq '.status.schedulerName'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">topo-aware-scheduler</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">Deployment</code> CR that uses scheduler named <code class="literal">topo-aware-scheduler</code>, for example:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Save the following YAML in the <code class="literal">nro-deployment.yaml</code> file:
								</p><pre class="programlisting language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: numa-deployment-1
  namespace: openshift-numaresources
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      schedulerName: topo-aware-scheduler <span id="CO11-1"><!--Empty--></span><span class="callout">1</span>
      containers:
      - name: ctnr
        image: quay.io/openshifttest/hello-openshift:openshift
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            memory: "100Mi"
            cpu: "10"
          requests:
            memory: "100Mi"
            cpu: "10"
      - name: ctnr2
        image: registry.access.redhat.com/rhel:latest
        imagePullPolicy: IfNotPresent
        command: ["/bin/sh", "-c"]
        args: [ "while true; do sleep 1h; done;" ]
        resources:
          limits:
            memory: "100Mi"
            cpu: "8"
          requests:
            memory: "100Mi"
            cpu: "8"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO11-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											<code class="literal">schedulerName</code> must match the name of the NUMA-aware scheduler that is deployed in your cluster, for example <code class="literal">topo-aware-scheduler</code>.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Create the <code class="literal">Deployment</code> CR by running the following command:
								</p><pre class="programlisting language-terminal">$ oc create -f nro-deployment.yaml</pre></li></ol></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Verify that the deployment was successful:
						</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-numaresources</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                                READY   STATUS    RESTARTS   AGE
numa-deployment-1-56954b7b46-pfgw8                  2/2     Running   0          129m
numaresources-controller-manager-7575848485-bns4s   1/1     Running   0          15h
numaresourcesoperator-worker-dvj4n                  2/2     Running   0          18h
numaresourcesoperator-worker-lcg4t                  2/2     Running   0          16h
secondary-scheduler-56994cf6cf-7qf4q                1/1     Running   0          18h</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Verify that the <code class="literal">topo-aware-scheduler</code> is scheduling the deployed pod by running the following command:
						</p><pre class="programlisting language-terminal">$ oc describe pod numa-deployment-1-56954b7b46-pfgw8 -n openshift-numaresources</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Events:
  Type    Reason          Age   From                  Message
  ----    ------          ----  ----                  -------
  Normal  Scheduled       130m  topo-aware-scheduler  Successfully assigned openshift-numaresources/numa-deployment-1-56954b7b46-pfgw8 to compute-0.example.com</pre>

							</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Deployments that request more resources than is available for scheduling will fail with a <code class="literal">MinimumReplicasUnavailable</code> error. The deployment succeeds when the required resources become available. Pods remain in the <code class="literal">Pending</code> state until the required resources are available.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Verify that the expected allocated resources are listed for the node.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Identify the node that is running the deployment pod by running the following command, replacing &lt;namespace&gt; with the namespace you specified in the <code class="literal">Deployment</code> CR:
								</p><pre class="programlisting language-terminal">$ oc get pods -n &lt;namespace&gt; -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                 READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
numa-deployment-1-65684f8fcc-bw4bw   0/2     Running   0          82m   10.128.2.50   worker-0   &lt;none&gt;  &lt;none&gt;</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Run the following command, replacing &lt;node_name&gt; with the name of that node that is running the deployment pod.
								</p><pre class="programlisting language-terminal">$ oc describe noderesourcetopologies.topology.node.k8s.io</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">...

Zones:
  Costs:
    Name:   node-0
    Value:  10
    Name:   node-1
    Value:  21
  Name:     node-0
  Resources:
    Allocatable:  39
    Available:    21 <span id="CO12-1"><!--Empty--></span><span class="callout">1</span>
    Capacity:     40
    Name:         cpu
    Allocatable:  6442450944
    Available:    6442450944
    Capacity:     6442450944
    Name:         hugepages-1Gi
    Allocatable:  134217728
    Available:    134217728
    Capacity:     134217728
    Name:         hugepages-2Mi
    Allocatable:  262415904768
    Available:    262206189568
    Capacity:     270146007040
    Name:         memory
  Type:           Node</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO12-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The <code class="literal">Available</code> capacity is reduced because of the resources that have been allocated to the guaranteed pod.
										</div></dd></dl></div><p class="simpara">
									Resources consumed by guaranteed pods are subtracted from the available node resources listed under <code class="literal">noderesourcetopologies.topology.node.k8s.io</code>.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Resource allocations for pods with a <code class="literal">Best-effort</code> or <code class="literal">Burstable</code> quality of service (<code class="literal">qosClass</code>) are not reflected in the NUMA node resources under <code class="literal">noderesourcetopologies.topology.node.k8s.io</code>. If a pod’s consumed resources are not reflected in the node resource calculation, verify that the pod has <code class="literal">qosClass</code> of <code class="literal">Guaranteed</code> and the CPU request is an integer value, not a decimal value. You can verify the that the pod has a <code class="literal">qosClass</code> of <code class="literal">Guaranteed</code> by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get pod &lt;pod_name&gt; -n &lt;pod_namespace&gt; -o jsonpath="{ .status.qosClass }"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Guaranteed</pre>

							</p></div></li></ol></div></section></section><section class="section" id="cnf-scheduling-numa-aware-workloads-with-manual-perofrmance-settings_numa-aware"><div class="titlepage"><div><div><h2 class="title">6.4. Scheduling NUMA-aware workloads with manual performance settings</h2></div></div></div><p>
				Clusters running latency-sensitive workloads typically feature performance profiles that help to minimize workload latency and optimize performance. However, you can schedule NUMA-aware workloads in a pristine cluster that does not feature a performance profile. The following workflow features a pristine cluster that you can manually configure for performance by using the <code class="literal">KubeletConfig</code> resource. This is not the typical environment for scheduling NUMA-aware workloads.
			</p><section class="section" id="cnf-creating-nrop-cr-with-manual-performance-settings_numa-aware"><div class="titlepage"><div><div><h3 class="title">6.4.1. Creating the NUMAResourcesOperator custom resource with manual performance settings</h3></div></div></div><p>
					When you have installed the NUMA Resources Operator, then create the <code class="literal">NUMAResourcesOperator</code> custom resource (CR) that instructs the NUMA Resources Operator to install all the cluster infrastructure needed to support the NUMA-aware scheduler, including daemon sets and APIs.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							Install the NUMA Resources Operator.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Optional: Create the <code class="literal">MachineConfigPool</code> custom resource that enables custom kubelet configurations for worker nodes:
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								By default, OpenShift Container Platform creates a <code class="literal">MachineConfigPool</code> resource for worker nodes in the cluster. You can create a custom <code class="literal">MachineConfigPool</code> resource if required.
							</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Save the following YAML in the <code class="literal">nro-machineconfig.yaml</code> file:
								</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  labels:
    cnf-worker-tuning: enabled
    machineconfiguration.openshift.io/mco-built-in: ""
    pools.operator.machineconfiguration.openshift.io/worker: ""
  name: worker
spec:
  machineConfigSelector:
    matchLabels:
      machineconfiguration.openshift.io/role: worker
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker: ""</pre></li><li class="listitem"><p class="simpara">
									Create the <code class="literal">MachineConfigPool</code> CR by running the following command:
								</p><pre class="programlisting language-terminal">$ oc create -f nro-machineconfig.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">NUMAResourcesOperator</code> custom resource:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Save the following YAML in the <code class="literal">nrop.yaml</code> file:
								</p><pre class="programlisting language-yaml">apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesOperator
metadata:
  name: numaresourcesoperator
spec:
  nodeGroups:
  - machineConfigPoolSelector:
      matchLabels:
        pools.operator.machineconfiguration.openshift.io/worker: "" <span id="CO13-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO13-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Should match the label applied to worker nodes in the related <code class="literal">MachineConfigPool</code> CR.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Create the <code class="literal">NUMAResourcesOperator</code> CR by running the following command:
								</p><pre class="programlisting language-terminal">$ oc create -f nrop.yaml</pre></li></ol></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Verify that the NUMA Resources Operator deployed successfully by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get numaresourcesoperators.nodetopology.openshift.io</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                    AGE
numaresourcesoperator   10m</pre>

							</p></div></li></ul></div></section><section class="section" id="cnf-deploying-the-numa-aware-scheduler-with-manual-performance-settings_numa-aware"><div class="titlepage"><div><div><h3 class="title">6.4.2. Deploying the NUMA-aware secondary pod scheduler with manual performance settings</h3></div></div></div><p>
					After you install the NUMA Resources Operator, do the following to deploy the NUMA-aware secondary pod scheduler:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Configure the pod admittance policy for the required machine profile
						</li><li class="listitem">
							Create the required machine config pool
						</li><li class="listitem">
							Deploy the NUMA-aware secondary scheduler
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							Install the NUMA Resources Operator.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the <code class="literal">KubeletConfig</code> custom resource that configures the pod admittance policy for the machine profile:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Save the following YAML in the <code class="literal">nro-kubeletconfig.yaml</code> file:
								</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: cnf-worker-tuning
spec:
  machineConfigPoolSelector:
    matchLabels:
      cnf-worker-tuning: enabled
  kubeletConfig:
    cpuManagerPolicy: "static" <span id="CO14-1"><!--Empty--></span><span class="callout">1</span>
    cpuManagerReconcilePeriod: "5s"
    reservedSystemCPUs: "0,1"
    memoryManagerPolicy: "Static" <span id="CO14-2"><!--Empty--></span><span class="callout">2</span>
    evictionHard:
      memory.available: "100Mi"
    kubeReserved:
      memory: "512Mi"
    reservedMemory:
      - numaNode: 0
        limits:
          memory: "1124Mi"
    systemReserved:
      memory: "512Mi"
    topologyManagerPolicy: "single-numa-node" <span id="CO14-3"><!--Empty--></span><span class="callout">3</span>
    topologyManagerScope: "pod"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO14-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											For <code class="literal">cpuManagerPolicy</code>, <code class="literal">static</code> must use a lowercase <code class="literal">s</code>.
										</div></dd><dt><a href="#CO14-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											For <code class="literal">memoryManagerPolicy</code>, <code class="literal">Static</code> must use an uppercase <code class="literal">S</code>.
										</div></dd><dt><a href="#CO14-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											<code class="literal">topologyManagerPolicy</code> must be set to <code class="literal">single-numa-node</code>.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Create the <code class="literal">KubeletConfig</code> custom resource (CR) by running the following command:
								</p><pre class="programlisting language-terminal">$ oc create -f nro-kubeletconfig.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">NUMAResourcesScheduler</code> custom resource that deploys the NUMA-aware custom pod scheduler:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Save the following YAML in the <code class="literal">nro-scheduler.yaml</code> file:
								</p><pre class="programlisting language-yaml">apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesScheduler
metadata:
  name: numaresourcesscheduler
spec:
  imageSpec: "registry.redhat.io/openshift4/noderesourcetopology-scheduler-container-rhel8:v4.13"
  cacheResyncPeriod: "5s" <span id="CO15-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO15-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Enter an interval value in seconds for synchronization of the scheduler cache. A value of <code class="literal">5s</code> is typical for most implementations.
										</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												Enable the <code class="literal">cacheResyncPeriod</code> specification to help the NUMA Resource Operator report more exact resource availability by monitoring pending resources on nodes and synchronizing this information in the scheduler cache at a defined interval. This also helps to minimize <code class="literal">Topology Affinity Error</code> errors because of sub-optimal scheduling decisions. The lower the interval the greater the network load. The <code class="literal">cacheResyncPeriod</code> specification is disabled by default.
											</li><li class="listitem">
												Setting a value of <code class="literal">Enabled</code> for the <code class="literal">podsFingerprinting</code> specification in the <code class="literal">NUMAResourcesOperator</code> CR is a requirement for the implementation of the <code class="literal">cacheResyncPeriod</code> specification.
											</li></ul></div></div></div></li><li class="listitem"><p class="simpara">
									Create the <code class="literal">NUMAResourcesScheduler</code> CR by running the following command:
								</p><pre class="programlisting language-terminal">$ oc create -f nro-scheduler.yaml</pre></li></ol></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Verify that the required resources deployed successfully by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get all -n openshift-numaresources</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                                    READY   STATUS    RESTARTS   AGE
pod/numaresources-controller-manager-7575848485-bns4s   1/1     Running   0          13m
pod/numaresourcesoperator-worker-dvj4n                  2/2     Running   0          16m
pod/numaresourcesoperator-worker-lcg4t                  2/2     Running   0          16m
pod/secondary-scheduler-56994cf6cf-7qf4q                1/1     Running   0          16m
NAME                                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                     AGE
daemonset.apps/numaresourcesoperator-worker   2         2         2       2            2           node-role.kubernetes.io/worker=   16m
NAME                                               READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/numaresources-controller-manager   1/1     1            1           13m
deployment.apps/secondary-scheduler                1/1     1            1           16m
NAME                                                          DESIRED   CURRENT   READY   AGE
replicaset.apps/numaresources-controller-manager-7575848485   1         1         1       13m
replicaset.apps/secondary-scheduler-56994cf6cf                1         1         1       16m</pre>

							</p></div></li></ul></div></section><section class="section" id="cnf-scheduling-numa-aware-workloads-with-manual-performance-setttings_numa-aware"><div class="titlepage"><div><div><h3 class="title">6.4.3. Scheduling workloads with the NUMA-aware scheduler with manual performance settings</h3></div></div></div><p>
					You can schedule workloads with the NUMA-aware scheduler using <code class="literal">Deployment</code> CRs that specify the minimum required resources to process the workload.
				</p><p>
					The following example deployment uses NUMA-aware scheduling for a sample workload.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							Install the NUMA Resources Operator and deploy the NUMA-aware secondary scheduler.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Get the name of the NUMA-aware scheduler that is deployed in the cluster by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get numaresourcesschedulers.nodetopology.openshift.io numaresourcesscheduler -o json | jq '.status.schedulerName'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">topo-aware-scheduler</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">Deployment</code> CR that uses scheduler named <code class="literal">topo-aware-scheduler</code>, for example:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Save the following YAML in the <code class="literal">nro-deployment.yaml</code> file:
								</p><pre class="programlisting language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: numa-deployment-1
  namespace: &lt;namespace&gt; <span id="CO16-1"><!--Empty--></span><span class="callout">1</span>
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      schedulerName: topo-aware-scheduler <span id="CO16-2"><!--Empty--></span><span class="callout">2</span>
      containers:
      - name: ctnr
        image: quay.io/openshifttest/hello-openshift:openshift
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            memory: "100Mi"
            cpu: "10"
          requests:
            memory: "100Mi"
            cpu: "10"
      - name: ctnr2
        image: gcr.io/google_containers/pause-amd64:3.0
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            memory: "100Mi"
            cpu: "8"
          requests:
            memory: "100Mi"
            cpu: "8"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO16-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Replace with the namespace for your deployment.
										</div></dd><dt><a href="#CO16-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											<code class="literal">schedulerName</code> must match the name of the NUMA-aware scheduler that is deployed in your cluster, for example <code class="literal">topo-aware-scheduler</code>.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Create the <code class="literal">Deployment</code> CR by running the following command:
								</p><pre class="programlisting language-terminal">$ oc create -f nro-deployment.yaml</pre></li></ol></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Verify that the deployment was successful:
						</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-numaresources</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                                READY   STATUS    RESTARTS   AGE
numa-deployment-1-56954b7b46-pfgw8                  2/2     Running   0          129m
numaresources-controller-manager-7575848485-bns4s   1/1     Running   0          15h
numaresourcesoperator-worker-dvj4n                  2/2     Running   0          18h
numaresourcesoperator-worker-lcg4t                  2/2     Running   0          16h
secondary-scheduler-56994cf6cf-7qf4q                1/1     Running   0          18h</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Verify that the <code class="literal">topo-aware-scheduler</code> is scheduling the deployed pod by running the following command:
						</p><pre class="programlisting language-terminal">$ oc describe pod numa-deployment-1-56954b7b46-pfgw8 -n openshift-numaresources</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Events:
  Type    Reason          Age   From                  Message
  ----    ------          ----  ----                  -------
  Normal  Scheduled       130m  topo-aware-scheduler  Successfully assigned openshift-numaresources/numa-deployment-1-56954b7b46-pfgw8 to compute-0.example.com</pre>

							</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Deployments that request more resources than is available for scheduling will fail with a <code class="literal">MinimumReplicasUnavailable</code> error. The deployment succeeds when the required resources become available. Pods remain in the <code class="literal">Pending</code> state until the required resources are available.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Verify that the expected allocated resources are listed for the node.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Identify the node that is running the deployment pod by running the following command, replacing &lt;namespace&gt; with the namespace you specified in the <code class="literal">Deployment</code> CR:
								</p><pre class="programlisting language-terminal">$ oc get pods -n &lt;namespace&gt; -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                 READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
numa-deployment-1-65684f8fcc-bw4bw   0/2     Running   0          82m   10.128.2.50   worker-0   &lt;none&gt;  &lt;none&gt;</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Run the following command, replacing &lt;node_name&gt; with the name of that node that is running the deployment pod:
								</p><pre class="programlisting language-terminal">$ oc describe noderesourcetopologies.topology.node.k8s.io &lt;node_name&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">...

Zones:
  Costs:
    Name:   node-0
    Value:  10
    Name:   node-1
    Value:  21
  Name:     node-0
  Resources:
    Allocatable:  39
    Available:    21 <span id="CO17-1"><!--Empty--></span><span class="callout">1</span>
    Capacity:     40
    Name:         cpu
    Allocatable:  6442450944
    Available:    6442450944
    Capacity:     6442450944
    Name:         hugepages-1Gi
    Allocatable:  134217728
    Available:    134217728
    Capacity:     134217728
    Name:         hugepages-2Mi
    Allocatable:  262415904768
    Available:    262206189568
    Capacity:     270146007040
    Name:         memory
  Type:           Node</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO17-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The <code class="literal">Available</code> capacity is reduced because of the resources that have been allocated to the guaranteed pod.
										</div></dd></dl></div><p class="simpara">
									Resources consumed by guaranteed pods are subtracted from the available node resources listed under <code class="literal">noderesourcetopologies.topology.node.k8s.io</code>.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Resource allocations for pods with a <code class="literal">Best-effort</code> or <code class="literal">Burstable</code> quality of service (<code class="literal">qosClass</code>) are not reflected in the NUMA node resources under <code class="literal">noderesourcetopologies.topology.node.k8s.io</code>. If a pod’s consumed resources are not reflected in the node resource calculation, verify that the pod has <code class="literal">qosClass</code> of <code class="literal">Guaranteed</code> and the CPU request is an integer value, not a decimal value. You can verify the that the pod has a <code class="literal">qosClass</code> of <code class="literal">Guaranteed</code> by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get pod &lt;pod_name&gt; -n &lt;pod_namespace&gt; -o jsonpath="{ .status.qosClass }"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Guaranteed</pre>

							</p></div></li></ol></div></section></section><section class="section" id="cnf-configuring-node-groups-for-the-numaresourcesoperator_numa-aware"><div class="titlepage"><div><div><h2 class="title">6.5. Optional: Configuring polling operations for NUMA resources updates</h2></div></div></div><p>
				The daemons controlled by the NUMA Resources Operator in their <code class="literal">nodeGroup</code> poll resources to retrieve updates about available NUMA resources. You can fine-tune polling operations for these daemons by configuring the <code class="literal">spec.nodeGroups</code> specification in the <code class="literal">NUMAResourcesOperator</code> custom resource (CR). This provides advanced control of polling operations. Configure these specifications to improve scheduling behaviour and troubleshoot suboptimal scheduling decisions.
			</p><p>
				The configuration options are the following:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">infoRefreshMode</code>: Determines the trigger condition for polling the kubelet. The NUMA Resources Operator reports the resulting information to the API server.
					</li><li class="listitem">
						<code class="literal">infoRefreshPeriod</code>: Determines the duration between polling updates.
					</li><li class="listitem"><p class="simpara">
						<code class="literal">podsFingerprinting</code>: Determines if point-in-time information for the current set of pods running on a node is exposed in polling updates.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							<code class="literal">podsFingerprinting</code> is enabled by default. <code class="literal">podsFingerprinting</code> is a requirement for the <code class="literal">cacheResyncPeriod</code> specification in the <code class="literal">NUMAResourcesScheduler</code> CR. The <code class="literal">cacheResyncPeriod</code> specification helps to report more exact resource availability by monitoring pending resources on nodes.
						</p></div></div></li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						Log in as a user with <code class="literal">cluster-admin</code> privileges.
					</li><li class="listitem">
						Install the NUMA Resources Operator.
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Configure the <code class="literal">spec.nodeGroups</code> specification in your <code class="literal">NUMAResourcesOperator</code> CR:
					</p><pre class="programlisting language-yaml">apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesOperator
metadata:
  name: numaresourcesoperator
spec:
  nodeGroups:
  - config:
      infoRefreshMode: Periodic <span id="CO18-1"><!--Empty--></span><span class="callout">1</span>
      infoRefreshPeriod: 10s <span id="CO18-2"><!--Empty--></span><span class="callout">2</span>
      podsFingerprinting: Enabled <span id="CO18-3"><!--Empty--></span><span class="callout">3</span>
    name: worker</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO18-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Valid values are <code class="literal">Periodic</code>, <code class="literal">Events</code>, <code class="literal">PeriodicAndEvents</code>. Use <code class="literal">Periodic</code> to poll the kubelet at intervals that you define in <code class="literal">infoRefreshPeriod</code>. Use <code class="literal">Events</code> to poll the kubelet at every pod lifecycle event. Use <code class="literal">PeriodicAndEvents</code> to enable both methods.
							</div></dd><dt><a href="#CO18-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Define the polling interval for <code class="literal">Periodic</code> or <code class="literal">PeriodicAndEvents</code> refresh modes. The field is ignored if the refresh mode is <code class="literal">Events</code>.
							</div></dd><dt><a href="#CO18-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Valid values are <code class="literal">Enabled</code> or <code class="literal">Disabled</code>. Setting to <code class="literal">Enabled</code> is a requirement for the <code class="literal">cacheResyncPeriod</code> specification in the <code class="literal">NUMAResourcesScheduler</code>.
							</div></dd></dl></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						After you deploy the NUMA Resources Operator, verify that the node group configurations were applied by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get numaresop numaresourcesoperator -o json | jq '.status'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">      ...

        "config": {
        "infoRefreshMode": "Periodic",
        "infoRefreshPeriod": "10s",
        "podsFingerprinting": "Enabled"
      },
      "name": "worker"

      ...</pre>

						</p></div></li></ol></div></section><section class="section" id="cnf-troubleshooting-numa-aware-workloads_numa-aware"><div class="titlepage"><div><div><h2 class="title">6.6. Troubleshooting NUMA-aware scheduling</h2></div></div></div><p>
				To troubleshoot common problems with NUMA-aware pod scheduling, perform the following steps.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install the OpenShift Container Platform CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						Log in as a user with cluster-admin privileges.
					</li><li class="listitem">
						Install the NUMA Resources Operator and deploy the NUMA-aware secondary scheduler.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Verify that the <code class="literal">noderesourcetopologies</code> CRD is deployed in the cluster by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get crd | grep noderesourcetopologies</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                                                              CREATED AT
noderesourcetopologies.topology.node.k8s.io                       2022-01-18T08:28:06Z</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Check that the NUMA-aware scheduler name matches the name specified in your NUMA-aware workloads by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get numaresourcesschedulers.nodetopology.openshift.io numaresourcesscheduler -o json | jq '.status.schedulerName'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">topo-aware-scheduler</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Verify that NUMA-aware scheduable nodes have the <code class="literal">noderesourcetopologies</code> CR applied to them. Run the following command:
					</p><pre class="programlisting language-terminal">$ oc get noderesourcetopologies.topology.node.k8s.io</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                    AGE
compute-0.example.com   17h
compute-1.example.com   17h</pre>

						</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The number of nodes should equal the number of worker nodes that are configured by the machine config pool (<code class="literal">mcp</code>) worker definition.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Verify the NUMA zone granularity for all scheduable nodes by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get noderesourcetopologies.topology.node.k8s.io -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: v1
items:
- apiVersion: topology.node.k8s.io/v1
  kind: NodeResourceTopology
  metadata:
    annotations:
      k8stopoawareschedwg/rte-update: periodic
    creationTimestamp: "2022-06-16T08:55:38Z"
    generation: 63760
    name: worker-0
    resourceVersion: "8450223"
    uid: 8b77be46-08c0-4074-927b-d49361471590
  topologyPolicies:
  - SingleNUMANodeContainerLevel
  zones:
  - costs:
    - name: node-0
      value: 10
    - name: node-1
      value: 21
    name: node-0
    resources:
    - allocatable: "38"
      available: "38"
      capacity: "40"
      name: cpu
    - allocatable: "134217728"
      available: "134217728"
      capacity: "134217728"
      name: hugepages-2Mi
    - allocatable: "262352048128"
      available: "262352048128"
      capacity: "270107316224"
      name: memory
    - allocatable: "6442450944"
      available: "6442450944"
      capacity: "6442450944"
      name: hugepages-1Gi
    type: Node
  - costs:
    - name: node-0
      value: 21
    - name: node-1
      value: 10
    name: node-1
    resources:
    - allocatable: "268435456"
      available: "268435456"
      capacity: "268435456"
      name: hugepages-2Mi
    - allocatable: "269231067136"
      available: "269231067136"
      capacity: "270573244416"
      name: memory
    - allocatable: "40"
      available: "40"
      capacity: "40"
      name: cpu
    - allocatable: "1073741824"
      available: "1073741824"
      capacity: "1073741824"
      name: hugepages-1Gi
    type: Node
- apiVersion: topology.node.k8s.io/v1
  kind: NodeResourceTopology
  metadata:
    annotations:
      k8stopoawareschedwg/rte-update: periodic
    creationTimestamp: "2022-06-16T08:55:37Z"
    generation: 62061
    name: worker-1
    resourceVersion: "8450129"
    uid: e8659390-6f8d-4e67-9a51-1ea34bba1cc3
  topologyPolicies:
  - SingleNUMANodeContainerLevel
  zones: <span id="CO19-1"><!--Empty--></span><span class="callout">1</span>
  - costs:
    - name: node-0
      value: 10
    - name: node-1
      value: 21
    name: node-0
    resources: <span id="CO19-2"><!--Empty--></span><span class="callout">2</span>
    - allocatable: "38"
      available: "38"
      capacity: "40"
      name: cpu
    - allocatable: "6442450944"
      available: "6442450944"
      capacity: "6442450944"
      name: hugepages-1Gi
    - allocatable: "134217728"
      available: "134217728"
      capacity: "134217728"
      name: hugepages-2Mi
    - allocatable: "262391033856"
      available: "262391033856"
      capacity: "270146301952"
      name: memory
    type: Node
  - costs:
    - name: node-0
      value: 21
    - name: node-1
      value: 10
    name: node-1
    resources:
    - allocatable: "40"
      available: "40"
      capacity: "40"
      name: cpu
    - allocatable: "1073741824"
      available: "1073741824"
      capacity: "1073741824"
      name: hugepages-1Gi
    - allocatable: "268435456"
      available: "268435456"
      capacity: "268435456"
      name: hugepages-2Mi
    - allocatable: "269192085504"
      available: "269192085504"
      capacity: "270534262784"
      name: memory
    type: Node
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO19-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Each stanza under <code class="literal">zones</code> describes the resources for a single NUMA zone.
							</div></dd><dt><a href="#CO19-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								<code class="literal">resources</code> describes the current state of the NUMA zone resources. Check that resources listed under <code class="literal">items.zones.resources.available</code> correspond to the exclusive NUMA zone resources allocated to each guaranteed pod.
							</div></dd></dl></div></li></ol></div><section class="section" id="cnf-checking-numa-aware-scheduler-logs_numa-aware"><div class="titlepage"><div><div><h3 class="title">6.6.1. Checking the NUMA-aware scheduler logs</h3></div></div></div><p>
					Troubleshoot problems with the NUMA-aware scheduler by reviewing the logs. If required, you can increase the scheduler log level by modifying the <code class="literal">spec.logLevel</code> field of the <code class="literal">NUMAResourcesScheduler</code> resource. Acceptable values are <code class="literal">Normal</code>, <code class="literal">Debug</code>, and <code class="literal">Trace</code>, with <code class="literal">Trace</code> being the most verbose option.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						To change the log level of the secondary scheduler, delete the running scheduler resource and re-deploy it with the changed log level. The scheduler is unavailable for scheduling new workloads during this downtime.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Delete the currently running <code class="literal">NUMAResourcesScheduler</code> resource:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Get the active <code class="literal">NUMAResourcesScheduler</code> by running the following command:
								</p><pre class="programlisting language-terminal">$ oc get NUMAResourcesScheduler</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                     AGE
numaresourcesscheduler   90m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Delete the secondary scheduler resource by running the following command:
								</p><pre class="programlisting language-terminal">$ oc delete NUMAResourcesScheduler numaresourcesscheduler</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">numaresourcesscheduler.nodetopology.openshift.io "numaresourcesscheduler" deleted</pre>

									</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Save the following YAML in the file <code class="literal">nro-scheduler-debug.yaml</code>. This example changes the log level to <code class="literal">Debug</code>:
						</p><pre class="programlisting language-yaml">apiVersion: nodetopology.openshift.io/v1
kind: NUMAResourcesScheduler
metadata:
  name: numaresourcesscheduler
spec:
  imageSpec: "registry.redhat.io/openshift4/noderesourcetopology-scheduler-container-rhel8:v4.13"
  logLevel: Debug</pre></li><li class="listitem"><p class="simpara">
							Create the updated <code class="literal">Debug</code> logging <code class="literal">NUMAResourcesScheduler</code> resource by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f nro-scheduler-debug.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">numaresourcesscheduler.nodetopology.openshift.io/numaresourcesscheduler created</pre>

							</p></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification steps</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check that the NUMA-aware scheduler was successfully deployed:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Run the following command to check that the CRD is created succesfully:
								</p><pre class="programlisting language-terminal">$ oc get crd | grep numaresourcesschedulers</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                                              CREATED AT
numaresourcesschedulers.nodetopology.openshift.io                 2022-02-25T11:57:03Z</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Check that the new custom scheduler is available by running the following command:
								</p><pre class="programlisting language-terminal">$ oc get numaresourcesschedulers.nodetopology.openshift.io</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                     AGE
numaresourcesscheduler   3h26m</pre>

									</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Check that the logs for the scheduler shows the increased log level:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Get the list of pods running in the <code class="literal">openshift-numaresources</code> namespace by running the following command:
								</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-numaresources</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                               READY   STATUS    RESTARTS   AGE
numaresources-controller-manager-d87d79587-76mrm   1/1     Running   0          46h
numaresourcesoperator-worker-5wm2k                 2/2     Running   0          45h
numaresourcesoperator-worker-pb75c                 2/2     Running   0          45h
secondary-scheduler-7976c4d466-qm4sc               1/1     Running   0          21m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Get the logs for the secondary scheduler pod by running the following command:
								</p><pre class="programlisting language-terminal">$ oc logs secondary-scheduler-7976c4d466-qm4sc -n openshift-numaresources</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">...
I0223 11:04:55.614788       1 reflector.go:535] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.Namespace total 11 items received
I0223 11:04:56.609114       1 reflector.go:535] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.ReplicationController total 10 items received
I0223 11:05:22.626818       1 reflector.go:535] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.StorageClass total 7 items received
I0223 11:05:31.610356       1 reflector.go:535] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.PodDisruptionBudget total 7 items received
I0223 11:05:31.713032       1 eventhandlers.go:186] "Add event for scheduled pod" pod="openshift-marketplace/certified-operators-thtvq"
I0223 11:05:53.461016       1 eventhandlers.go:244] "Delete event for scheduled pod" pod="openshift-marketplace/certified-operators-thtvq"</pre>

									</p></div></li></ol></div></li></ol></div></section><section class="section" id="cnf-troubleshooting-resource-topo-exporter_numa-aware"><div class="titlepage"><div><div><h3 class="title">6.6.2. Troubleshooting the resource topology exporter</h3></div></div></div><p>
					Troubleshoot <code class="literal">noderesourcetopologies</code> objects where unexpected results are occurring by inspecting the corresponding <code class="literal">resource-topology-exporter</code> logs.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						It is recommended that NUMA resource topology exporter instances in the cluster are named for nodes they refer to. For example, a worker node with the name <code class="literal">worker</code> should have a corresponding <code class="literal">noderesourcetopologies</code> object called <code class="literal">worker</code>.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Get the daemonsets managed by the NUMA Resources Operator. Each daemonset has a corresponding <code class="literal">nodeGroup</code> in the <code class="literal">NUMAResourcesOperator</code> CR. Run the following command:
						</p><pre class="programlisting language-terminal">$ oc get numaresourcesoperators.nodetopology.openshift.io numaresourcesoperator -o jsonpath="{.status.daemonsets[0]}"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-json">{"name":"numaresourcesoperator-worker","namespace":"openshift-numaresources"}</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Get the label for the daemonset of interest using the value for <code class="literal">name</code> from the previous step:
						</p><pre class="programlisting language-terminal">$ oc get ds -n openshift-numaresources numaresourcesoperator-worker -o jsonpath="{.spec.selector.matchLabels}"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-json">{"name":"resource-topology"}</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Get the pods using the <code class="literal">resource-topology</code> label by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-numaresources -l name=resource-topology -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                 READY   STATUS    RESTARTS   AGE    IP            NODE
numaresourcesoperator-worker-5wm2k   2/2     Running   0          2d1h   10.135.0.64   compute-0.example.com
numaresourcesoperator-worker-pb75c   2/2     Running   0          2d1h   10.132.2.33   compute-1.example.com</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Examine the logs of the <code class="literal">resource-topology-exporter</code> container running on the worker pod that corresponds to the node you are troubleshooting. Run the following command:
						</p><pre class="programlisting language-terminal">$ oc logs -n openshift-numaresources -c resource-topology-exporter numaresourcesoperator-worker-pb75c</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">I0221 13:38:18.334140       1 main.go:206] using sysinfo:
reservedCpus: 0,1
reservedMemory:
  "0": 1178599424
I0221 13:38:18.334370       1 main.go:67] === System information ===
I0221 13:38:18.334381       1 sysinfo.go:231] cpus: reserved "0-1"
I0221 13:38:18.334493       1 sysinfo.go:237] cpus: online "0-103"
I0221 13:38:18.546750       1 main.go:72]
cpus: allocatable "2-103"
hugepages-1Gi:
  numa cell 0 -&gt; 6
  numa cell 1 -&gt; 1
hugepages-2Mi:
  numa cell 0 -&gt; 64
  numa cell 1 -&gt; 128
memory:
  numa cell 0 -&gt; 45758Mi
  numa cell 1 -&gt; 48372Mi</pre>

							</p></div></li></ol></div></section><section class="section" id="cnf-troubleshooting-missing-rte-config-maps_numa-aware"><div class="titlepage"><div><div><h3 class="title">6.6.3. Correcting a missing resource topology exporter config map</h3></div></div></div><p>
					If you install the NUMA Resources Operator in a cluster with misconfigured cluster settings, in some circumstances, the Operator is shown as active but the logs of the resource topology exporter (RTE) daemon set pods show that the configuration for the RTE is missing, for example:
				</p><pre class="programlisting language-text">Info: couldn't find configuration in "/etc/resource-topology-exporter/config.yaml"</pre><p>
					This log message indicates that the <code class="literal">kubeletconfig</code> with the required configuration was not properly applied in the cluster, resulting in a missing RTE <code class="literal">configmap</code>. For example, the following cluster is missing a <code class="literal">numaresourcesoperator-worker</code> <code class="literal">configmap</code> custom resource (CR):
				</p><pre class="programlisting language-terminal">$ oc get configmap</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">NAME                           DATA   AGE
0e2a6bd3.openshift-kni.io      0      6d21h
kube-root-ca.crt               1      6d21h
openshift-service-ca.crt       1      6d21h
topo-aware-scheduler-config    1      6d18h</pre>

					</p></div><p>
					In a correctly configured cluster, <code class="literal">oc get configmap</code> also returns a <code class="literal">numaresourcesoperator-worker</code> <code class="literal">configmap</code> CR.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift Container Platform CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with cluster-admin privileges.
						</li><li class="listitem">
							Install the NUMA Resources Operator and deploy the NUMA-aware secondary scheduler.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Compare the values for <code class="literal">spec.machineConfigPoolSelector.matchLabels</code> in <code class="literal">kubeletconfig</code> and <code class="literal">metadata.labels</code> in the <code class="literal">MachineConfigPool</code> (<code class="literal">mcp</code>) worker CR using the following commands:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Check the <code class="literal">kubeletconfig</code> labels by running the following command:
								</p><pre class="programlisting language-terminal">$ oc get kubeletconfig -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">machineConfigPoolSelector:
  matchLabels:
    cnf-worker-tuning: enabled</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Check the <code class="literal">mcp</code> labels by running the following command:
								</p><pre class="programlisting language-terminal">$ oc get mcp worker -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">labels:
  machineconfiguration.openshift.io/mco-built-in: ""
  pools.operator.machineconfiguration.openshift.io/worker: ""</pre>

									</p></div><p class="simpara">
									The <code class="literal">cnf-worker-tuning: enabled</code> label is not present in the <code class="literal">MachineConfigPool</code> object.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Edit the <code class="literal">MachineConfigPool</code> CR to include the missing label, for example:
						</p><pre class="programlisting language-terminal">$ oc edit mcp worker -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">labels:
  machineconfiguration.openshift.io/mco-built-in: ""
  pools.operator.machineconfiguration.openshift.io/worker: ""
  cnf-worker-tuning: enabled</pre>

							</p></div></li><li class="listitem">
							Apply the label changes and wait for the cluster to apply the updated configuration. Run the following command:
						</li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Check that the missing <code class="literal">numaresourcesoperator-worker</code> <code class="literal">configmap</code> CR is applied:
						</p><pre class="programlisting language-terminal">$ oc get configmap</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                           DATA   AGE
0e2a6bd3.openshift-kni.io      0      6d21h
kube-root-ca.crt               1      6d21h
numaresourcesoperator-worker   1      5m
openshift-service-ca.crt       1      6d21h
topo-aware-scheduler-config    1      6d18h</pre>

							</p></div></li></ul></div></section></section></section><section class="chapter" id="scalability-and-performance-optimization"><div class="titlepage"><div><div><h1 class="title">Chapter 7. Scalability and performance optimization</h1></div></div></div><section class="section" id="optimizing-storage"><div class="titlepage"><div><div><h2 class="title">7.1. Optimizing storage</h2></div></div></div><p>
				Optimizing storage helps to minimize storage use across all resources. By optimizing storage, administrators help ensure that existing storage resources are working in an efficient manner.
			</p><section class="section" id="available-persistent-storage-options_persistent-storage"><div class="titlepage"><div><div><h3 class="title">7.1.1. Available persistent storage options</h3></div></div></div><p>
					Understand your persistent storage options so that you can optimize your OpenShift Container Platform environment.
				</p><div class="table" id="idm139735332477696"><p class="title"><strong>Table 7.1. Available storage options</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 13%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col><col style="width: 38%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735336716816" scope="col">Storage type</th><th align="left" valign="top" id="idm139735336715728" scope="col">Description</th><th align="left" valign="top" id="idm139735336714640" scope="col">Examples</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735336716816"> <p>
									Block
								</p>
								 </td><td align="left" valign="top" headers="idm139735336715728"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											Presented to the operating system (OS) as a block device
										</li><li class="listitem">
											Suitable for applications that need full control of storage and operate at a low level on files bypassing the file system
										</li><li class="listitem">
											Also referred to as a Storage Area Network (SAN)
										</li><li class="listitem">
											Non-shareable, which means that only one client at a time can mount an endpoint of this type
										</li></ul></div>
								 </td><td align="left" valign="top" headers="idm139735336714640"> <p>
									AWS EBS and VMware vSphere support dynamic persistent volume (PV) provisioning natively in OpenShift Container Platform.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735336716816"> <p>
									File
								</p>
								 </td><td align="left" valign="top" headers="idm139735336715728"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											Presented to the OS as a file system export to be mounted
										</li><li class="listitem">
											Also referred to as Network Attached Storage (NAS)
										</li><li class="listitem">
											Concurrency, latency, file locking mechanisms, and other capabilities vary widely between protocols, implementations, vendors, and scales.
										</li></ul></div>
								 </td><td align="left" valign="top" headers="idm139735336714640"> <p>
									RHEL NFS, NetApp NFS <sup>[1]</sup>, and Vendor NFS
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735336716816"> <p>
									Object
								</p>
								 </td><td align="left" valign="top" headers="idm139735336715728"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											Accessible through a REST API endpoint
										</li><li class="listitem">
											Configurable for use in the OpenShift image registry
										</li><li class="listitem">
											Applications must build their drivers into the application and/or container.
										</li></ul></div>
								 </td><td align="left" valign="top" headers="idm139735336714640"> <p>
									AWS S3
								</p>
								 </td></tr></tbody></table></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							NetApp NFS supports dynamic PV provisioning when using the Trident plugin.
						</li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Currently, CNS is not supported in OpenShift Container Platform 4.13.
					</p></div></div></section><section class="section" id="recommended-configurable-storage-technology_persistent-storage"><div class="titlepage"><div><div><h3 class="title">7.1.2. Recommended configurable storage technology</h3></div></div></div><p>
					The following table summarizes the recommended and configurable storage technologies for the given OpenShift Container Platform cluster application.
				</p><div class="table" id="idm139735341486944"><p class="title"><strong>Table 7.2. Recommended and configurable storage technology</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735355592464" scope="col">Storage type</th><th align="left" valign="top" id="idm139735355591376" scope="col">Block</th><th align="left" valign="top" id="idm139735353051504" scope="col">File</th><th align="left" valign="top" id="idm139735353050416" scope="col">Object</th></tr></thead><tfoot><tr><th colspan="4" align="left" valign="top" id="idm139735353048368" scope=""> <p>
								<sup>1</sup> <code class="literal">ReadOnlyMany</code>
							</p>
							 <p>
								<sup>2</sup> <code class="literal">ReadWriteMany</code>
							</p>
							 <p>
								<sup>3</sup> Prometheus is the underlying technology used for metrics.
							</p>
							 <p>
								<sup>4</sup> This does not apply to physical disk, VM physical disk, VMDK, loopback over NFS, AWS EBS, and Azure Disk.
							</p>
							 <p>
								<sup>5</sup> For metrics, using file storage with the <code class="literal">ReadWriteMany</code> (RWX) access mode is unreliable. If you use file storage, do not configure the RWX access mode on any persistent volume claims (PVCs) that are configured for use with metrics.
							</p>
							 <p>
								<sup>6</sup> For logging, review the recommended storage solution in Configuring persistent storage for the log store section. Using NFS storage as a persistent volume or through NAS, such as Gluster, can corrupt the data. Hence, NFS is not supported for Elasticsearch storage and LokiStack log store in OpenShift Container Platform Logging. You must use one persistent volume type per log store.
							</p>
							 <p>
								<sup>7</sup> Object storage is not consumed through OpenShift Container Platform’s PVs or PVCs. Apps must integrate with the object storage REST API.
							</p>
							 </th></tr></tfoot><tbody><tr><td align="left" valign="top" headers="idm139735355592464"> <p>
									ROX<sup>1</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm139735355591376"> <p>
									Yes<sup>4</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm139735353051504"> <p>
									Yes<sup>4</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm139735353050416"> <p>
									Yes
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735355592464"> <p>
									RWX<sup>2</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm139735355591376"> <p>
									No
								</p>
								 </td><td align="left" valign="top" headers="idm139735353051504"> <p>
									Yes
								</p>
								 </td><td align="left" valign="top" headers="idm139735353050416"> <p>
									Yes
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735355592464"> <p>
									Registry
								</p>
								 </td><td align="left" valign="top" headers="idm139735355591376"> <p>
									Configurable
								</p>
								 </td><td align="left" valign="top" headers="idm139735353051504"> <p>
									Configurable
								</p>
								 </td><td align="left" valign="top" headers="idm139735353050416"> <p>
									Recommended
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735355592464"> <p>
									Scaled registry
								</p>
								 </td><td align="left" valign="top" headers="idm139735355591376"> <p>
									Not configurable
								</p>
								 </td><td align="left" valign="top" headers="idm139735353051504"> <p>
									Configurable
								</p>
								 </td><td align="left" valign="top" headers="idm139735353050416"> <p>
									Recommended
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735355592464"> <p>
									Metrics<sup>3</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm139735355591376"> <p>
									Recommended
								</p>
								 </td><td align="left" valign="top" headers="idm139735353051504"> <p>
									Configurable<sup>5</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm139735353050416"> <p>
									Not configurable
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735355592464"> <p>
									Elasticsearch Logging
								</p>
								 </td><td align="left" valign="top" headers="idm139735355591376"> <p>
									Recommended
								</p>
								 </td><td align="left" valign="top" headers="idm139735353051504"> <p>
									Configurable<sup>6</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm139735353050416"> <p>
									Not supported<sup>6</sup>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735355592464"> <p>
									Loki Logging
								</p>
								 </td><td align="left" valign="top" headers="idm139735355591376"> <p>
									Configurable
								</p>
								 </td><td align="left" valign="top" headers="idm139735353051504"> <p>
									Not configurable
								</p>
								 </td><td align="left" valign="top" headers="idm139735353050416"> <p>
									Recommended
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735355592464"> <p>
									Apps
								</p>
								 </td><td align="left" valign="top" headers="idm139735355591376"> <p>
									Recommended
								</p>
								 </td><td align="left" valign="top" headers="idm139735353051504"> <p>
									Recommended
								</p>
								 </td><td align="left" valign="top" headers="idm139735353050416"> <p>
									Not configurable<sup>7</sup>
								</p>
								 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						A scaled registry is an OpenShift image registry where two or more pod replicas are running.
					</p></div></div><section class="section" id="specific-application-storage-recommendations"><div class="titlepage"><div><div><h4 class="title">7.1.2.1. Specific application storage recommendations</h4></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							Testing shows issues with using the NFS server on Red Hat Enterprise Linux (RHEL) as storage backend for core services. This includes the OpenShift Container Registry and Quay, Prometheus for monitoring storage, and Elasticsearch for logging storage. Therefore, using RHEL NFS to back PVs used by core services is not recommended.
						</p><p>
							Other NFS implementations on the marketplace might not have these issues. Contact the individual NFS implementation vendor for more information on any testing that was possibly completed against these OpenShift Container Platform core components.
						</p></div></div><section class="section" id="registry"><div class="titlepage"><div><div><h5 class="title">7.1.2.1.1. Registry</h5></div></div></div><p>
							In a non-scaled/high-availability (HA) OpenShift image registry cluster deployment:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The storage technology does not have to support RWX access mode.
								</li><li class="listitem">
									The storage technology must ensure read-after-write consistency.
								</li><li class="listitem">
									The preferred storage technology is object storage followed by block storage.
								</li><li class="listitem">
									File storage is not recommended for OpenShift image registry cluster deployment with production workloads.
								</li></ul></div></section><section class="section" id="scaled-registry"><div class="titlepage"><div><div><h5 class="title">7.1.2.1.2. Scaled registry</h5></div></div></div><p>
							In a scaled/HA OpenShift image registry cluster deployment:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The storage technology must support RWX access mode.
								</li><li class="listitem">
									The storage technology must ensure read-after-write consistency.
								</li><li class="listitem">
									The preferred storage technology is object storage.
								</li><li class="listitem">
									Red Hat OpenShift Data Foundation (ODF), Amazon Simple Storage Service (Amazon S3), Google Cloud Storage (GCS), Microsoft Azure Blob Storage, and OpenStack Swift are supported.
								</li><li class="listitem">
									Object storage should be S3 or Swift compliant.
								</li><li class="listitem">
									For non-cloud platforms, such as vSphere and bare metal installations, the only configurable technology is file storage.
								</li><li class="listitem">
									Block storage is not configurable.
								</li></ul></div></section><section class="section" id="metrics"><div class="titlepage"><div><div><h5 class="title">7.1.2.1.3. Metrics</h5></div></div></div><p>
							In an OpenShift Container Platform hosted metrics cluster deployment:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The preferred storage technology is block storage.
								</li><li class="listitem">
									Object storage is not configurable.
								</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								It is not recommended to use file storage for a hosted metrics cluster deployment with production workloads.
							</p></div></div></section><section class="section" id="logging"><div class="titlepage"><div><div><h5 class="title">7.1.2.1.4. Logging</h5></div></div></div><p>
							In an OpenShift Container Platform hosted logging cluster deployment:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The preferred storage technology is block storage.
								</li><li class="listitem">
									Object storage is not configurable.
								</li></ul></div></section><section class="section" id="applications"><div class="titlepage"><div><div><h5 class="title">7.1.2.1.5. Applications</h5></div></div></div><p>
							Application use cases vary from application to application, as described in the following examples:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Storage technologies that support dynamic PV provisioning have low mount time latencies, and are not tied to nodes to support a healthy cluster.
								</li><li class="listitem">
									Application developers are responsible for knowing and understanding the storage requirements for their application, and how it works with the provided storage to ensure that issues do not occur when an application scales or interacts with the storage layer.
								</li></ul></div></section></section><section class="section" id="other-specific-application-storage-recommendations"><div class="titlepage"><div><div><h4 class="title">7.1.2.2. Other specific application storage recommendations</h4></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							It is not recommended to use RAID configurations on <code class="literal">Write</code> intensive workloads, such as <code class="literal">etcd</code>. If you are running <code class="literal">etcd</code> with a RAID configuration, you might be at risk of encountering performance issues with your workloads.
						</p></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Red Hat OpenStack Platform (RHOSP) Cinder: RHOSP Cinder tends to be adept in ROX access mode use cases.
							</li><li class="listitem">
								Databases: Databases (RDBMSs, NoSQL DBs, etc.) tend to perform best with dedicated block storage.
							</li><li class="listitem">
								The etcd database must have enough storage and adequate performance capacity to enable a large cluster. Information about monitoring and benchmarking tools to establish ample storage and a high-performance environment is described in <span class="emphasis"><em>Recommended etcd practices</em></span>.
							</li></ul></div></section></section><section class="section" id="data-storage-management_persistent-storage"><div class="titlepage"><div><div><h3 class="title">7.1.3. Data storage management</h3></div></div></div><p>
					The following table summarizes the main directories that OpenShift Container Platform components write data to.
				</p><div class="table" id="idm139735356469440"><p class="title"><strong>Table 7.3. Main directories for storing OpenShift Container Platform data</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735339577072" scope="col">Directory</th><th align="left" valign="top" id="idm139735339575984" scope="col">Notes</th><th align="left" valign="top" id="idm139735341396336" scope="col">Sizing</th><th align="left" valign="top" id="idm139735341395248" scope="col">Expected growth</th></tr></thead><tfoot><tr><th align="left" valign="top" id="idm139735341393200" scope=""> <p>
								<span class="strong strong"><strong><span class="emphasis"><em>/var/log</em></span></strong></span>
							</p>
							 </th><th align="left" valign="top" id="idm139735341390688" scope=""> <p>
								Log files for all components.
							</p>
							 </th><th align="left" valign="top" id="idm139735343514544" scope=""> <p>
								10 to 30 GB.
							</p>
							 </th><th align="left" valign="top" id="idm139735343513008" scope=""> <p>
								Log files can grow quickly; size can be managed by growing disks or by using log rotate.
							</p>
							 </th></tr></tfoot><tbody><tr><td align="left" valign="top" headers="idm139735339577072"> <p>
									<span class="strong strong"><strong><span class="emphasis"><em>/var/lib/etcd</em></span></strong></span>
								</p>
								 </td><td align="left" valign="top" headers="idm139735339575984"> <p>
									Used for etcd storage when storing the database.
								</p>
								 </td><td align="left" valign="top" headers="idm139735341396336"> <p>
									Less than 20 GB.
								</p>
								 <p>
									Database can grow up to 8 GB.
								</p>
								 </td><td align="left" valign="top" headers="idm139735341395248"> <p>
									Will grow slowly with the environment. Only storing metadata.
								</p>
								 <p>
									Additional 20-25 GB for every additional 8 GB of memory.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735339577072"> <p>
									<span class="strong strong"><strong><span class="emphasis"><em>/var/lib/containers</em></span></strong></span>
								</p>
								 </td><td align="left" valign="top" headers="idm139735339575984"> <p>
									This is the mount point for the CRI-O runtime. Storage used for active container runtimes, including pods, and storage of local images. Not used for registry storage.
								</p>
								 </td><td align="left" valign="top" headers="idm139735341396336"> <p>
									50 GB for a node with 16 GB memory. Note that this sizing should not be used to determine minimum cluster requirements.
								</p>
								 <p>
									Additional 20-25 GB for every additional 8 GB of memory.
								</p>
								 </td><td align="left" valign="top" headers="idm139735341395248"> <p>
									Growth is limited by capacity for running containers.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735339577072"> <p>
									<span class="strong strong"><strong><span class="emphasis"><em>/var/lib/kubelet</em></span></strong></span>
								</p>
								 </td><td align="left" valign="top" headers="idm139735339575984"> <p>
									Ephemeral volume storage for pods. This includes anything external that is mounted into a container at runtime. Includes environment variables, kube secrets, and data volumes not backed by persistent volumes.
								</p>
								 </td><td align="left" valign="top" headers="idm139735341396336"> <p>
									Varies
								</p>
								 </td><td align="left" valign="top" headers="idm139735341395248"> <p>
									Minimal if pods requiring storage are using persistent volumes. If using ephemeral storage, this can grow quickly.
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section" id="optimizing-storage-azure_persistent-storage"><div class="titlepage"><div><div><h3 class="title">7.1.4. Optimizing storage performance for Microsoft Azure</h3></div></div></div><p>
					OpenShift Container Platform and Kubernetes are sensitive to disk performance, and faster storage is recommended, particularly for etcd on the control plane nodes.
				</p><p>
					For production Azure clusters and clusters with intensive workloads, the virtual machine operating system disk for control plane machines should be able to sustain a tested and recommended minimum throughput of 5000 IOPS / 200MBps. This throughput can be provided by having a minimum of 1 TiB Premium SSD (P30). In Azure and Azure Stack Hub, disk performance is directly dependent on SSD disk sizes. To achieve the throughput supported by a <code class="literal">Standard_D8s_v3</code> virtual machine, or other similar machine types, and the target of 5000 IOPS, at least a P30 disk is required.
				</p><p>
					Host caching must be set to <code class="literal">ReadOnly</code> for low latency and high IOPS and throughput when reading data. Reading data from the cache, which is present either in the VM memory or in the local SSD disk, is much faster than reading from the disk, which is in the blob storage.
				</p></section><section class="section _additional-resources" id="admission-plug-ins-additional-resources"><div class="titlepage"><div><div><h3 class="title">7.1.5. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/logging/#cluster-logging-elasticsearch-storage_cluster-logging-log-store">Configuring persistent storage for the log store</a>
						</li></ul></div></section></section><section class="section" id="routing-optimization"><div class="titlepage"><div><div><h2 class="title">7.2. Optimizing routing</h2></div></div></div><p>
				The OpenShift Container Platform HAProxy router can be scaled or configured to optimize performance.
			</p><section class="section" id="baseline-router-performance_routing-optimization"><div class="titlepage"><div><div><h3 class="title">7.2.1. Baseline Ingress Controller (router) performance</h3></div></div></div><p>
					The OpenShift Container Platform Ingress Controller, or router, is the ingress point for ingress traffic for applications and services that are configured using routes and ingresses.
				</p><p>
					When evaluating a single HAProxy router performance in terms of HTTP requests handled per second, the performance varies depending on many factors. In particular:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							HTTP keep-alive/close mode
						</li><li class="listitem">
							Route type
						</li><li class="listitem">
							TLS session resumption client support
						</li><li class="listitem">
							Number of concurrent connections per target route
						</li><li class="listitem">
							Number of target routes
						</li><li class="listitem">
							Back end server page size
						</li><li class="listitem">
							Underlying infrastructure (network/SDN solution, CPU, and so on)
						</li></ul></div><p>
					While performance in your specific environment will vary, Red Hat lab tests on a public cloud instance of size 4 vCPU/16GB RAM. A single HAProxy router handling 100 routes terminated by backends serving 1kB static pages is able to handle the following number of transactions per second.
				</p><p>
					In HTTP keep-alive mode scenarios:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735333140896" scope="col"><span class="strong strong"><strong>Encryption</strong></span></th><th align="left" valign="top" id="idm139735333139344" scope="col"><span class="strong strong"><strong>LoadBalancerService</strong></span></th><th align="left" valign="top" id="idm139735333137792" scope="col"><span class="strong strong"><strong>HostNetwork</strong></span></th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735333140896"> <p>
									none
								</p>
								 </td><td align="left" valign="top" headers="idm139735333139344"> <p>
									21515
								</p>
								 </td><td align="left" valign="top" headers="idm139735333137792"> <p>
									29622
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735333140896"> <p>
									edge
								</p>
								 </td><td align="left" valign="top" headers="idm139735333139344"> <p>
									16743
								</p>
								 </td><td align="left" valign="top" headers="idm139735333137792"> <p>
									22913
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735333140896"> <p>
									passthrough
								</p>
								 </td><td align="left" valign="top" headers="idm139735333139344"> <p>
									36786
								</p>
								 </td><td align="left" valign="top" headers="idm139735333137792"> <p>
									53295
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735333140896"> <p>
									re-encrypt
								</p>
								 </td><td align="left" valign="top" headers="idm139735333139344"> <p>
									21583
								</p>
								 </td><td align="left" valign="top" headers="idm139735333137792"> <p>
									25198
								</p>
								 </td></tr></tbody></table></div><p>
					In HTTP close (no keep-alive) scenarios:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735349483664" scope="col"><span class="strong strong"><strong>Encryption</strong></span></th><th align="left" valign="top" id="idm139735349482112" scope="col"><span class="strong strong"><strong>LoadBalancerService</strong></span></th><th align="left" valign="top" id="idm139735349480560" scope="col"><span class="strong strong"><strong>HostNetwork</strong></span></th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735349483664"> <p>
									none
								</p>
								 </td><td align="left" valign="top" headers="idm139735349482112"> <p>
									5719
								</p>
								 </td><td align="left" valign="top" headers="idm139735349480560"> <p>
									8273
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735349483664"> <p>
									edge
								</p>
								 </td><td align="left" valign="top" headers="idm139735349482112"> <p>
									2729
								</p>
								 </td><td align="left" valign="top" headers="idm139735349480560"> <p>
									4069
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735349483664"> <p>
									passthrough
								</p>
								 </td><td align="left" valign="top" headers="idm139735349482112"> <p>
									4121
								</p>
								 </td><td align="left" valign="top" headers="idm139735349480560"> <p>
									5344
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735349483664"> <p>
									re-encrypt
								</p>
								 </td><td align="left" valign="top" headers="idm139735349482112"> <p>
									2320
								</p>
								 </td><td align="left" valign="top" headers="idm139735349480560"> <p>
									2941
								</p>
								 </td></tr></tbody></table></div><p>
					The default Ingress Controller configuration was used with the <code class="literal">spec.tuningOptions.threadCount</code> field set to <code class="literal">4</code>. Two different endpoint publishing strategies were tested: Load Balancer Service and Host Network. TLS session resumption was used for encrypted routes. With HTTP keep-alive, a single HAProxy router is capable of saturating a 1 Gbit NIC at page sizes as small as 8 kB.
				</p><p>
					When running on bare metal with modern processors, you can expect roughly twice the performance of the public cloud instance above. This overhead is introduced by the virtualization layer in place on public clouds and holds mostly true for private cloud-based virtualization as well. The following table is a guide to how many applications to use behind the router:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 67%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735333184416" scope="col"><span class="strong strong"><strong>Number of applications</strong></span></th><th align="left" valign="top" id="idm139735333490848" scope="col"><span class="strong strong"><strong>Application type</strong></span></th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735333184416"> <p>
									5-10
								</p>
								 </td><td align="left" valign="top" headers="idm139735333490848"> <p>
									static file/web server or caching proxy
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735333184416"> <p>
									100-1000
								</p>
								 </td><td align="left" valign="top" headers="idm139735333490848"> <p>
									applications generating dynamic content
								</p>
								 </td></tr></tbody></table></div><p>
					In general, HAProxy can support routes for up to 1000 applications, depending on the technology in use. Ingress Controller performance might be limited by the capabilities and performance of the applications behind it, such as language or static versus dynamic content.
				</p><p>
					Ingress, or router, sharding should be used to serve more routes towards applications and help horizontally scale the routing tier.
				</p><p>
					For more information on Ingress sharding, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-ingress-sharding-route-labels_configuring-ingress">Configuring Ingress Controller sharding by using route labels</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-ingress-sharding-namespace-labels_configuring-ingress">Configuring Ingress Controller sharding by using namespace labels</a>.
				</p><p>
					You can modify the Ingress Controller deployment using the information provided in <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-ingress-setting-thread-count">Setting Ingress Controller thread count</a> for threads and <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-ingress-controller-configuration-parameters_configuring-ingress">Ingress Controller configuration parameters</a> for timeouts, and other tuning configurations in the Ingress Controller specification.
				</p></section><section class="section" id="ingress-liveness-readiness-startup-probes_routing-optimization"><div class="titlepage"><div><div><h3 class="title">7.2.2. Configuring Ingress Controller liveness, readiness, and startup probes</h3></div></div></div><p>
					Cluster administrators can configure the timeout values for the kubelet’s liveness, readiness, and startup probes for router deployments that are managed by the OpenShift Container Platform Ingress Controller (router). The liveness and readiness probes of the router use the default timeout value of 1 second, which is too brief when networking or runtime performance is severely degraded. Probe timeouts can cause unwanted router restarts that interrupt application connections. The ability to set larger timeout values can reduce the risk of unnecessary and unwanted restarts.
				</p><p>
					You can update the <code class="literal">timeoutSeconds</code> value on the <code class="literal">livenessProbe</code>, <code class="literal">readinessProbe</code>, and <code class="literal">startupProbe</code> parameters of the router container.
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 27%; " class="col_1"><!--Empty--></col><col style="width: 73%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735332757360" scope="col">Parameter</th><th align="left" valign="top" id="idm139735341948288" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735332757360"> <p>
									<code class="literal">livenessProbe</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735341948288"> <p>
									The <code class="literal">livenessProbe</code> reports to the kubelet whether a pod is dead and needs to be restarted.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735332757360"> <p>
									<code class="literal">readinessProbe</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735341948288"> <p>
									The <code class="literal">readinessProbe</code> reports whether a pod is healthy or unhealthy. When the readiness probe reports an unhealthy pod, then the kubelet marks the pod as not ready to accept traffic. Subsequently, the endpoints for that pod are marked as not ready, and this status propagates to the kube-proxy. On cloud platforms with a configured load balancer, the kube-proxy communicates to the cloud load-balancer not to send traffic to the node with that pod.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735332757360"> <p>
									<code class="literal">startupProbe</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735341948288"> <p>
									The <code class="literal">startupProbe</code> gives the router pod up to 2 minutes to initialize before the kubelet begins sending the router liveness and readiness probes. This initialization time can prevent routers with many routes or endpoints from prematurely restarting.
								</p>
								 </td></tr></tbody></table></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The timeout configuration option is an advanced tuning technique that can be used to work around issues. However, these issues should eventually be diagnosed and possibly a support case or <a class="link" href="https://issues.redhat.com/secure/CreateIssueDetails!init.jspa?pid=12332330&amp;summary=Summary&amp;issuetype=1&amp;priority=10200&amp;versions=12385624">Jira issue</a> opened for any issues that causes probes to time out.
					</p></div></div><p>
					The following example demonstrates how you can directly patch the default router deployment to set a 5-second timeout for the liveness and readiness probes:
				</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress patch deploy/router-default --type=strategic --patch='{"spec":{"template":{"spec":{"containers":[{"name":"router","livenessProbe":{"timeoutSeconds":5},"readinessProbe":{"timeoutSeconds":5}}]}}}}'</pre><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						
<pre class="programlisting language-terminal">$ oc -n openshift-ingress describe deploy/router-default | grep -e Liveness: -e Readiness:
    Liveness:   http-get http://:1936/healthz delay=0s timeout=5s period=10s #success=1 #failure=3
    Readiness:  http-get http://:1936/healthz/ready delay=0s timeout=5s period=10s #success=1 #failure=3</pre>

					</p></div></section><section class="section" id="configuring-haproxy-interval_routing-optimization"><div class="titlepage"><div><div><h3 class="title">7.2.3. Configuring HAProxy reload interval</h3></div></div></div><p>
					When you update a route or an endpoint associated with a route, OpenShift Container Platform router updates the configuration for HAProxy. Then, HAProxy reloads the updated configuration for those changes to take effect. When HAProxy reloads, it generates a new process that handles new connections using the updated configuration.
				</p><p>
					HAProxy keeps the old process running to handle existing connections until those connections are all closed. When old processes have long-lived connections, these processes can accumulate and consume resources.
				</p><p>
					The default minimum HAProxy reload interval is five seconds. You can configure an Ingress Controller using its <code class="literal">spec.tuningOptions.reloadInterval</code> field to set a longer minimum reload interval.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Setting a large value for the minimum HAProxy reload interval can cause latency in observing updates to routes and their endpoints. To lessen the risk, avoid setting a value larger than the tolerable latency for updates.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Change the minimum HAProxy reload interval of the default Ingress Controller to 15 seconds by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress-operator patch ingresscontrollers/default --type=merge --patch='{"spec":{"tuningOptions":{"reloadInterval":"15s"}}}'</pre></li></ul></div></section></section><section class="section" id="optimizing-networking"><div class="titlepage"><div><div><h2 class="title">7.3. Optimizing networking</h2></div></div></div><p>
				The <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-openshift-sdn">OpenShift SDN</a> uses OpenvSwitch, virtual extensible LAN (VXLAN) tunnels, OpenFlow rules, and iptables. This network can be tuned by using jumbo frames, network interface controllers (NIC) offloads, multi-queue, and ethtool settings.
			</p><p>
				<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-ovn-kubernetes">OVN-Kubernetes</a> uses Geneve (Generic Network Virtualization Encapsulation) instead of VXLAN as the tunnel protocol.
			</p><p>
				VXLAN provides benefits over VLANs, such as an increase in networks from 4096 to over 16 million, and layer 2 connectivity across physical networks. This allows for all pods behind a service to communicate with each other, even if they are running on different systems.
			</p><p>
				VXLAN encapsulates all tunneled traffic in user datagram protocol (UDP) packets. However, this leads to increased CPU utilization. Both these outer- and inner-packets are subject to normal checksumming rules to guarantee data is not corrupted during transit. Depending on CPU performance, this additional processing overhead can cause a reduction in throughput and increased latency when compared to traditional, non-overlay networks.
			</p><p>
				Cloud, VM, and bare metal CPU performance can be capable of handling much more than one Gbps network throughput. When using higher bandwidth links such as 10 or 40 Gbps, reduced performance can occur. This is a known issue in VXLAN-based environments and is not specific to containers or OpenShift Container Platform. Any network that relies on VXLAN tunnels will perform similarly because of the VXLAN implementation.
			</p><p>
				If you are looking to push beyond one Gbps, you can:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Evaluate network plugins that implement different routing techniques, such as border gateway protocol (BGP).
					</li><li class="listitem">
						Use VXLAN-offload capable network adapters. VXLAN-offload moves the packet checksum calculation and associated CPU overhead off of the system CPU and onto dedicated hardware on the network adapter. This frees up CPU cycles for use by pods and applications, and allows users to utilize the full bandwidth of their network infrastructure.
					</li></ul></div><p>
				VXLAN-offload does not reduce latency. However, CPU utilization is reduced even in latency tests.
			</p><section class="section" id="optimizing-mtu_optimizing-networking"><div class="titlepage"><div><div><h3 class="title">7.3.1. Optimizing the MTU for your network</h3></div></div></div><p>
					There are two important maximum transmission units (MTUs): the network interface controller (NIC) MTU and the cluster network MTU.
				</p><p>
					The NIC MTU is only configured at the time of OpenShift Container Platform installation. The MTU must be less than or equal to the maximum supported value of the NIC of your network. If you are optimizing for throughput, choose the largest possible value. If you are optimizing for lowest latency, choose a lower value.
				</p><p>
					The OpenShift SDN network plugin overlay MTU must be less than the NIC MTU by 50 bytes at a minimum. This accounts for the SDN overlay header. So, on a normal ethernet network, this should be set to <code class="literal">1450</code>. On a jumbo frame ethernet network, this should be set to <code class="literal">8950</code>. These values should be set automatically by the Cluster Network Operator based on the NIC’s configured MTU. Therefore, cluster administrators do not typically update these values. Amazon Web Services (AWS) and bare-metal environments support jumbo frame ethernet networks. This setting will help throughput, especially with transmission control protocol (TCP).
				</p><p>
					For OVN and Geneve, the MTU must be less than the NIC MTU by 100 bytes at a minimum.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						This 50 byte overlay header is relevant to the OpenShift SDN network plugin. Other SDN solutions might require the value to be more or less.
					</p></div></div></section><section class="section" id="recommended-install-practices_optimizing-networking"><div class="titlepage"><div><div><h3 class="title">7.3.2. Recommended practices for installing large scale clusters</h3></div></div></div><p>
					When installing large clusters or scaling the cluster to larger node counts, set the cluster network <code class="literal">cidr</code> accordingly in your <code class="literal">install-config.yaml</code> file before you install the cluster:
				</p><pre class="programlisting language-yaml">networking:
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  machineNetwork:
  - cidr: 10.0.0.0/16
  networkType: OVNKubernetes
  serviceNetwork:
  - 172.30.0.0/16</pre><p>
					The default cluster network <code class="literal">cidr</code> <code class="literal">10.128.0.0/14</code> cannot be used if the cluster size is more than 500 nodes. It must be set to <code class="literal">10.128.0.0/12</code> or <code class="literal">10.128.0.0/10</code> to get to larger node counts beyond 500 nodes.
				</p></section><section class="section" id="ipsec-impact_optimizing-networking"><div class="titlepage"><div><div><h3 class="title">7.3.3. Impact of IPsec</h3></div></div></div><p>
					Because encrypting and decrypting node hosts uses CPU power, performance is affected both in throughput and CPU usage on the nodes when encryption is enabled, regardless of the IP security system being used.
				</p><p>
					IPSec encrypts traffic at the IP payload level, before it hits the NIC, protecting fields that would otherwise be used for NIC offloading. This means that some NIC acceleration features might not be usable when IPSec is enabled and will lead to decreased throughput and increased CPU usage.
				</p></section><section class="section _additional-resources" id="optimizing-networking-additional-resources"><div class="titlepage"><div><div><h3 class="title">7.3.4. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#modifying-nwoperator-config-startup_installing-aws-network-customizations">Modifying advanced network configuration parameters</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-operator-configuration-parameters-for-ovn-sdn_cluster-network-operator">Configuration parameters for the OVN-Kubernetes network plugin</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-operator-configuration-parameters-for-openshift-sdn_cluster-network-operator">Configuration parameters for the OpenShift SDN network plugin</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#scaling-worker-latency-profiles">Improving cluster stability in high latency environments using worker latency profiles</a>
						</li></ul></div></section></section><section class="section" id="optimizing-cpu-usage"><div class="titlepage"><div><div><h2 class="title">7.4. Optimizing CPU usage with mount namespace encapsulation</h2></div></div></div><p>
				You can optimize CPU usage in OpenShift Container Platform clusters by using mount namespace encapsulation to provide a private namespace for kubelet and CRI-O processes. This reduces the cluster CPU resources used by systemd with no difference in functionality.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Mount namespace encapsulation is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
				</p><p>
					For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
				</p></div></div><section class="section" id="optimizing-cpu-usage_optimizing-cpu-usage"><div class="titlepage"><div><div><h3 class="title">7.4.1. Encapsulating mount namespaces</h3></div></div></div><p>
					Mount namespaces are used to isolate mount points so that processes in different namespaces cannot view each others' files. Encapsulation is the process of moving Kubernetes mount namespaces to an alternative location where they will not be constantly scanned by the host operating system.
				</p><p>
					The host operating system uses systemd to constantly scan all mount namespaces: both the standard Linux mounts and the numerous mounts that Kubernetes uses to operate. The current implementation of kubelet and CRI-O both use the top-level namespace for all container runtime and kubelet mount points. However, encapsulating these container-specific mount points in a private namespace reduces systemd overhead with no difference in functionality. Using a separate mount namespace for both CRI-O and kubelet can encapsulate container-specific mounts from any systemd or other host operating system interaction.
				</p><p>
					This ability to potentially achieve major CPU optimization is now available to all OpenShift Container Platform administrators. Encapsulation can also improve security by storing Kubernetes-specific mount points in a location safe from inspection by unprivileged users.
				</p><p>
					The following diagrams illustrate a Kubernetes installation before and after encapsulation. Both scenarios show example containers which have mount propagation settings of bidirectional, host-to-container, and none.
				</p><div class="informalfigure"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Scalability_and_performance-en-US/images/49f9f5423ea61eba1a7c695297639858/before-k8s-mount-propagation.png" alt="Before encapsulation"/></div></div><p>
					Here we see systemd, host operating system processes, kubelet, and the container runtime sharing a single mount namespace.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							systemd, host operating system processes, kubelet, and the container runtime each have access to and visibility of all mount points.
						</li><li class="listitem">
							Container 1, configured with bidirectional mount propagation, can access systemd and host mounts, kubelet and CRI-O mounts. A mount originating in Container 1, such as <code class="literal">/run/a</code> is visible to systemd, host operating system processes, kubelet, container runtime, and other containers with host-to-container or bidirectional mount propagation configured (as in Container 2).
						</li><li class="listitem">
							Container 2, configured with host-to-container mount propagation, can access systemd and host mounts, kubelet and CRI-O mounts. A mount originating in Container 2, such as <code class="literal">/run/b</code>, is not visible to any other context.
						</li><li class="listitem">
							Container 3, configured with no mount propagation, has no visibility of external mount points. A mount originating in Container 3, such as <code class="literal">/run/c</code>, is not visible to any other context.
						</li></ul></div><p>
					The following diagram illustrates the system state after encapsulation.
				</p><div class="informalfigure"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Scalability_and_performance-en-US/images/94309215668384092001f3be82e840bf/after-k8s-mount-propagation.png" alt="After encapsulation"/></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The main systemd process is no longer devoted to unnecessary scanning of Kubernetes-specific mount points. It only monitors systemd-specific and host mount points.
						</li><li class="listitem">
							The host operating system processes can access only the systemd and host mount points.
						</li><li class="listitem">
							Using a separate mount namespace for both CRI-O and kubelet completely separates all container-specific mounts away from any systemd or other host operating system interaction whatsoever.
						</li><li class="listitem">
							The behavior of Container 1 is unchanged, except a mount it creates such as <code class="literal">/run/a</code> is no longer visible to systemd or host operating system processes. It is still visible to kubelet, CRI-O, and other containers with host-to-container or bidirectional mount propagation configured (like Container 2).
						</li><li class="listitem">
							The behavior of Container 2 and Container 3 is unchanged.
						</li></ul></div></section><section class="section" id="enabling-encapsulation_optimizing-cpu-usage"><div class="titlepage"><div><div><h3 class="title">7.4.2. Configuring mount namespace encapsulation</h3></div></div></div><p>
					You can configure mount namespace encapsulation so that a cluster runs with less resource overhead.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Mount namespace encapsulation is a Technology Preview feature and it is disabled by default. To use it, you must enable the feature manually.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have logged in as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a file called <code class="literal">mount_namespace_config.yaml</code> with the following YAML:
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 99-kubens-master
spec:
  config:
    ignition:
      version: 3.2.0
    systemd:
      units:
      - enabled: true
        name: kubens.service
---
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 99-kubens-worker
spec:
  config:
    ignition:
      version: 3.2.0
    systemd:
      units:
      - enabled: true
        name: kubens.service</pre></li><li class="listitem"><p class="simpara">
							Apply the mount namespace <code class="literal">MachineConfig</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f mount_namespace_config.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">machineconfig.machineconfiguration.openshift.io/99-kubens-master created
machineconfig.machineconfiguration.openshift.io/99-kubens-worker created</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							The <code class="literal">MachineConfig</code> CR can take up to 30 minutes to finish being applied in the cluster. You can check the status of the <code class="literal">MachineConfig</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get mcp</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master   rendered-master-03d4bc4befb0f4ed3566a2c8f7636751   False     True       False      3              0                   0                     0                      45m
worker   rendered-worker-10577f6ab0117ed1825f8af2ac687ddf   False     True       False      3              1                   1</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Wait for the <code class="literal">MachineConfig</code> CR to be applied successfully across all control plane and worker nodes after running the following command:
						</p><pre class="programlisting language-terminal">$ oc wait --for=condition=Updated mcp --all --timeout=30m</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">machineconfigpool.machineconfiguration.openshift.io/master condition met
machineconfigpool.machineconfiguration.openshift.io/worker condition met</pre>

							</p></div></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						To verify encapsulation for a cluster host, run the following commands:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Open a debug shell to the cluster host:
						</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Open a <code class="literal">chroot</code> session:
						</p><pre class="programlisting language-terminal">sh-4.4# chroot /host</pre></li><li class="listitem"><p class="simpara">
							Check the systemd mount namespace:
						</p><pre class="programlisting language-terminal">sh-4.4# readlink /proc/1/ns/mnt</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">mnt:[4026531953]</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check kubelet mount namespace:
						</p><pre class="programlisting language-terminal">sh-4.4# readlink /proc/$(pgrep kubelet)/ns/mnt</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">mnt:[4026531840]</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check the CRI-O mount namespace:
						</p><pre class="programlisting language-terminal">sh-4.4# readlink /proc/$(pgrep crio)/ns/mnt</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">mnt:[4026531840]</pre>

							</p></div></li></ol></div><p>
					These commands return the mount namespaces associated with systemd, kubelet, and the container runtime. In OpenShift Container Platform, the container runtime is CRI-O.
				</p><p>
					Encapsulation is in effect if systemd is in a different mount namespace to kubelet and CRI-O as in the above example. Encapsulation is not in effect if all three processes are in the same mount namespace.
				</p></section><section class="section" id="supporting-encapsulation_optimizing-cpu-usage"><div class="titlepage"><div><div><h3 class="title">7.4.3. Inspecting encapsulated namespaces</h3></div></div></div><p>
					You can inspect Kubernetes-specific mount points in the cluster host operating system for debugging or auditing purposes by using the <code class="literal">kubensenter</code> script that is available in Red Hat Enterprise Linux CoreOS (RHCOS).
				</p><p>
					SSH shell sessions to the cluster host are in the default namespace. To inspect Kubernetes-specific mount points in an SSH shell prompt, you need to run the <code class="literal">kubensenter</code> script as root. The <code class="literal">kubensenter</code> script is aware of the state of the mount encapsulation, and is safe to run even if encapsulation is not enabled.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						<code class="literal">oc debug</code> remote shell sessions start inside the Kubernetes namespace by default. You do not need to run <code class="literal">kubensenter</code> to inspect mount points when you use <code class="literal">oc debug</code>.
					</p></div></div><p>
					If the encapsulation feature is not enabled, the <code class="literal">kubensenter findmnt</code> and <code class="literal">findmnt</code> commands return the same output, regardless of whether they are run in an <code class="literal">oc debug</code> session or in an SSH shell prompt.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have logged in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							You have configured SSH access to the cluster host.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Open a remote SSH shell to the cluster host. For example:
						</p><pre class="programlisting language-terminal">$ ssh core@&lt;node_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Run commands using the provided <code class="literal">kubensenter</code> script as the root user. To run a single command inside the Kubernetes namespace, provide the command and any arguments to the <code class="literal">kubensenter</code> script. For example, to run the <code class="literal">findmnt</code> command inside the Kubernetes namespace, run the following command:
						</p><pre class="programlisting language-terminal">[core@control-plane-1 ~]$ sudo kubensenter findmnt</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">kubensenter: Autodetect: kubens.service namespace found at /run/kubens/mnt
TARGET                                SOURCE                 FSTYPE     OPTIONS
/                                     /dev/sda4[/ostree/deploy/rhcos/deploy/32074f0e8e5ec453e56f5a8a7bc9347eaa4172349ceab9c22b709d9d71a3f4b0.0]
|                                                            xfs        rw,relatime,seclabel,attr2,inode64,logbufs=8,logbsize=32k,prjquota
                                      shm                    tmpfs
...</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							To start a new interactive shell inside the Kubernetes namespace, run the <code class="literal">kubensenter</code> script without any arguments:
						</p><pre class="programlisting language-terminal">[core@control-plane-1 ~]$ sudo kubensenter</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">kubensenter: Autodetect: kubens.service namespace found at /run/kubens/mnt</pre>

							</p></div></li></ol></div></section><section class="section" id="running-services-with-encapsulation_optimizing-cpu-usage"><div class="titlepage"><div><div><h3 class="title">7.4.4. Running additional services in the encapsulated namespace</h3></div></div></div><p>
					Any monitoring tool that relies on the ability to run in the host operating system and have visibility of mount points created by kubelet, CRI-O, or containers themselves, must enter the container mount namespace to see these mount points. The <code class="literal">kubensenter</code> script that is provided with OpenShift Container Platform executes another command inside the Kubernetes mount point and can be used to adapt any existing tools.
				</p><p>
					The <code class="literal">kubensenter</code> script is aware of the state of the mount encapsulation feature status, and is safe to run even if encapsulation is not enabled. In that case the script executes the provided command in the default mount namespace.
				</p><p>
					For example, if a systemd service needs to run inside the new Kubernetes mount namespace, edit the service file and use the <code class="literal">ExecStart=</code> command line with <code class="literal">kubensenter</code>.
				</p><pre class="programlisting language-terminal">[Unit]
Description=Example service
[Service]
ExecStart=/usr/bin/kubensenter /path/to/original/command arg1 arg2</pre></section><section class="section _additional-resources" id="optimizing-cpu-usage-additional-resources"><div class="titlepage"><div><div><h3 class="title">7.4.5. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/monitoring_and_managing_system_status_and_performance/setting-limits-for-applications_monitoring-and-managing-system-status-and-performance#what-namespaces-are_setting-limits-for-applications">What are namespaces</a>
						</li><li class="listitem">
							<a class="link" href="https://www.redhat.com/sysadmin/container-namespaces-nsenter">Manage containers in namespaces by using nsenter</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/api_reference/#machineconfig-machineconfiguration.openshift.io/v1">MachineConfig</a>
						</li></ul></div></section></section></section><section class="chapter" id="managing-bare-metal-hosts"><div class="titlepage"><div><div><h1 class="title">Chapter 8. Managing bare metal hosts</h1></div></div></div><p>
			When you install OpenShift Container Platform on a bare metal cluster, you can provision and manage bare metal nodes using <code class="literal">machine</code> and <code class="literal">machineset</code> custom resources (CRs) for bare metal hosts that exist in the cluster.
		</p><section class="section" id="about-bare-metal-hosts-and-nodes_managing-bare-metal-hosts"><div class="titlepage"><div><div><h2 class="title">8.1. About bare metal hosts and nodes</h2></div></div></div><p>
				To provision a Red Hat Enterprise Linux CoreOS (RHCOS) bare metal host as a node in your cluster, first create a <code class="literal">MachineSet</code> custom resource (CR) object that corresponds to the bare metal host hardware. Bare metal host compute machine sets describe infrastructure components specific to your configuration. You apply specific Kubernetes labels to these compute machine sets and then update the infrastructure components to run on only those machines.
			</p><p>
				<code class="literal">Machine</code> CR’s are created automatically when you scale up the relevant <code class="literal">MachineSet</code> containing a <code class="literal">metal3.io/autoscale-to-hosts</code> annotation. OpenShift Container Platform uses <code class="literal">Machine</code> CR’s to provision the bare metal node that corresponds to the host as specified in the <code class="literal">MachineSet</code> CR.
			</p></section><section class="section" id="maintaining-bare-metal-hosts_managing-bare-metal-hosts"><div class="titlepage"><div><div><h2 class="title">8.2. Maintaining bare metal hosts</h2></div></div></div><p>
				You can maintain the details of the bare metal hosts in your cluster from the OpenShift Container Platform web console. Navigate to <span class="strong strong"><strong>Compute</strong></span> → <span class="strong strong"><strong>Bare Metal Hosts</strong></span>, and select a task from the <span class="strong strong"><strong>Actions</strong></span> drop down menu. Here you can manage items such as BMC details, boot MAC address for the host, enable power management, and so on. You can also review the details of the network interfaces and drives for the host.
			</p><p>
				You can move a bare metal host into maintenance mode. When you move a host into maintenance mode, the scheduler moves all managed workloads off the corresponding bare metal node. No new workloads are scheduled while in maintenance mode.
			</p><p>
				You can deprovision a bare metal host in the web console. Deprovisioning a host does the following actions:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Annotates the bare metal host CR with <code class="literal">cluster.k8s.io/delete-machine: true</code>
					</li><li class="listitem">
						Scales down the related compute machine set
					</li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Powering off the host without first moving the daemon set and unmanaged static pods to another node can cause service disruption and loss of data.
				</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#adding-bare-metal-compute-user-infra">Adding compute machines to bare metal</a>
					</li></ul></div><section class="section" id="adding-bare-metal-host-to-cluster-using-web-console_managing-bare-metal-hosts"><div class="titlepage"><div><div><h3 class="title">8.2.1. Adding a bare metal host to the cluster using the web console</h3></div></div></div><p>
					You can add bare metal hosts to the cluster in the web console.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install an RHCOS cluster on bare metal.
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the web console, navigate to <span class="strong strong"><strong>Compute</strong></span> → <span class="strong strong"><strong>Bare Metal Hosts</strong></span>.
						</li><li class="listitem">
							Select <span class="strong strong"><strong>Add Host</strong></span> → <span class="strong strong"><strong>New with Dialog</strong></span>.
						</li><li class="listitem">
							Specify a unique name for the new bare metal host.
						</li><li class="listitem">
							Set the <span class="strong strong"><strong>Boot MAC address</strong></span>.
						</li><li class="listitem">
							Set the <span class="strong strong"><strong>Baseboard Management Console (BMC) Address</strong></span>.
						</li><li class="listitem">
							Enter the user credentials for the host’s baseboard management controller (BMC).
						</li><li class="listitem">
							Select to power on the host after creation, and select <span class="strong strong"><strong>Create</strong></span>.
						</li><li class="listitem">
							Scale up the number of replicas to match the number of available bare metal hosts. Navigate to <span class="strong strong"><strong>Compute</strong></span> → <span class="strong strong"><strong>MachineSets</strong></span>, and increase the number of machine replicas in the cluster by selecting <span class="strong strong"><strong>Edit Machine count</strong></span> from the <span class="strong strong"><strong>Actions</strong></span> drop-down menu.
						</li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can also manage the number of bare metal nodes using the <code class="literal">oc scale</code> command and the appropriate bare metal compute machine set.
					</p></div></div></section><section class="section" id="adding-bare-metal-host-to-cluster-using-yaml_managing-bare-metal-hosts"><div class="titlepage"><div><div><h3 class="title">8.2.2. Adding a bare metal host to the cluster using YAML in the web console</h3></div></div></div><p>
					You can add bare metal hosts to the cluster in the web console using a YAML file that describes the bare metal host.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install a RHCOS compute machine on bare metal infrastructure for use in the cluster.
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							Create a <code class="literal">Secret</code> CR for the bare metal host.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the web console, navigate to <span class="strong strong"><strong>Compute</strong></span> → <span class="strong strong"><strong>Bare Metal Hosts</strong></span>.
						</li><li class="listitem">
							Select <span class="strong strong"><strong>Add Host</strong></span> → <span class="strong strong"><strong>New from YAML</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Copy and paste the below YAML, modifying the relevant fields with the details of your host:
						</p><pre class="programlisting language-yaml">apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: &lt;bare_metal_host_name&gt;
spec:
  online: true
  bmc:
    address: &lt;bmc_address&gt;
    credentialsName: &lt;secret_credentials_name&gt;  <span id="CO20-1"><!--Empty--></span><span class="callout">1</span>
    disableCertificateVerification: True <span id="CO20-2"><!--Empty--></span><span class="callout">2</span>
  bootMACAddress: &lt;host_boot_mac_address&gt;</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO20-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									<code class="literal">credentialsName</code> must reference a valid <code class="literal">Secret</code> CR. The <code class="literal">baremetal-operator</code> cannot manage the bare metal host without a valid <code class="literal">Secret</code> referenced in the <code class="literal">credentialsName</code>. For more information about secrets and how to create them, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-secrets-about_nodes-pods-secrets">Understanding secrets</a>.
								</div></dd><dt><a href="#CO20-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Setting <code class="literal">disableCertificateVerification</code> to <code class="literal">true</code> disables TLS host validation between the cluster and the baseboard management controller (BMC).
								</div></dd></dl></div></li><li class="listitem">
							Select <span class="strong strong"><strong>Create</strong></span> to save the YAML and create the new bare metal host.
						</li><li class="listitem"><p class="simpara">
							Scale up the number of replicas to match the number of available bare metal hosts. Navigate to <span class="strong strong"><strong>Compute</strong></span> → <span class="strong strong"><strong>MachineSets</strong></span>, and increase the number of machines in the cluster by selecting <span class="strong strong"><strong>Edit Machine count</strong></span> from the <span class="strong strong"><strong>Actions</strong></span> drop-down menu.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								You can also manage the number of bare metal nodes using the <code class="literal">oc scale</code> command and the appropriate bare metal compute machine set.
							</p></div></div></li></ol></div></section><section class="section" id="automatically-scaling-machines-to-available-bare-metal-hosts_managing-bare-metal-hosts"><div class="titlepage"><div><div><h3 class="title">8.2.3. Automatically scaling machines to the number of available bare metal hosts</h3></div></div></div><p>
					To automatically create the number of <code class="literal">Machine</code> objects that matches the number of available <code class="literal">BareMetalHost</code> objects, add a <code class="literal">metal3.io/autoscale-to-hosts</code> annotation to the <code class="literal">MachineSet</code> object.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install RHCOS bare metal compute machines for use in the cluster, and create corresponding <code class="literal">BareMetalHost</code> objects.
						</li><li class="listitem">
							Install the OpenShift Container Platform CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Annotate the compute machine set that you want to configure for automatic scaling by adding the <code class="literal">metal3.io/autoscale-to-hosts</code> annotation. Replace <code class="literal">&lt;machineset&gt;</code> with the name of the compute machine set.
						</p><pre class="programlisting language-terminal">$ oc annotate machineset &lt;machineset&gt; -n openshift-machine-api 'metal3.io/autoscale-to-hosts=&lt;any_value&gt;'</pre><p class="simpara">
							Wait for the new scaled machines to start.
						</p></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						When you use a <code class="literal">BareMetalHost</code> object to create a machine in the cluster and labels or selectors are subsequently changed on the <code class="literal">BareMetalHost</code>, the <code class="literal">BareMetalHost</code> object continues be counted against the <code class="literal">MachineSet</code> that the <code class="literal">Machine</code> object was created from.
					</p></div></div></section><section class="section" id="removing-bare-metal-hosts-from-provisioner_managing-bare-metal-hosts"><div class="titlepage"><div><div><h3 class="title">8.2.4. Removing bare metal hosts from the provisioner node</h3></div></div></div><p>
					In certain circumstances, you might want to temporarily remove bare metal hosts from the provisioner node. For example, during provisioning when a bare metal host reboot is triggered by using the OpenShift Container Platform administration console or as a result of a Machine Config Pool update, OpenShift Container Platform logs into the integrated Dell Remote Access Controller (iDrac) and issues a delete of the job queue.
				</p><p>
					To prevent the management of the number of <code class="literal">Machine</code> objects that matches the number of available <code class="literal">BareMetalHost</code> objects, add a <code class="literal">baremetalhost.metal3.io/detached</code> annotation to the <code class="literal">MachineSet</code> object.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						This annotation has an effect for only <code class="literal">BareMetalHost</code> objects that are in either <code class="literal">Provisioned</code>, <code class="literal">ExternallyProvisioned</code> or <code class="literal">Ready/Available</code> state.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install RHCOS bare metal compute machines for use in the cluster and create corresponding <code class="literal">BareMetalHost</code> objects.
						</li><li class="listitem">
							Install the OpenShift Container Platform CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Annotate the compute machine set that you want to remove from the provisioner node by adding the <code class="literal">baremetalhost.metal3.io/detached</code> annotation.
						</p><pre class="programlisting language-terminal">$ oc annotate machineset &lt;machineset&gt; -n openshift-machine-api 'baremetalhost.metal3.io/detached'</pre><p class="simpara">
							Wait for the new machines to start.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								When you use a <code class="literal">BareMetalHost</code> object to create a machine in the cluster and labels or selectors are subsequently changed on the <code class="literal">BareMetalHost</code>, the <code class="literal">BareMetalHost</code> object continues be counted against the <code class="literal">MachineSet</code> that the <code class="literal">Machine</code> object was created from.
							</p></div></div></li><li class="listitem"><p class="simpara">
							In the provisioning use case, remove the annotation after the reboot is complete by using the following command:
						</p><pre class="programlisting language-terminal">$ oc annotate machineset &lt;machineset&gt; -n openshift-machine-api 'baremetalhost.metal3.io/detached-'</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#ipi-install-expanding-the-cluster">Expanding the cluster</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#machine-health-checks-bare-metal_deploying-machine-health-checks">MachineHealthChecks on bare metal</a>
						</li></ul></div></section></section></section><section class="chapter" id="what-huge-pages-do-and-how-they-are-consumed"><div class="titlepage"><div><div><h1 class="title">Chapter 9. What huge pages do and how they are consumed by applications</h1></div></div></div><section class="section" id="what-huge-pages-do_huge-pages"><div class="titlepage"><div><div><h2 class="title">9.1. What huge pages do</h2></div></div></div><p>
				Memory is managed in blocks known as pages. On most systems, a page is 4Ki. 1Mi of memory is equal to 256 pages; 1Gi of memory is 256,000 pages, and so on. CPUs have a built-in memory management unit that manages a list of these pages in hardware. The Translation Lookaside Buffer (TLB) is a small hardware cache of virtual-to-physical page mappings. If the virtual address passed in a hardware instruction can be found in the TLB, the mapping can be determined quickly. If not, a TLB miss occurs, and the system falls back to slower, software-based address translation, resulting in performance issues. Since the size of the TLB is fixed, the only way to reduce the chance of a TLB miss is to increase the page size.
			</p><p>
				A huge page is a memory page that is larger than 4Ki. On x86_64 architectures, there are two common huge page sizes: 2Mi and 1Gi. Sizes vary on other architectures. To use huge pages, code must be written so that applications are aware of them. Transparent Huge Pages (THP) attempt to automate the management of huge pages without application knowledge, but they have limitations. In particular, they are limited to 2Mi page sizes. THP can lead to performance degradation on nodes with high memory utilization or fragmentation due to defragmenting efforts of THP, which can lock memory pages. For this reason, some applications may be designed to (or recommend) usage of pre-allocated huge pages instead of THP.
			</p><p>
				In OpenShift Container Platform, applications in a pod can allocate and consume pre-allocated huge pages.
			</p></section><section class="section" id="how-huge-pages-are-consumed-by-apps_huge-pages"><div class="titlepage"><div><div><h2 class="title">9.2. How huge pages are consumed by apps</h2></div></div></div><p>
				Nodes must pre-allocate huge pages in order for the node to report its huge page capacity. A node can only pre-allocate huge pages for a single size.
			</p><p>
				Huge pages can be consumed through container-level resource requirements using the resource name <code class="literal">hugepages-&lt;size&gt;</code>, where size is the most compact binary notation using integer values supported on a particular node. For example, if a node supports 2048KiB page sizes, it exposes a schedulable resource <code class="literal">hugepages-2Mi</code>. Unlike CPU or memory, huge pages do not support over-commitment.
			</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  generateName: hugepages-volume-
spec:
  containers:
  - securityContext:
      privileged: true
    image: rhel7:latest
    command:
    - sleep
    - inf
    name: example
    volumeMounts:
    - mountPath: /dev/hugepages
      name: hugepage
    resources:
      limits:
        hugepages-2Mi: 100Mi <span id="CO21-1"><!--Empty--></span><span class="callout">1</span>
        memory: "1Gi"
        cpu: "1"
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO21-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						Specify the amount of memory for <code class="literal">hugepages</code> as the exact amount to be allocated. Do not specify this value as the amount of memory for <code class="literal">hugepages</code> multiplied by the size of the page. For example, given a huge page size of 2MB, if you want to use 100MB of huge-page-backed RAM for your application, then you would allocate 50 huge pages. OpenShift Container Platform handles the math for you. As in the above example, you can specify <code class="literal">100MB</code> directly.
					</div></dd></dl></div><p>
				<span class="strong strong"><strong>Allocating huge pages of a specific size</strong></span>
			</p><p>
				Some platforms support multiple huge page sizes. To allocate huge pages of a specific size, precede the huge pages boot command parameters with a huge page size selection parameter <code class="literal">hugepagesz=&lt;size&gt;</code>. The <code class="literal">&lt;size&gt;</code> value must be specified in bytes with an optional scale suffix [<code class="literal">kKmMgG</code>]. The default huge page size can be defined with the <code class="literal">default_hugepagesz=&lt;size&gt;</code> boot parameter.
			</p><p>
				<span class="strong strong"><strong>Huge page requirements</strong></span>
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Huge page requests must equal the limits. This is the default if limits are specified, but requests are not.
					</li><li class="listitem">
						Huge pages are isolated at a pod scope. Container isolation is planned in a future iteration.
					</li><li class="listitem">
						<code class="literal">EmptyDir</code> volumes backed by huge pages must not consume more huge page memory than the pod request.
					</li><li class="listitem">
						Applications that consume huge pages via <code class="literal">shmget()</code> with <code class="literal">SHM_HUGETLB</code> must run with a supplemental group that matches <span class="strong strong"><strong><span class="emphasis"><em>proc/sys/vm/hugetlb_shm_group</em></span></strong></span>.
					</li></ul></div></section><section class="section" id="consuming-huge-pages-resource-using-the-downward-api_huge-pages"><div class="titlepage"><div><div><h2 class="title">9.3. Consuming huge pages resources using the Downward API</h2></div></div></div><p>
				You can use the Downward API to inject information about the huge pages resources that are consumed by a container.
			</p><p>
				You can inject the resource allocation as environment variables, a volume plugin, or both. Applications that you develop and run in the container can determine the resources that are available by reading the environment variables or files in the specified volumes.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a <code class="literal">hugepages-volume-pod.yaml</code> file that is similar to the following example:
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  generateName: hugepages-volume-
  labels:
    app: hugepages-example
spec:
  containers:
  - securityContext:
      capabilities:
        add: [ "IPC_LOCK" ]
    image: rhel7:latest
    command:
    - sleep
    - inf
    name: example
    volumeMounts:
    - mountPath: /dev/hugepages
      name: hugepage
    - mountPath: /etc/podinfo
      name: podinfo
    resources:
      limits:
        hugepages-1Gi: 2Gi
        memory: "1Gi"
        cpu: "1"
      requests:
        hugepages-1Gi: 2Gi
    env:
    - name: REQUESTS_HUGEPAGES_1GI &lt;.&gt;
      valueFrom:
        resourceFieldRef:
          containerName: example
          resource: requests.hugepages-1Gi
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages
  - name: podinfo
    downwardAPI:
      items:
        - path: "hugepages_1G_request" &lt;.&gt;
          resourceFieldRef:
            containerName: example
            resource: requests.hugepages-1Gi
            divisor: 1Gi</pre><p class="simpara">
						&lt;.&gt; Specifies to read the resource use from <code class="literal">requests.hugepages-1Gi</code> and expose the value as the <code class="literal">REQUESTS_HUGEPAGES_1GI</code> environment variable. &lt;.&gt; Specifies to read the resource use from <code class="literal">requests.hugepages-1Gi</code> and expose the value as the file <code class="literal">/etc/podinfo/hugepages_1G_request</code>.
					</p></li><li class="listitem"><p class="simpara">
						Create the pod from the <code class="literal">hugepages-volume-pod.yaml</code> file:
					</p><pre class="programlisting language-terminal">$ oc create -f hugepages-volume-pod.yaml</pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Check the value of the <code class="literal">REQUESTS_HUGEPAGES_1GI</code> environment variable:
					</p><pre class="programlisting language-terminal">$ oc exec -it $(oc get pods -l app=hugepages-example -o jsonpath='{.items[0].metadata.name}') \
     -- env | grep REQUESTS_HUGEPAGES_1GI</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">REQUESTS_HUGEPAGES_1GI=2147483648</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Check the value of the <code class="literal">/etc/podinfo/hugepages_1G_request</code> file:
					</p><pre class="programlisting language-terminal">$ oc exec -it $(oc get pods -l app=hugepages-example -o jsonpath='{.items[0].metadata.name}') \
     -- cat /etc/podinfo/hugepages_1G_request</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">2</pre>

						</p></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-containers-downward-api">Allowing containers to consume Downward API objects</a>
					</li></ul></div></section><section class="section" id="configuring-huge-pages_huge-pages"><div class="titlepage"><div><div><h2 class="title">9.4. Configuring huge pages at boot time</h2></div></div></div><p>
				Nodes must pre-allocate huge pages used in an OpenShift Container Platform cluster. There are two ways of reserving huge pages: at boot time and at run time. Reserving at boot time increases the possibility of success because the memory has not yet been significantly fragmented. The Node Tuning Operator currently supports boot time allocation of huge pages on specific nodes.
			</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					To minimize node reboots, the order of the steps below needs to be followed:
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Label all nodes that need the same huge pages setting by a label.
					</p><pre class="programlisting language-terminal">$ oc label node &lt;node_using_hugepages&gt; node-role.kubernetes.io/worker-hp=</pre></li><li class="listitem"><p class="simpara">
						Create a file with the following content and name it <code class="literal">hugepages-tuned-boottime.yaml</code>:
					</p><pre class="programlisting language-yaml">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: hugepages <span id="CO22-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile: <span id="CO22-2"><!--Empty--></span><span class="callout">2</span>
  - data: |
      [main]
      summary=Boot time configuration for hugepages
      include=openshift-node
      [bootloader]
      cmdline_openshift_node_hugepages=hugepagesz=2M hugepages=50 <span id="CO22-3"><!--Empty--></span><span class="callout">3</span>
    name: openshift-node-hugepages

  recommend:
  - machineConfigLabels: <span id="CO22-4"><!--Empty--></span><span class="callout">4</span>
      machineconfiguration.openshift.io/role: "worker-hp"
    priority: 30
    profile: openshift-node-hugepages</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO22-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Set the <code class="literal">name</code> of the Tuned resource to <code class="literal">hugepages</code>.
							</div></dd><dt><a href="#CO22-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Set the <code class="literal">profile</code> section to allocate huge pages.
							</div></dd><dt><a href="#CO22-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Note the order of parameters is important as some platforms support huge pages of various sizes.
							</div></dd><dt><a href="#CO22-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Enable machine config pool based matching.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Create the Tuned <code class="literal">hugepages</code> object
					</p><pre class="programlisting language-terminal">$ oc create -f hugepages-tuned-boottime.yaml</pre></li><li class="listitem"><p class="simpara">
						Create a file with the following content and name it <code class="literal">hugepages-mcp.yaml</code>:
					</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker-hp
  labels:
    worker-hp: ""
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-hp]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker-hp: ""</pre></li><li class="listitem"><p class="simpara">
						Create the machine config pool:
					</p><pre class="programlisting language-terminal">$ oc create -f hugepages-mcp.yaml</pre></li></ol></div><p>
				Given enough non-fragmented memory, all the nodes in the <code class="literal">worker-hp</code> machine config pool should now have 50 2Mi huge pages allocated.
			</p><pre class="programlisting language-terminal">$ oc get node &lt;node_using_hugepages&gt; -o jsonpath="{.status.allocatable.hugepages-2Mi}"
100Mi</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The TuneD bootloader plugin only supports Red Hat Enterprise Linux CoreOS (RHCOS) worker nodes.
				</p></div></div></section><section class="section" id="disable-thp_huge-pages"><div class="titlepage"><div><div><h2 class="title">9.5. Disabling Transparent Huge Pages</h2></div></div></div><p>
				Transparent Huge Pages (THP) attempt to automate most aspects of creating, managing, and using huge pages. Since THP automatically manages the huge pages, this is not always handled optimally for all types of workloads. THP can lead to performance regressions, since many applications handle huge pages on their own. Therefore, consider disabling THP. The following steps describe how to disable THP using the Node Tuning Operator (NTO).
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a file with the following content and name it <code class="literal">thp-disable-tuned.yaml</code>:
					</p><pre class="programlisting language-yaml">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: thp-workers-profile
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=Custom tuned profile for OpenShift to turn off THP on worker nodes
      include=openshift-node

      [vm]
      transparent_hugepages=never
    name: openshift-thp-never-worker

  recommend:
  - match:
    - label: node-role.kubernetes.io/worker
    priority: 25
    profile: openshift-thp-never-worker</pre></li><li class="listitem"><p class="simpara">
						Create the Tuned object:
					</p><pre class="programlisting language-terminal">$ oc create -f thp-disable-tuned.yaml</pre></li><li class="listitem"><p class="simpara">
						Check the list of active profiles:
					</p><pre class="programlisting language-terminal">$ oc get profile -n openshift-cluster-node-tuning-operator</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Log in to one of the nodes and do a regular THP check to verify if the nodes applied the profile successfully:
					</p><pre class="programlisting language-terminal">$ cat /sys/kernel/mm/transparent_hugepage/enabled</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">always madvise [never]</pre>

						</p></div></li></ul></div></section></section><section class="chapter" id="cnf-low-latency-tuning"><div class="titlepage"><div><div><h1 class="title">Chapter 10. Low latency tuning</h1></div></div></div><section class="section" id="cnf-understanding-low-latency_cnf-master"><div class="titlepage"><div><div><h2 class="title">10.1. Understanding low latency</h2></div></div></div><p>
				The emergence of Edge computing in the area of Telco / 5G plays a key role in reducing latency and congestion problems and improving application performance.
			</p><p>
				Simply put, latency determines how fast data (packets) moves from the sender to receiver and returns to the sender after processing by the receiver. Maintaining a network architecture with the lowest possible delay of latency speeds is key for meeting the network performance requirements of 5G. Compared to 4G technology, with an average latency of 50 ms, 5G is targeted to reach latency numbers of 1 ms or less. This reduction in latency boosts wireless throughput by a factor of 10.
			</p><p>
				Many of the deployed applications in the Telco space require low latency that can only tolerate zero packet loss. Tuning for zero packet loss helps mitigate the inherent issues that degrade network performance. For more information, see <a class="link" href="https://www.redhat.com/en/blog/tuning-zero-packet-loss-red-hat-openstack-platform-part-1">Tuning for Zero Packet Loss in Red Hat OpenStack Platform (RHOSP)</a>.
			</p><p>
				The Edge computing initiative also comes in to play for reducing latency rates. Think of it as being on the edge of the cloud and closer to the user. This greatly reduces the distance between the user and distant data centers, resulting in reduced application response times and performance latency.
			</p><p>
				Administrators must be able to manage their many Edge sites and local services in a centralized way so that all of the deployments can run at the lowest possible management cost. They also need an easy way to deploy and configure certain nodes of their cluster for real-time low latency and high-performance purposes. Low latency nodes are useful for applications such as Cloud-native Network Functions (CNF) and Data Plane Development Kit (DPDK).
			</p><p>
				OpenShift Container Platform currently provides mechanisms to tune software on an OpenShift Container Platform cluster for real-time running and low latency (around &lt;20 microseconds reaction time). This includes tuning the kernel and OpenShift Container Platform set values, installing a kernel, and reconfiguring the machine. But this method requires setting up four different Operators and performing many configurations that, when done manually, is complex and could be prone to mistakes.
			</p><p>
				OpenShift Container Platform uses the Node Tuning Operator to implement automatic tuning to achieve low latency performance for OpenShift Container Platform applications. The cluster administrator uses this performance profile configuration that makes it easier to make these changes in a more reliable way. The administrator can specify whether to update the kernel to kernel-rt, reserve CPUs for cluster and operating system housekeeping duties, including pod infra containers, and isolate CPUs for application containers to run the workloads.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Currently, disabling CPU load balancing is not supported by cgroup v2. As a result, you might not get the desired behavior from performance profiles if you have cgroup v2 enabled. Enabling cgroup v2 is not recommended if you are using performance profiles.
				</p></div></div><p>
				OpenShift Container Platform also supports workload hints for the Node Tuning Operator that can tune the <code class="literal">PerformanceProfile</code> to meet the demands of different industry environments. Workload hints are available for <code class="literal">highPowerConsumption</code> (very low latency at the cost of increased power consumption) and <code class="literal">realTime</code> (priority given to optimum latency). A combination of <code class="literal">true/false</code> settings for these hints can be used to deal with application-specific workload profiles and requirements.
			</p><p>
				Workload hints simplify the fine-tuning of performance to industry sector settings. Instead of a “one size fits all” approach, workload hints can cater to usage patterns such as placing priority on:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Low latency
					</li><li class="listitem">
						Real-time capability
					</li><li class="listitem">
						Efficient use of power
					</li></ul></div><p>
				In an ideal world, all of those would be prioritized: in real life, some come at the expense of others. The Node Tuning Operator is now aware of the workload expectations and better able to meet the demands of the workload. The cluster admin can now specify into which use case that workload falls. The Node Tuning Operator uses the <code class="literal">PerformanceProfile</code> to fine tune the performance settings for the workload.
			</p><p>
				The environment in which an application is operating influences its behavior. For a typical data center with no strict latency requirements, only minimal default tuning is needed that enables CPU partitioning for some high performance workload pods. For data centers and workloads where latency is a higher priority, measures are still taken to optimize power consumption. The most complicated cases are clusters close to latency-sensitive equipment such as manufacturing machinery and software-defined radios. This last class of deployment is often referred to as Far edge. For Far edge deployments, ultra-low latency is the ultimate priority, and is achieved at the expense of power management.
			</p><p>
				In OpenShift Container Platform version 4.10 and previous versions, the Performance Addon Operator was used to implement automatic tuning to achieve low latency performance. Now this functionality is part of the Node Tuning Operator.
			</p><section class="section" id="about_hyperthreading_for_low_latency_and_real_time_applications_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.1.1. About hyperthreading for low latency and real-time applications</h3></div></div></div><p>
					Hyperthreading is an Intel processor technology that allows a physical CPU processor core to function as two logical cores, executing two independent threads simultaneously. Hyperthreading allows for better system throughput for certain workload types where parallel processing is beneficial. The default OpenShift Container Platform configuration expects hyperthreading to be enabled by default.
				</p><p>
					For telecommunications applications, it is important to design your application infrastructure to minimize latency as much as possible. Hyperthreading can slow performance times and negatively affect throughput for compute intensive workloads that require low latency. Disabling hyperthreading ensures predictable performance and can decrease processing times for these workloads.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Hyperthreading implementation and configuration differs depending on the hardware you are running OpenShift Container Platform on. Consult the relevant host hardware tuning information for more details of the hyperthreading implementation specific to that hardware. Disabling hyperthreading can increase the cost per core of the cluster.
					</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="#configuring_hyperthreading_for_a_cluster_cnf-master" title="10.3.5. Configuring hyperthreading for a cluster">Configuring hyperthreading for a cluster</a>
						</li></ul></div></section></section><section class="section" id="cnf-provisioning-real-time-and-low-latency-workloads_cnf-master"><div class="titlepage"><div><div><h2 class="title">10.2. Provisioning real-time and low latency workloads</h2></div></div></div><p>
				Many industries and organizations need extremely high performance computing and might require low and predictable latency, especially in the financial and telecommunications industries. For these industries, with their unique requirements, OpenShift Container Platform provides the Node Tuning Operator to implement automatic tuning to achieve low latency performance and consistent response time for OpenShift Container Platform applications.
			</p><p>
				The cluster administrator can use this performance profile configuration to make these changes in a more reliable way. The administrator can specify whether to update the kernel to kernel-rt (real-time), reserve CPUs for cluster and operating system housekeeping duties, including pod infra containers, isolate CPUs for application containers to run the workloads, and disable unused CPUs to reduce power consumption.
			</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
					The usage of execution probes in conjunction with applications that require guaranteed CPUs can cause latency spikes. It is recommended to use other probes, such as a properly configured set of network probes, as an alternative.
				</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					In earlier versions of OpenShift Container Platform, the Performance Addon Operator was used to implement automatic tuning to achieve low latency performance for OpenShift applications. In OpenShift Container Platform 4.11 and later, these functions are part of the Node Tuning Operator.
				</p></div></div><section class="section" id="node-tuning-operator-known-limitations-for-real-time_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.2.1. Known limitations for real-time</h3></div></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						In most deployments, kernel-rt is supported only on worker nodes when you use a standard cluster with three control plane nodes and three worker nodes. There are exceptions for compact and single nodes on OpenShift Container Platform deployments. For installations on a single node, kernel-rt is supported on the single control plane node.
					</p></div></div><p>
					To fully utilize the real-time mode, the containers must run with elevated privileges. See <a class="link" href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container">Set capabilities for a Container</a> for information on granting privileges.
				</p><p>
					OpenShift Container Platform restricts the allowed capabilities, so you might need to create a <code class="literal">SecurityContext</code> as well.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						This procedure is fully supported with bare metal installations using Red Hat Enterprise Linux CoreOS (RHCOS) systems.
					</p></div></div><p>
					Establishing the right performance expectations refers to the fact that the real-time kernel is not a panacea. Its objective is consistent, low-latency determinism offering predictable response times. There is some additional kernel overhead associated with the real-time kernel. This is due primarily to handling hardware interruptions in separately scheduled threads. The increased overhead in some workloads results in some degradation in overall throughput. The exact amount of degradation is very workload dependent, ranging from 0% to 30%. However, it is the cost of determinism.
				</p></section><section class="section" id="node-tuning-operator-provisioning-worker-with-real-time-capabilities_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.2.2. Provisioning a worker with real-time capabilities</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Optional: Add a node to the OpenShift Container Platform cluster. See <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/8/html/optimizing_rhel_8_for_real_time_for_low_latency_operation/setting-bios-parameters-for-system-tuning_optimizing-rhel8-for-real-time-for-low-latency-operation">Setting BIOS parameters for system tuning</a>.
						</li><li class="listitem">
							Add the label <code class="literal">worker-rt</code> to the worker nodes that require the real-time capability by using the <code class="literal">oc</code> command.
						</li><li class="listitem"><p class="simpara">
							Create a new machine config pool for real-time nodes:
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker-rt
  labels:
    machineconfiguration.openshift.io/role: worker-rt
spec:
  machineConfigSelector:
    matchExpressions:
      - {
           key: machineconfiguration.openshift.io/role,
           operator: In,
           values: [worker, worker-rt],
        }
  paused: false
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker-rt: ""</pre><p class="simpara">
							Note that a machine config pool worker-rt is created for group of nodes that have the label <code class="literal">worker-rt</code>.
						</p></li><li class="listitem"><p class="simpara">
							Add the node to the proper machine config pool by using node role labels.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								You must decide which nodes are configured with real-time workloads. You could configure all of the nodes in the cluster, or a subset of the nodes. The Node Tuning Operator that expects all of the nodes are part of a dedicated machine config pool. If you use all of the nodes, you must point the Node Tuning Operator to the worker node role label. If you use a subset, you must group the nodes into a new machine config pool.
							</p></div></div></li><li class="listitem">
							Create the <code class="literal">PerformanceProfile</code> with the proper set of housekeeping cores and <code class="literal">realTimeKernel: enabled: true</code>.
						</li><li class="listitem"><p class="simpara">
							You must set <code class="literal">machineConfigPoolSelector</code> in <code class="literal">PerformanceProfile</code>:
						</p><pre class="programlisting language-yaml">  apiVersion: performance.openshift.io/v2
  kind: PerformanceProfile
  metadata:
   name: example-performanceprofile
  spec:
  ...
    realTimeKernel:
      enabled: true
    nodeSelector:
       node-role.kubernetes.io/worker-rt: ""
    machineConfigPoolSelector:
       machineconfiguration.openshift.io/role: worker-rt</pre></li><li class="listitem"><p class="simpara">
							Verify that a matching machine config pool exists with a label:
						</p><pre class="programlisting language-terminal">$ oc describe mcp/worker-rt</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">Name:         worker-rt
Namespace:
Labels:       machineconfiguration.openshift.io/role=worker-rt</pre>

							</p></div></li><li class="listitem">
							OpenShift Container Platform will start configuring the nodes, which might involve multiple reboots. Wait for the nodes to settle. This can take a long time depending on the specific hardware you use, but 20 minutes per node is expected.
						</li><li class="listitem">
							Verify everything is working as expected.
						</li></ol></div></section><section class="section" id="node-tuning-operator-verifying-real-time-kernel-installation_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.2.3. Verifying the real-time kernel installation</h3></div></div></div><p>
					Use this command to verify that the real-time kernel is installed:
				</p><pre class="programlisting language-terminal">$ oc get node -o wide</pre><p>
					Note the worker with the role <code class="literal">worker-rt</code> that contains the string <code class="literal">4.18.0-305.30.1.rt7.102.el8_4.x86_64 cri-o://1.26.0-99.rhaos4.10.gitc3131de.el8</code>:
				</p><pre class="programlisting language-terminal">NAME                               	STATUS   ROLES           	AGE 	VERSION                  	INTERNAL-IP
EXTERNAL-IP   OS-IMAGE                                       	KERNEL-VERSION
CONTAINER-RUNTIME
rt-worker-0.example.com	          Ready	 worker,worker-rt   5d17h   v1.26.0
128.66.135.107   &lt;none&gt;    	        Red Hat Enterprise Linux CoreOS 46.82.202008252340-0 (Ootpa)
4.18.0-305.30.1.rt7.102.el8_4.x86_64   cri-o://1.26.0-99.rhaos4.10.gitc3131de.el8
[...]</pre></section><section class="section" id="node-tuning-operator-creating-workload-that-works-in-real-time_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.2.4. Creating a workload that works in real-time</h3></div></div></div><p>
					Use the following procedures for preparing a workload that will use real-time capabilities.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Create a pod with a QoS class of <code class="literal">Guaranteed</code>.
						</li><li class="listitem">
							Optional: Disable CPU load balancing for DPDK.
						</li><li class="listitem">
							Assign a proper node selector.
						</li></ol></div><p>
					When writing your applications, follow the general recommendations described in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/8/html-single/tuning_guide/index#chap-Application_Tuning_and_Deployment">Application tuning and deployment</a>.
				</p></section><section class="section" id="node-tuning-operator-creating-pod-with-guaranteed-qos-class_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.2.5. Creating a pod with a QoS class of <code class="literal">Guaranteed</code></h3></div></div></div><p>
					Keep the following in mind when you create a pod that is given a QoS class of <code class="literal">Guaranteed</code>:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Every container in the pod must have a memory limit and a memory request, and they must be the same.
						</li><li class="listitem">
							Every container in the pod must have a CPU limit and a CPU request, and they must be the same.
						</li></ul></div><p>
					The following example shows the configuration file for a pod that has one container. The container has a memory limit and a memory request, both equal to 200 MiB. The container has a CPU limit and a CPU request, both equal to 1 CPU.
				</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: qos-demo
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-ctr
    image: &lt;image-pull-spec&gt;
    resources:
      limits:
        memory: "200Mi"
        cpu: "1"
      requests:
        memory: "200Mi"
        cpu: "1"</pre><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the pod:
						</p><pre class="programlisting language-terminal">$ oc  apply -f qos-pod.yaml --namespace=qos-example</pre></li><li class="listitem"><p class="simpara">
							View detailed information about the pod:
						</p><pre class="programlisting language-terminal">$ oc get pod qos-demo --namespace=qos-example --output=yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">spec:
  containers:
    ...
status:
  qosClass: Guaranteed</pre>

							</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If a container specifies its own memory limit, but does not specify a memory request, OpenShift Container Platform automatically assigns a memory request that matches the limit. Similarly, if a container specifies its own CPU limit, but does not specify a CPU request, OpenShift Container Platform automatically assigns a CPU request that matches the limit.
							</p></div></div></li></ol></div></section><section class="section" id="node-tuning-operator-disabling-cpu-load-balancing-for-dpdk_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.2.6. Optional: Disabling CPU load balancing for DPDK</h3></div></div></div><p>
					Functionality to disable or enable CPU load balancing is implemented on the CRI-O level. The code under the CRI-O disables or enables CPU load balancing only when the following requirements are met.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							The pod must use the <code class="literal">performance-&lt;profile-name&gt;</code> runtime class. You can get the proper name by looking at the status of the performance profile, as shown here:
						</p><pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
...
status:
  ...
  runtimeClass: performance-manual</pre></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Currently, disabling CPU load balancing is not supported with cgroup v2.
					</p></div></div><p>
					The Node Tuning Operator is responsible for the creation of the high-performance runtime handler config snippet under relevant nodes and for creation of the high-performance runtime class under the cluster. It will have the same content as default runtime handler except it enables the CPU load balancing configuration functionality.
				</p><p>
					To disable the CPU load balancing for the pod, the <code class="literal">Pod</code> specification must include the following fields:
				</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  ...
  annotations:
    ...
    cpu-load-balancing.crio.io: "disable"
    ...
  ...
spec:
  ...
  runtimeClassName: performance-&lt;profile_name&gt;
  ...</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Only disable CPU load balancing when the CPU manager static policy is enabled and for pods with guaranteed QoS that use whole CPUs. Otherwise, disabling CPU load balancing can affect the performance of other containers in the cluster.
					</p></div></div></section><section class="section" id="node-tuning-operator-assigning-proper-node-selector_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.2.7. Assigning a proper node selector</h3></div></div></div><p>
					The preferred way to assign a pod to nodes is to use the same node selector the performance profile used, as shown here:
				</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: example
spec:
  # ...
  nodeSelector:
    node-role.kubernetes.io/worker-rt: ""</pre><p>
					For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.5/html-single/nodes/index#nodes-scheduler-node-selectors">Placing pods on specific nodes using node selectors</a>.
				</p></section><section class="section" id="node-tuning-operator-scheduling-workload-onto-worker-with-real-time-capabilities_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.2.8. Scheduling a workload onto a worker with real-time capabilities</h3></div></div></div><p>
					Use label selectors that match the nodes attached to the machine config pool that was configured for low latency by the Node Tuning Operator. For more information, see <a class="link" href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">Assigning pods to nodes</a>.
				</p></section><section class="section" id="node-tuning-operator-disabling-CPUs-for-power-consumption_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.2.9. Reducing power consumption by taking CPUs offline</h3></div></div></div><p>
					You can generally anticipate telecommunication workloads. When not all of the CPU resources are required, the Node Tuning Operator allows you take unused CPUs offline to reduce power consumption by manually updating the performance profile.
				</p><p>
					To take unused CPUs offline, you must perform the following tasks:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Set the offline CPUs in the performance profile and save the contents of the YAML file:
						</p><div class="formalpara"><p class="title"><strong>Example performance profile with offlined CPUs</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: performance
spec:
  additionalKernelArgs:
  - nmi_watchdog=0
  - audit=0
  - mce=off
  - processor.max_cstate=1
  - intel_idle.max_cstate=0
  - idle=poll
  cpu:
    isolated: "2-23,26-47"
    reserved: "0,1,24,25"
    offlined: “48-59” <span id="CO23-1"><!--Empty--></span><span class="callout">1</span>
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""
  numa:
    topologyPolicy: single-numa-node
  realTimeKernel:
    enabled: true</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO23-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Optional. You can list CPUs in the <code class="literal">offlined</code> field to take the specified CPUs offline.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Apply the updated profile by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f my-performance-profile.yaml</pre></li></ol></div></section><section class="section" id="node-tuning-operator-pod-power-saving-config_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.2.10. Optional: Power saving configurations</h3></div></div></div><p>
					You can enable power savings for a node that has low priority workloads that are colocated with high priority workloads without impacting the latency or throughput of the high priority workloads. Power saving is possible without modifications to the workloads themselves.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The feature is supported on Intel Ice Lake and later generations of Intel CPUs. The capabilities of the processor might impact the latency and throughput of the high priority workloads.
					</p></div></div><p>
					When you configure a node with a power saving configuration, you must configure high priority workloads with performance configuration at the pod level, which means that the configuration applies to all the cores used by the pod.
				</p><p>
					By disabling P-states and C-states at the pod level, you can configure high priority workloads for best performance and lowest latency.
				</p><div class="table" id="idm139735333823856"><p class="title"><strong>Table 10.1. Configuration for high priority workloads</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 67%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735333819056" scope="col">Annotation</th><th align="left" valign="top" id="idm139735333817968" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735333819056">
<pre class="programlisting language-yaml">annotations:
  cpu-c-states.crio.io: "disable"
  cpu-freq-governor.crio.io: "&lt;governor&gt;"</pre>
								 </td><td align="left" valign="top" headers="idm139735333817968"> <p>
									Provides the best performance for a pod by disabling C-states and specifying the governor type for CPU scaling. The <code class="literal">performance</code> governor is recommended for high priority workloads.
								</p>
								 </td></tr></tbody></table></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You enabled C-states and OS-controlled P-states in the BIOS
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Generate a <code class="literal">PerformanceProfile</code> with <code class="literal">per-pod-power-management</code> set to <code class="literal">true</code>:
						</p><pre class="programlisting language-terminal">$ podman run --entrypoint performance-profile-creator -v \
/must-gather:/must-gather:z registry.redhat.io/openshift4/ose-cluster-node-tuning-operator:v4.13 \
--mcp-name=worker-cnf --reserved-cpu-count=20 --rt-kernel=true \
--split-reserved-cpus-across-numa=false --topology-manager-policy=single-numa-node \
--must-gather-dir-path /must-gather -power-consumption-mode=low-latency \ <span id="CO24-1"><!--Empty--></span><span class="callout">1</span>
--per-pod-power-management=true &gt; my-performance-profile.yaml</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO24-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal">power-consumption-mode</code> must be <code class="literal">default</code> or <code class="literal">low-latency</code> when the <code class="literal">per-pod-power-management</code> is set to <code class="literal">true</code>.
								</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example <code class="literal">PerformanceProfile</code> with <code class="literal">perPodPowerManagement</code></strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
     name: performance
spec:
    [.....]
    workloadHints:
        realTime: true
        highPowerConsumption: false
        perPodPowerManagement: true</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Set the default <code class="literal">cpufreq</code> governor as an additional kernel argument in the <code class="literal">PerformanceProfile</code> custom resource (CR):
						</p><pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
     name: performance
spec:
    ...
    additionalKernelArgs:
    - cpufreq.default_governor=schedutil <span id="CO25-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO25-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Using the <code class="literal">schedutil</code> governor is recommended, however, you can use other governors such as the <code class="literal">ondemand</code> or <code class="literal">powersave</code> governors.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Set the maximum CPU frequency in the <code class="literal">TunedPerformancePatch</code> CR:
						</p><pre class="programlisting language-yaml">spec:
  profile:
  - data: |
      [sysfs]
      /sys/devices/system/cpu/intel_pstate/max_perf_pct = &lt;x&gt; <span id="CO26-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO26-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal">max_perf_pct</code> controls the maximum frequency the <code class="literal">cpufreq</code> driver is allowed to set as a percentage of the maximum supported cpu frequency. This value applies to all CPUs. You can check the maximum supported frequency in <code class="literal">/sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq</code>. As a starting point, you can use a percentage that caps all CPUs at the <code class="literal">All Cores Turbo</code> frequency. The <code class="literal">All Cores Turbo</code> frequency is the frequency that all cores will run at when the cores are all fully occupied.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Add the desired annotations to your high priority workload pods. The annotations override the <code class="literal">default</code> settings.
						</p><div class="formalpara"><p class="title"><strong>Example high priority workload annotation</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  ...
  annotations:
    ...
    cpu-c-states.crio.io: "disable"
    cpu-freq-governor.crio.io: "&lt;governor&gt;"
    ...
  ...
spec:
  ...
  runtimeClassName: performance-&lt;profile_name&gt;
  ...</pre>

							</p></div></li><li class="listitem">
							Restart the pods.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For more information about recommended firmware configuration, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-du-firmware-config-reference_vdu-config-ref">Recommended firmware configuration for vDU cluster hosts</a>.
						</li></ul></div></section><section class="section" id="managing-device-interrupt-processing-for-guaranteed-pod-isolated-cpus_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.2.11. Managing device interrupt processing for guaranteed pod isolated CPUs</h3></div></div></div><p>
					The Node Tuning Operator can manage host CPUs by dividing them into reserved CPUs for cluster and operating system housekeeping duties, including pod infra containers, and isolated CPUs for application containers to run the workloads. This allows you to set CPUs for low latency workloads as isolated.
				</p><p>
					Device interrupts are load balanced between all isolated and reserved CPUs to avoid CPUs being overloaded, with the exception of CPUs where there is a guaranteed pod running. Guaranteed pod CPUs are prevented from processing device interrupts when the relevant annotations are set for the pod.
				</p><p>
					In the performance profile, <code class="literal">globallyDisableIrqLoadBalancing</code> is used to manage whether device interrupts are processed or not. For certain workloads, the reserved CPUs are not always sufficient for dealing with device interrupts, and for this reason, device interrupts are not globally disabled on the isolated CPUs. By default, Node Tuning Operator does not disable device interrupts on isolated CPUs.
				</p><p>
					To achieve low latency for workloads, some (but not all) pods require the CPUs they are running on to not process device interrupts. A pod annotation, <code class="literal">irq-load-balancing.crio.io</code>, is used to define whether device interrupts are processed or not. When configured, CRI-O disables device interrupts only as long as the pod is running.
				</p><section class="section" id="disabling-cpu-cfs-quota_cnf-master"><div class="titlepage"><div><div><h4 class="title">10.2.11.1. Disabling CPU CFS quota</h4></div></div></div><p>
						To reduce CPU throttling for individual guaranteed pods, create a pod specification with the annotation <code class="literal">cpu-quota.crio.io: "disable"</code>. This annotation disables the CPU completely fair scheduler (CFS) quota at the pod run time. The following pod specification contains this annotation:
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  annotations:
      cpu-quota.crio.io: "disable"
spec:
    runtimeClassName: performance-&lt;profile_name&gt;
...</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Only disable CPU CFS quota when the CPU manager static policy is enabled and for pods with guaranteed QoS that use whole CPUs. Otherwise, disabling CPU CFS quota can affect the performance of other containers in the cluster.
						</p></div></div></section><section class="section" id="configuring-global-device-interrupts-handling-for-isolated-cpus_cnf-master"><div class="titlepage"><div><div><h4 class="title">10.2.11.2. Disabling global device interrupts handling in Node Tuning Operator</h4></div></div></div><p>
						To configure Node Tuning Operator to disable global device interrupts for the isolated CPU set, set the <code class="literal">globallyDisableIrqLoadBalancing</code> field in the performance profile to <code class="literal">true</code>. When <code class="literal">true</code>, conflicting pod annotations are ignored. When <code class="literal">false</code>, IRQ loads are balanced across all CPUs.
					</p><p>
						A performance profile snippet illustrates this setting:
					</p><pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: manual
spec:
  globallyDisableIrqLoadBalancing: true
...</pre></section><section class="section" id="disabling_interrupt_processing_for_individual_pods_cnf-master"><div class="titlepage"><div><div><h4 class="title">10.2.11.3. Disabling interrupt processing for individual pods</h4></div></div></div><p>
						To disable interrupt processing for individual pods, ensure that <code class="literal">globallyDisableIrqLoadBalancing</code> is set to <code class="literal">false</code> in the performance profile. Then, in the pod specification, set the <code class="literal">irq-load-balancing.crio.io</code> pod annotation to <code class="literal">disable</code>. The following pod specification contains this annotation:
					</p><pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
kind: Pod
metadata:
  annotations:
      irq-load-balancing.crio.io: "disable"
spec:
    runtimeClassName: performance-&lt;profile_name&gt;
...</pre></section></section><section class="section" id="use-device-interrupt-processing-for-isolated-cpus_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.2.12. Upgrading the performance profile to use device interrupt processing</h3></div></div></div><p>
					When you upgrade the Node Tuning Operator performance profile custom resource definition (CRD) from v1 or v1alpha1 to v2, <code class="literal">globallyDisableIrqLoadBalancing</code> is set to <code class="literal">true</code> on existing profiles.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						<code class="literal">globallyDisableIrqLoadBalancing</code> toggles whether IRQ load balancing will be disabled for the Isolated CPU set. When the option is set to <code class="literal">true</code> it disables IRQ load balancing for the Isolated CPU set. Setting the option to <code class="literal">false</code> allows the IRQs to be balanced across all CPUs.
					</p></div></div><section class="section" id="nto_supported_api_versions_cnf-master"><div class="titlepage"><div><div><h4 class="title">10.2.12.1. Supported API Versions</h4></div></div></div><p>
						The Node Tuning Operator supports <code class="literal">v2</code>, <code class="literal">v1</code>, and <code class="literal">v1alpha1</code> for the performance profile <code class="literal">apiVersion</code> field. The v1 and v1alpha1 APIs are identical. The v2 API includes an optional boolean field <code class="literal">globallyDisableIrqLoadBalancing</code> with a default value of <code class="literal">false</code>.
					</p><section class="section" id="upgrading_nto_api_from_v1alpha1_to_v1_cnf-master"><div class="titlepage"><div><div><h5 class="title">10.2.12.1.1. Upgrading Node Tuning Operator API from v1alpha1 to v1</h5></div></div></div><p>
							When upgrading Node Tuning Operator API version from v1alpha1 to v1, the v1alpha1 performance profiles are converted on-the-fly using a "None" Conversion strategy and served to the Node Tuning Operator with API version v1.
						</p></section><section class="section" id="upgrading_nto_api_from_v1alpha1_to_v1_or_v2_cnf-master"><div class="titlepage"><div><div><h5 class="title">10.2.12.1.2. Upgrading Node Tuning Operator API from v1alpha1 or v1 to v2</h5></div></div></div><p>
							When upgrading from an older Node Tuning Operator API version, the existing v1 and v1alpha1 performance profiles are converted using a conversion webhook that injects the <code class="literal">globallyDisableIrqLoadBalancing</code> field with a value of <code class="literal">true</code>.
						</p></section></section></section></section><section class="section" id="cnf-tuning-nodes-for-low-latency-via-performanceprofile_cnf-master"><div class="titlepage"><div><div><h2 class="title">10.3. Tuning nodes for low latency with the performance profile</h2></div></div></div><p>
				The performance profile lets you control latency tuning aspects of nodes that belong to a certain machine config pool. After you specify your settings, the <code class="literal">PerformanceProfile</code> object is compiled into multiple objects that perform the actual node level tuning:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						A <code class="literal">MachineConfig</code> file that manipulates the nodes.
					</li><li class="listitem">
						A <code class="literal">KubeletConfig</code> file that configures the Topology Manager, the CPU Manager, and the OpenShift Container Platform nodes.
					</li><li class="listitem">
						The Tuned profile that configures the Node Tuning Operator.
					</li></ul></div><p>
				You can use a performance profile to specify whether to update the kernel to kernel-rt, to allocate huge pages, and to partition the CPUs for performing housekeeping duties or running workloads.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					You can manually create the <code class="literal">PerformanceProfile</code> object or use the Performance Profile Creator (PPC) to generate a performance profile. See the additional resources below for more information on the PPC.
				</p></div></div><div class="formalpara"><p class="title"><strong>Sample performance profile</strong></p><p>
					
<pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
 name: performance
spec:
 cpu:
  isolated: "4-15" <span id="CO27-1"><!--Empty--></span><span class="callout">1</span>
  reserved: "0-3" <span id="CO27-2"><!--Empty--></span><span class="callout">2</span>
 hugepages:
  defaultHugepagesSize: "1G"
  pages:
  - size: "1G"
    count: 16
    node: 0
 realTimeKernel:
  enabled: true  <span id="CO27-3"><!--Empty--></span><span class="callout">3</span>
 numa:  <span id="CO27-4"><!--Empty--></span><span class="callout">4</span>
  topologyPolicy: "best-effort"
 nodeSelector:
  node-role.kubernetes.io/worker-cnf: "" <span id="CO27-5"><!--Empty--></span><span class="callout">5</span></pre>

				</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO27-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						Use this field to isolate specific CPUs to use with application containers for workloads. Set an even number of isolated CPUs to enable the pods to run without errors when hyperthreading is enabled.
					</div></dd><dt><a href="#CO27-2"><span class="callout">2</span></a> </dt><dd><div class="para">
						Use this field to reserve specific CPUs to use with infra containers for housekeeping.
					</div></dd><dt><a href="#CO27-3"><span class="callout">3</span></a> </dt><dd><div class="para">
						Use this field to install the real-time kernel on the node. Valid values are <code class="literal">true</code> or <code class="literal">false</code>. Setting the <code class="literal">true</code> value installs the real-time kernel.
					</div></dd><dt><a href="#CO27-4"><span class="callout">4</span></a> </dt><dd><div class="para">
						Use this field to configure the topology manager policy. Valid values are <code class="literal">none</code> (default), <code class="literal">best-effort</code>, <code class="literal">restricted</code>, and <code class="literal">single-numa-node</code>. For more information, see <a class="link" href="https://kubernetes.io/docs/tasks/administer-cluster/topology-manager/#topology-manager-policies">Topology Manager Policies</a>.
					</div></dd><dt><a href="#CO27-5"><span class="callout">5</span></a> </dt><dd><div class="para">
						Use this field to specify a node selector to apply the performance profile to specific nodes.
					</div></dd></dl></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						For information on using the Performance Profile Creator (PPC) to generate a performance profile, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#cnf-create-performance-profiles">Creating a performance profile</a>.
					</li></ul></div><section class="section" id="cnf-configuring-huge-pages_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.3.1. Configuring huge pages</h3></div></div></div><p>
					Nodes must pre-allocate huge pages used in an OpenShift Container Platform cluster. Use the Node Tuning Operator to allocate huge pages on a specific node.
				</p><p>
					OpenShift Container Platform provides a method for creating and allocating huge pages. Node Tuning Operator provides an easier method for doing this using the performance profile.
				</p><p>
					For example, in the <code class="literal">hugepages</code> <code class="literal">pages</code> section of the performance profile, you can specify multiple blocks of <code class="literal">size</code>, <code class="literal">count</code>, and, optionally, <code class="literal">node</code>:
				</p><pre class="programlisting language-yaml">hugepages:
   defaultHugepagesSize: "1G"
   pages:
   - size:  "1G"
     count:  4
     node:  0 <span id="CO28-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO28-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							<code class="literal">node</code> is the NUMA node in which the huge pages are allocated. If you omit <code class="literal">node</code>, the pages are evenly spread across all NUMA nodes.
						</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Wait for the relevant machine config pool status that indicates the update is finished.
					</p></div></div><p>
					These are the only configuration steps you need to do to allocate huge pages.
				</p><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To verify the configuration, see the <code class="literal">/proc/meminfo</code> file on the node:
						</p><pre class="programlisting language-terminal">$ oc debug node/ip-10-0-141-105.ec2.internal</pre><pre class="programlisting language-terminal"># grep -i huge /proc/meminfo</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">AnonHugePages:    ###### ##
ShmemHugePages:        0 kB
HugePages_Total:       2
HugePages_Free:        2
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       #### ##
Hugetlb:            #### ##</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Use <code class="literal">oc describe</code> to report the new size:
						</p><pre class="programlisting language-terminal">$ oc describe node worker-0.ocp4poc.example.com | grep -i huge</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">                                   hugepages-1g=true
 hugepages-###:  ###
 hugepages-###:  ###</pre>

							</p></div></li></ul></div></section><section class="section" id="cnf-allocating-multiple-huge-page-sizes_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.3.2. Allocating multiple huge page sizes</h3></div></div></div><p>
					You can request huge pages with different sizes under the same container. This allows you to define more complicated pods consisting of containers with different huge page size needs.
				</p><p>
					For example, you can define sizes <code class="literal">1G</code> and <code class="literal">2M</code> and the Node Tuning Operator will configure both sizes on the node, as shown here:
				</p><pre class="programlisting language-yaml">spec:
  hugepages:
    defaultHugepagesSize: 1G
    pages:
    - count: 1024
      node: 0
      size: 2M
    - count: 4
      node: 1
      size: 1G</pre></section><section class="section" id="configuring_for_irq_dynamic_load_balancing_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.3.3. Configuring a node for IRQ dynamic load balancing</h3></div></div></div><p>
					Configure a cluster node for IRQ dynamic load balancing to control which cores can receive device interrupt requests (IRQ).
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							For core isolation, all server hardware components must support IRQ affinity. To check if the hardware components of your server support IRQ affinity, view the server’s hardware specifications or contact your hardware provider.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to the OpenShift Container Platform cluster as a user with cluster-admin privileges.
						</li><li class="listitem">
							Set the performance profile <code class="literal">apiVersion</code> to use <code class="literal">performance.openshift.io/v2</code>.
						</li><li class="listitem">
							Remove the <code class="literal">globallyDisableIrqLoadBalancing</code> field or set it to <code class="literal">false</code>.
						</li><li class="listitem"><p class="simpara">
							Set the appropriate isolated and reserved CPUs. The following snippet illustrates a profile that reserves 2 CPUs. IRQ load-balancing is enabled for pods running on the <code class="literal">isolated</code> CPU set:
						</p><pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: dynamic-irq-profile
spec:
  cpu:
    isolated: 2-5
    reserved: 0-1
...</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								When you configure reserved and isolated CPUs, the infra containers in pods use the reserved CPUs and the application containers use the isolated CPUs.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Create the pod that uses exclusive CPUs, and set <code class="literal">irq-load-balancing.crio.io</code> and <code class="literal">cpu-quota.crio.io</code> annotations to <code class="literal">disable</code>. For example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: dynamic-irq-pod
  annotations:
     irq-load-balancing.crio.io: "disable"
     cpu-quota.crio.io: "disable"
spec:
  containers:
  - name: dynamic-irq-pod
    image: "registry.redhat.io/openshift4/cnf-tests-rhel8:v4.13"
    command: ["sleep", "10h"]
    resources:
      requests:
        cpu: 2
        memory: "200M"
      limits:
        cpu: 2
        memory: "200M"
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""
  runtimeClassName: performance-dynamic-irq-profile
...</pre></li><li class="listitem">
							Enter the pod <code class="literal">runtimeClassName</code> in the form performance-&lt;profile_name&gt;, where &lt;profile_name&gt; is the <code class="literal">name</code> from the <code class="literal">PerformanceProfile</code> YAML, in this example, <code class="literal">performance-dynamic-irq-profile</code>.
						</li><li class="listitem">
							Set the node selector to target a cnf-worker.
						</li><li class="listitem"><p class="simpara">
							Ensure the pod is running correctly. Status should be <code class="literal">running</code>, and the correct cnf-worker node should be set:
						</p><pre class="programlisting language-terminal">$ oc get pod -o wide</pre><div class="formalpara"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME              READY   STATUS    RESTARTS   AGE     IP             NODE          NOMINATED NODE   READINESS GATES
dynamic-irq-pod   1/1     Running   0          5h33m   &lt;ip-address&gt;   &lt;node-name&gt;   &lt;none&gt;           &lt;none&gt;</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Get the CPUs that the pod configured for IRQ dynamic load balancing runs on:
						</p><pre class="programlisting language-terminal">$ oc exec -it dynamic-irq-pod -- /bin/bash -c "grep Cpus_allowed_list /proc/self/status | awk '{print $2}'"</pre><div class="formalpara"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">Cpus_allowed_list:  2-3</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Ensure the node configuration is applied correctly. Log in to the node to verify the configuration.
						</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node-name&gt;</pre><div class="formalpara"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">Starting pod/&lt;node-name&gt;-debug ...
To use host binaries, run `chroot /host`

Pod IP: &lt;ip-address&gt;
If you don't see a command prompt, try pressing enter.

sh-4.4#</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Verify that you can use the node file system:
						</p><pre class="programlisting language-terminal">sh-4.4# chroot /host</pre><div class="formalpara"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">sh-4.4#</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Ensure the default system CPU affinity mask does not include the <code class="literal">dynamic-irq-pod</code> CPUs, for example, CPUs 2 and 3.
						</p><pre class="programlisting language-terminal">$ cat /proc/irq/default_smp_affinity</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">33</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Ensure the system IRQs are not configured to run on the <code class="literal">dynamic-irq-pod</code> CPUs:
						</p><pre class="programlisting language-terminal">find /proc/irq/ -name smp_affinity_list -exec sh -c 'i="$1"; mask=$(cat $i); file=$(echo $i); echo $file: $mask' _ {} \;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">/proc/irq/0/smp_affinity_list: 0-5
/proc/irq/1/smp_affinity_list: 5
/proc/irq/2/smp_affinity_list: 0-5
/proc/irq/3/smp_affinity_list: 0-5
/proc/irq/4/smp_affinity_list: 0
/proc/irq/5/smp_affinity_list: 0-5
/proc/irq/6/smp_affinity_list: 0-5
/proc/irq/7/smp_affinity_list: 0-5
/proc/irq/8/smp_affinity_list: 4
/proc/irq/9/smp_affinity_list: 4
/proc/irq/10/smp_affinity_list: 0-5
/proc/irq/11/smp_affinity_list: 0
/proc/irq/12/smp_affinity_list: 1
/proc/irq/13/smp_affinity_list: 0-5
/proc/irq/14/smp_affinity_list: 1
/proc/irq/15/smp_affinity_list: 0
/proc/irq/24/smp_affinity_list: 1
/proc/irq/25/smp_affinity_list: 1
/proc/irq/26/smp_affinity_list: 1
/proc/irq/27/smp_affinity_list: 5
/proc/irq/28/smp_affinity_list: 1
/proc/irq/29/smp_affinity_list: 0
/proc/irq/30/smp_affinity_list: 0-5</pre>

							</p></div></li></ol></div></section><section class="section" id="about_irq_affinity_setting_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.3.4. About support of IRQ affinity setting</h3></div></div></div><p>
					Some IRQ controllers lack support for IRQ affinity setting and will always expose all online CPUs as the IRQ mask. These IRQ controllers effectively run on CPU 0.
				</p><p>
					The following are examples of drivers and hardware that Red Hat are aware lack support for IRQ affinity setting. The list is, by no means, exhaustive:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Some RAID controller drivers, such as <code class="literal">megaraid_sas</code>
						</li><li class="listitem">
							Many non-volatile memory express (NVMe) drivers
						</li><li class="listitem">
							Some LAN on motherboard (LOM) network controllers
						</li><li class="listitem">
							The driver uses <code class="literal">managed_irqs</code>
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The reason they do not support IRQ affinity setting might be associated with factors such as the type of processor, the IRQ controller, or the circuitry connections in the motherboard.
					</p></div></div><p>
					If the effective affinity of any IRQ is set to an isolated CPU, it might be a sign of some hardware or driver not supporting IRQ affinity setting. To find the effective affinity, log in to the host and run the following command:
				</p><pre class="programlisting language-terminal">$ find /proc/irq/ -name effective_affinity -exec sh -c 'i="$1"; mask=$(cat $i); file=$(echo $i); echo $file: $mask' _ {} \;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">/proc/irq/0/effective_affinity: 1
/proc/irq/1/effective_affinity: 8
/proc/irq/2/effective_affinity: 0
/proc/irq/3/effective_affinity: 1
/proc/irq/4/effective_affinity: 2
/proc/irq/5/effective_affinity: 1
/proc/irq/6/effective_affinity: 1
/proc/irq/7/effective_affinity: 1
/proc/irq/8/effective_affinity: 1
/proc/irq/9/effective_affinity: 2
/proc/irq/10/effective_affinity: 1
/proc/irq/11/effective_affinity: 1
/proc/irq/12/effective_affinity: 4
/proc/irq/13/effective_affinity: 1
/proc/irq/14/effective_affinity: 1
/proc/irq/15/effective_affinity: 1
/proc/irq/24/effective_affinity: 2
/proc/irq/25/effective_affinity: 4
/proc/irq/26/effective_affinity: 2
/proc/irq/27/effective_affinity: 1
/proc/irq/28/effective_affinity: 8
/proc/irq/29/effective_affinity: 4
/proc/irq/30/effective_affinity: 4
/proc/irq/31/effective_affinity: 8
/proc/irq/32/effective_affinity: 8
/proc/irq/33/effective_affinity: 1
/proc/irq/34/effective_affinity: 2</pre>

					</p></div><p>
					Some drivers use <code class="literal">managed_irqs</code>, whose affinity is managed internally by the kernel and userspace cannot change the affinity. In some cases, these IRQs might be assigned to isolated CPUs. For more information about <code class="literal">managed_irqs</code>, see <a class="link" href="https://access.redhat.com/solutions/4819541">Affinity of managed interrupts cannot be changed even if they target isolated CPU</a>.
				</p></section><section class="section" id="configuring_hyperthreading_for_a_cluster_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.3.5. Configuring hyperthreading for a cluster</h3></div></div></div><p>
					To configure hyperthreading for an OpenShift Container Platform cluster, set the CPU threads in the performance profile to the same cores that are configured for the reserved or isolated CPU pools.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you configure a performance profile, and subsequently change the hyperthreading configuration for the host, ensure that you update the CPU <code class="literal">isolated</code> and <code class="literal">reserved</code> fields in the <code class="literal">PerformanceProfile</code> YAML to match the new configuration.
					</p></div></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Disabling a previously enabled host hyperthreading configuration can cause the CPU core IDs listed in the <code class="literal">PerformanceProfile</code> YAML to be incorrect. This incorrect configuration can cause the node to become unavailable because the listed CPUs can no longer be found.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Install the OpenShift CLI (oc).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Ascertain which threads are running on what CPUs for the host you want to configure.
						</p><p class="simpara">
							You can view which threads are running on the host CPUs by logging in to the cluster and running the following command:
						</p><pre class="programlisting language-terminal">$ lscpu --all --extended</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE MAXMHZ    MINMHZ
0   0    0      0    0:0:0:0       yes    4800.0000 400.0000
1   0    0      1    1:1:1:0       yes    4800.0000 400.0000
2   0    0      2    2:2:2:0       yes    4800.0000 400.0000
3   0    0      3    3:3:3:0       yes    4800.0000 400.0000
4   0    0      0    0:0:0:0       yes    4800.0000 400.0000
5   0    0      1    1:1:1:0       yes    4800.0000 400.0000
6   0    0      2    2:2:2:0       yes    4800.0000 400.0000
7   0    0      3    3:3:3:0       yes    4800.0000 400.0000</pre>

							</p></div><p class="simpara">
							In this example, there are eight logical CPU cores running on four physical CPU cores. CPU0 and CPU4 are running on physical Core0, CPU1 and CPU5 are running on physical Core 1, and so on.
						</p><p class="simpara">
							Alternatively, to view the threads that are set for a particular physical CPU core (<code class="literal">cpu0</code> in the example below), open a command prompt and run the following:
						</p><pre class="programlisting language-terminal">$ cat /sys/devices/system/cpu/cpu0/topology/thread_siblings_list</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">0-4</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Apply the isolated and reserved CPUs in the <code class="literal">PerformanceProfile</code> YAML. For example, you can set logical cores CPU0 and CPU4 as <code class="literal">isolated</code>, and logical cores CPU1 to CPU3 and CPU5 to CPU7 as <code class="literal">reserved</code>. When you configure reserved and isolated CPUs, the infra containers in pods use the reserved CPUs and the application containers use the isolated CPUs.
						</p><pre class="programlisting language-yaml">...
  cpu:
    isolated: 0,4
    reserved: 1-3,5-7
...</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The reserved and isolated CPU pools must not overlap and together must span all available cores in the worker node.
							</p></div></div></li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Hyperthreading is enabled by default on most Intel processors. If you enable hyperthreading, all threads processed by a particular core must be isolated or processed on the same core.
					</p></div></div><section class="section" id="disabling_hyperthreading_for_low_latency_applications_cnf-master"><div class="titlepage"><div><div><h4 class="title">10.3.5.1. Disabling hyperthreading for low latency applications</h4></div></div></div><p>
						When configuring clusters for low latency processing, consider whether you want to disable hyperthreading before you deploy the cluster. To disable hyperthreading, do the following:
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								Create a performance profile that is appropriate for your hardware and topology.
							</li><li class="listitem"><p class="simpara">
								Set <code class="literal">nosmt</code> as an additional kernel argument. The following example performance profile illustrates this setting:
							</p><pre class="programlisting language-yaml">﻿apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: example-performanceprofile
spec:
  additionalKernelArgs:
    - nmi_watchdog=0
    - audit=0
    - mce=off
    - processor.max_cstate=1
    - idle=poll
    - intel_idle.max_cstate=0
    - nosmt
  cpu:
    isolated: 2-3
    reserved: 0-1
  hugepages:
    defaultHugepagesSize: 1G
    pages:
      - count: 2
        node: 0
        size: 1G
  nodeSelector:
    node-role.kubernetes.io/performance: ''
  realTimeKernel:
    enabled: true</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									When you configure reserved and isolated CPUs, the infra containers in pods use the reserved CPUs and the application containers use the isolated CPUs.
								</p></div></div></li></ol></div></section></section><section class="section" id="cnf-understanding-workload-hints_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.3.6. Understanding workload hints</h3></div></div></div><p>
					The following table describes how combinations of power consumption and real-time settings impact on latency.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The following workload hints can be configured manually. You can also work with workload hints using the Performance Profile Creator. For more information about the performance profile, see the "Creating a performance profile" section. If the workload hint is configured manually and the <code class="literal">realTime</code> workload hint is not explicitly set then it defaults to <code class="literal">true</code>.
					</p></div></div><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735340381344" scope="col">Performance Profile creator setting</th><th align="left" valign="top" id="idm139735340380288" scope="col">Hint</th><th align="left" valign="top" id="idm139735351734432" scope="col">Environment</th><th align="left" valign="top" id="idm139735351733344" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735340381344"> <p>
									Default
								</p>
								 </td><td align="left" valign="top" headers="idm139735340380288">
<pre class="programlisting language-terminal">workloadHints:
highPowerConsumption: false
realTime: false</pre>
								 </td><td align="left" valign="top" headers="idm139735351734432"> <p>
									High throughput cluster without latency requirements
								</p>
								 </td><td align="left" valign="top" headers="idm139735351733344"> <p>
									Performance achieved through CPU partitioning only.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735340381344"> <p>
									Low-latency
								</p>
								 </td><td align="left" valign="top" headers="idm139735340380288">
<pre class="programlisting language-terminal">workloadHints:
highPowerConsumption: false
realTime: true</pre>
								 </td><td align="left" valign="top" headers="idm139735351734432"> <p>
									Regional datacenters
								</p>
								 </td><td align="left" valign="top" headers="idm139735351733344"> <p>
									Both energy savings and low-latency are desirable: compromise between power management, latency and throughput.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735340381344"> <p>
									Ultra-low-latency
								</p>
								 </td><td align="left" valign="top" headers="idm139735340380288">
<pre class="programlisting language-terminal">workloadHints:
highPowerConsumption: true
realTime: true</pre>
								 </td><td align="left" valign="top" headers="idm139735351734432"> <p>
									Far edge clusters, latency critical workloads
								</p>
								 </td><td align="left" valign="top" headers="idm139735351733344"> <p>
									Optimized for absolute minimal latency and maximum determinism at the cost of increased power consumption.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735340381344"> <p>
									Per-pod power management
								</p>
								 </td><td align="left" valign="top" headers="idm139735340380288">
<pre class="programlisting language-terminal">workloadHints:
realTime: true
highPowerConsumption: false
perPodPowerManagement: true</pre>
								 </td><td align="left" valign="top" headers="idm139735351734432"> <p>
									Critical and non-critical workloads
								</p>
								 </td><td align="left" valign="top" headers="idm139735351733344"> <p>
									Allows for power management per pod.
								</p>
								 </td></tr></tbody></table></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For information about using the Performance Profile Creator (PPC) to generate a performance profile, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#cnf-create-performance-profiles">Creating a performance profile</a>.
						</li></ul></div></section><section class="section" id="configuring-workload-hints_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.3.7. Configuring workload hints manually</h3></div></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Create a <code class="literal">PerformanceProfile</code> appropriate for the environment’s hardware and topology as described in the table in "Understanding workload hints". Adjust the profile to match the expected workload. In this example, we tune for the lowest possible latency.
						</li><li class="listitem"><p class="simpara">
							Add the <code class="literal">highPowerConsumption</code> and <code class="literal">realTime</code> workload hints. Both are set to <code class="literal">true</code> here.
						</p><pre class="programlisting language-yaml">    apiVersion: performance.openshift.io/v2
    kind: PerformanceProfile
    metadata:
      name: workload-hints
    spec:
      ...
      workloadHints:
        highPowerConsumption: true <span id="CO29-1"><!--Empty--></span><span class="callout">1</span>
        realTime: true <span id="CO29-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO29-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									If <code class="literal">highPowerConsumption</code> is <code class="literal">true</code>, the node is tuned for very low latency at the cost of increased power consumption.
								</div></dd><dt><a href="#CO29-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Disables some debugging and monitoring features that can affect system latency.
								</div></dd></dl></div></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						When the <code class="literal">realTime</code> workload hint flag is set to <code class="literal">true</code> in a performance profile, add the <code class="literal">cpu-quota.crio.io: disable</code> annotation to every guaranteed pod with pinned CPUs. This annotation is necessary to prevent the degradation of the process performance within the pod. If the <code class="literal">realTime</code> workload hint is not explicitly set then it defaults to <code class="literal">true</code>.
					</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For information about reducing CPU throttling for individual guaranteed pods, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#disabling-cpu-cfs-quota_cnf-master">Disabling CPU CFS quota</a>.
						</li></ul></div></section><section class="section" id="cnf-cpu-infra-container_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.3.8. Restricting CPUs for infra and application containers</h3></div></div></div><p>
					Generic housekeeping and workload tasks use CPUs in a way that may impact latency-sensitive processes. By default, the container runtime uses all online CPUs to run all containers together, which can result in context switches and spikes in latency. Partitioning the CPUs prevents noisy processes from interfering with latency-sensitive processes by separating them from each other. The following table describes how processes run on a CPU after you have tuned the node using the Node Tuning Operator:
				</p><div class="table" id="idm139735337005136"><p class="title"><strong>Table 10.2. Process' CPU assignments</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735343384304" scope="col">Process type</th><th align="left" valign="top" id="idm139735343383216" scope="col">Details</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735343384304"> <p>
									<code class="literal">Burstable</code> and <code class="literal">BestEffort</code> pods
								</p>
								 </td><td align="left" valign="top" headers="idm139735343383216"> <p>
									Runs on any CPU except where low latency workload is running
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735343384304"> <p>
									Infrastructure pods
								</p>
								 </td><td align="left" valign="top" headers="idm139735343383216"> <p>
									Runs on any CPU except where low latency workload is running
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735343384304"> <p>
									Interrupts
								</p>
								 </td><td align="left" valign="top" headers="idm139735343383216"> <p>
									Redirects to reserved CPUs (optional in OpenShift Container Platform 4.7 and later)
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735343384304"> <p>
									Kernel processes
								</p>
								 </td><td align="left" valign="top" headers="idm139735343383216"> <p>
									Pins to reserved CPUs
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735343384304"> <p>
									Latency-sensitive workload pods
								</p>
								 </td><td align="left" valign="top" headers="idm139735343383216"> <p>
									Pins to a specific set of exclusive CPUs from the isolated pool
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735343384304"> <p>
									OS processes/systemd services
								</p>
								 </td><td align="left" valign="top" headers="idm139735343383216"> <p>
									Pins to reserved CPUs
								</p>
								 </td></tr></tbody></table></div></div><p>
					The allocatable capacity of cores on a node for pods of all QoS process types, <code class="literal">Burstable</code>, <code class="literal">BestEffort</code>, or <code class="literal">Guaranteed</code>, is equal to the capacity of the isolated pool. The capacity of the reserved pool is removed from the node’s total core capacity for use by the cluster and operating system housekeeping duties.
				</p><div class="formalpara"><p class="title"><strong>Example 1</strong></p><p>
						A node features a capacity of 100 cores. Using a performance profile, the cluster administrator allocates 50 cores to the isolated pool and 50 cores to the reserved pool. The cluster administrator assigns 25 cores to QoS <code class="literal">Guaranteed</code> pods and 25 cores for <code class="literal">BestEffort</code> or <code class="literal">Burstable</code> pods. This matches the capacity of the isolated pool.
					</p></div><div class="formalpara"><p class="title"><strong>Example 2</strong></p><p>
						A node features a capacity of 100 cores. Using a performance profile, the cluster administrator allocates 50 cores to the isolated pool and 50 cores to the reserved pool. The cluster administrator assigns 50 cores to QoS <code class="literal">Guaranteed</code> pods and one core for <code class="literal">BestEffort</code> or <code class="literal">Burstable</code> pods. This exceeds the capacity of the isolated pool by one core. Pod scheduling fails because of insufficient CPU capacity.
					</p></div><p>
					The exact partitioning pattern to use depends on many factors like hardware, workload characteristics and the expected system load. Some sample use cases are as follows:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							If the latency-sensitive workload uses specific hardware, such as a network interface controller (NIC), ensure that the CPUs in the isolated pool are as close as possible to this hardware. At a minimum, you should place the workload in the same Non-Uniform Memory Access (NUMA) node.
						</li><li class="listitem">
							The reserved pool is used for handling all interrupts. When depending on system networking, allocate a sufficiently-sized reserve pool to handle all the incoming packet interrupts. In 4.13 and later versions, workloads can optionally be labeled as sensitive.
						</li></ul></div><p>
					The decision regarding which specific CPUs should be used for reserved and isolated partitions requires detailed analysis and measurements. Factors like NUMA affinity of devices and memory play a role. The selection also depends on the workload architecture and the specific use case.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The reserved and isolated CPU pools must not overlap and together must span all available cores in the worker node.
					</p></div></div><p>
					To ensure that housekeeping tasks and workloads do not interfere with each other, specify two groups of CPUs in the <code class="literal">spec</code> section of the performance profile.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">isolated</code> - Specifies the CPUs for the application container workloads. These CPUs have the lowest latency. Processes in this group have no interruptions and can, for example, reach much higher DPDK zero packet loss bandwidth.
						</li><li class="listitem">
							<code class="literal">reserved</code> - Specifies the CPUs for the cluster and operating system housekeeping duties. Threads in the <code class="literal">reserved</code> group are often busy. Do not run latency-sensitive applications in the <code class="literal">reserved</code> group. Latency-sensitive applications run in the <code class="literal">isolated</code> group.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Create a performance profile appropriate for the environment’s hardware and topology.
						</li><li class="listitem"><p class="simpara">
							Add the <code class="literal">reserved</code> and <code class="literal">isolated</code> parameters with the CPUs you want reserved and isolated for the infra and application containers:
						</p><pre class="programlisting language-yaml">﻿apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: infra-cpus
spec:
  cpu:
    reserved: "0-4,9" <span id="CO30-1"><!--Empty--></span><span class="callout">1</span>
    isolated: "5-8" <span id="CO30-2"><!--Empty--></span><span class="callout">2</span>
  nodeSelector: <span id="CO30-3"><!--Empty--></span><span class="callout">3</span>
    node-role.kubernetes.io/worker: ""</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO30-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify which CPUs are for infra containers to perform cluster and operating system housekeeping duties.
								</div></dd><dt><a href="#CO30-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify which CPUs are for application containers to run workloads.
								</div></dd><dt><a href="#CO30-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Optional: Specify a node selector to apply the performance profile to specific nodes.
								</div></dd></dl></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="#managing-device-interrupt-processing-for-guaranteed-pod-isolated-cpus_cnf-master" title="10.2.11. Managing device interrupt processing for guaranteed pod isolated CPUs">Managing device interrupt processing for guaranteed pod isolated CPUs</a>
						</li><li class="listitem">
							<a class="link" href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed">Create a pod that gets assigned a QoS class of Guaranteed</a>
						</li></ul></div></section></section><section class="section" id="reducing-nic-queues-using-the-node-tuning-operator_cnf-master"><div class="titlepage"><div><div><h2 class="title">10.4. Reducing NIC queues using the Node Tuning Operator</h2></div></div></div><p>
				The Node Tuning Operator allows you to adjust the network interface controller (NIC) queue count for each network device by configuring the performance profile. Device network queues allows the distribution of packets among different physical queues and each queue gets a separate thread for packet processing.
			</p><p>
				In real-time or low latency systems, all the unnecessary interrupt request lines (IRQs) pinned to the isolated CPUs must be moved to reserved or housekeeping CPUs.
			</p><p>
				In deployments with applications that require system, OpenShift Container Platform networking or in mixed deployments with Data Plane Development Kit (DPDK) workloads, multiple queues are needed to achieve good throughput and the number of NIC queues should be adjusted or remain unchanged. For example, to achieve low latency the number of NIC queues for DPDK based workloads should be reduced to just the number of reserved or housekeeping CPUs.
			</p><p>
				Too many queues are created by default for each CPU and these do not fit into the interrupt tables for housekeeping CPUs when tuning for low latency. Reducing the number of queues makes proper tuning possible. Smaller number of queues means a smaller number of interrupts that then fit in the IRQ table.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					In earlier versions of OpenShift Container Platform, the Performance Addon Operator provided automatic, low latency performance tuning for applications. In OpenShift Container Platform 4.11 and later, this functionality is part of the Node Tuning Operator.
				</p></div></div><section class="section" id="adjusting-nic-queues-with-the-performance-profile_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.4.1. Adjusting the NIC queues with the performance profile</h3></div></div></div><p>
					The performance profile lets you adjust the queue count for each network device.
				</p><p>
					Supported network devices:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Non-virtual network devices
						</li><li class="listitem">
							Network devices that support multiple queues (channels)
						</li></ul></div><p>
					Unsupported network devices:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Pure software network interfaces
						</li><li class="listitem">
							Block devices
						</li><li class="listitem">
							Intel DPDK virtual functions
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to the OpenShift Container Platform cluster running the Node Tuning Operator as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							Create and apply a performance profile appropriate for your hardware and topology. For guidance on creating a profile, see the "Creating a performance profile" section.
						</li><li class="listitem"><p class="simpara">
							Edit this created performance profile:
						</p><pre class="programlisting language-terminal">$ oc edit -f &lt;your_profile_name&gt;.yaml</pre></li><li class="listitem"><p class="simpara">
							Populate the <code class="literal">spec</code> field with the <code class="literal">net</code> object. The object list can contain two fields:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">userLevelNetworking</code> is a required field specified as a boolean flag. If <code class="literal">userLevelNetworking</code> is <code class="literal">true</code>, the queue count is set to the reserved CPU count for all supported devices. The default is <code class="literal">false</code>.
								</li><li class="listitem"><p class="simpara">
									<code class="literal">devices</code> is an optional field specifying a list of devices that will have the queues set to the reserved CPU count. If the device list is empty, the configuration applies to all network devices. The configuration is as follows:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p class="simpara">
											<code class="literal">interfaceName</code>: This field specifies the interface name, and it supports shell-style wildcards, which can be positive or negative.
										</p><div class="itemizedlist"><ul class="itemizedlist" type="square"><li class="listitem">
													Example wildcard syntax is as follows: <code class="literal">&lt;string&gt; .*</code>
												</li><li class="listitem">
													Negative rules are prefixed with an exclamation mark. To apply the net queue changes to all devices other than the excluded list, use <code class="literal">!&lt;device&gt;</code>, for example, <code class="literal">!eno1</code>.
												</li></ul></div></li><li class="listitem">
											<code class="literal">vendorID</code>: The network device vendor ID represented as a 16-bit hexadecimal number with a <code class="literal">0x</code> prefix.
										</li><li class="listitem"><p class="simpara">
											<code class="literal">deviceID</code>: The network device ID (model) represented as a 16-bit hexadecimal number with a <code class="literal">0x</code> prefix.
										</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												When a <code class="literal">deviceID</code> is specified, the <code class="literal">vendorID</code> must also be defined. A device that matches all of the device identifiers specified in a device entry <code class="literal">interfaceName</code>, <code class="literal">vendorID</code>, or a pair of <code class="literal">vendorID</code> plus <code class="literal">deviceID</code> qualifies as a network device. This network device then has its net queues count set to the reserved CPU count.
											</p><p>
												When two or more devices are specified, the net queues count is set to any net device that matches one of them.
											</p></div></div></li></ul></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Set the queue count to the reserved CPU count for all devices by using this example performance profile:
						</p><pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: manual
spec:
  cpu:
    isolated: 3-51,54-103
    reserved: 0-2,52-54
  net:
    userLevelNetworking: true
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""</pre></li><li class="listitem"><p class="simpara">
							Set the queue count to the reserved CPU count for all devices matching any of the defined device identifiers by using this example performance profile:
						</p><pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: manual
spec:
  cpu:
    isolated: 3-51,54-103
    reserved: 0-2,52-54
  net:
    userLevelNetworking: true
    devices:
    - interfaceName: “eth0”
    - interfaceName: “eth1”
    - vendorID: “0x1af4”
    - deviceID: “0x1000”
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""</pre></li><li class="listitem"><p class="simpara">
							Set the queue count to the reserved CPU count for all devices starting with the interface name <code class="literal">eth</code> by using this example performance profile:
						</p><pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: manual
spec:
  cpu:
    isolated: 3-51,54-103
    reserved: 0-2,52-54
  net:
    userLevelNetworking: true
    devices:
    - interfaceName: “eth*”
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""</pre></li><li class="listitem"><p class="simpara">
							Set the queue count to the reserved CPU count for all devices with an interface named anything other than <code class="literal">eno1</code> by using this example performance profile:
						</p><pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: manual
spec:
  cpu:
    isolated: 3-51,54-103
    reserved: 0-2,52-54
  net:
    userLevelNetworking: true
    devices:
    - interfaceName: “!eno1”
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""</pre></li><li class="listitem"><p class="simpara">
							Set the queue count to the reserved CPU count for all devices that have an interface name <code class="literal">eth0</code>, <code class="literal">vendorID</code> of <code class="literal">0x1af4</code>, and <code class="literal">deviceID</code> of <code class="literal">0x1000</code> by using this example performance profile:
						</p><pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: manual
spec:
  cpu:
    isolated: 3-51,54-103
    reserved: 0-2,52-54
  net:
    userLevelNetworking: true
    devices:
    - interfaceName: “eth0”
    - vendorID: “0x1af4”
    - deviceID: “0x1000”
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""</pre></li><li class="listitem"><p class="simpara">
							Apply the updated performance profile:
						</p><pre class="programlisting language-terminal">$ oc apply -f &lt;your_profile_name&gt;.yaml</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#cnf-create-performance-profiles">Creating a performance profile</a>.
						</li></ul></div></section><section class="section" id="verifying-queue-status_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.4.2. Verifying the queue status</h3></div></div></div><p>
					In this section, a number of examples illustrate different performance profiles and how to verify the changes are applied.
				</p><div class="formalpara"><p class="title"><strong>Example 1</strong></p><p>
						In this example, the net queue count is set to the reserved CPU count (2) for <span class="emphasis"><em>all</em></span> supported devices.
					</p></div><p>
					The relevant section from the performance profile is:
				</p><pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
metadata:
  name: performance
spec:
  kind: PerformanceProfile
  spec:
    cpu:
      reserved: 0-1  #total = 2
      isolated: 2-8
    net:
      userLevelNetworking: true
# ...</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Display the status of the queues associated with a device using the following command:
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Run this command on the node where the performance profile was applied.
							</p></div></div><pre class="programlisting language-terminal">$ ethtool -l &lt;device&gt;</pre></li><li class="listitem"><p class="simpara">
							Verify the queue status before the profile is applied:
						</p><pre class="programlisting language-terminal">$ ethtool -l ens4</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Channel parameters for ens4:
Pre-set maximums:
RX:         0
TX:         0
Other:      0
Combined:   4
Current hardware settings:
RX:         0
TX:         0
Other:      0
Combined:   4</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Verify the queue status after the profile is applied:
						</p><pre class="programlisting language-terminal">$ ethtool -l ens4</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Channel parameters for ens4:
Pre-set maximums:
RX:         0
TX:         0
Other:      0
Combined:   4
Current hardware settings:
RX:         0
TX:         0
Other:      0
Combined:   2 <span id="CO31-1"><!--Empty--></span><span class="callout">1</span></pre>

							</p></div></li></ul></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO31-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The combined channel shows that the total count of reserved CPUs for <span class="emphasis"><em>all</em></span> supported devices is 2. This matches what is configured in the performance profile.
						</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example 2</strong></p><p>
						In this example, the net queue count is set to the reserved CPU count (2) for <span class="emphasis"><em>all</em></span> supported network devices with a specific <code class="literal">vendorID</code>.
					</p></div><p>
					The relevant section from the performance profile is:
				</p><pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
metadata:
  name: performance
spec:
  kind: PerformanceProfile
  spec:
    cpu:
      reserved: 0-1  #total = 2
      isolated: 2-8
    net:
      userLevelNetworking: true
      devices:
      - vendorID = 0x1af4
# ...</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Display the status of the queues associated with a device using the following command:
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Run this command on the node where the performance profile was applied.
							</p></div></div><pre class="programlisting language-terminal">$ ethtool -l &lt;device&gt;</pre></li><li class="listitem"><p class="simpara">
							Verify the queue status after the profile is applied:
						</p><pre class="programlisting language-terminal">$ ethtool -l ens4</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Channel parameters for ens4:
Pre-set maximums:
RX:         0
TX:         0
Other:      0
Combined:   4
Current hardware settings:
RX:         0
TX:         0
Other:      0
Combined:   2 <span id="CO32-1"><!--Empty--></span><span class="callout">1</span></pre>

							</p></div></li></ul></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO32-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The total count of reserved CPUs for all supported devices with <code class="literal">vendorID=0x1af4</code> is 2. For example, if there is another network device <code class="literal">ens2</code> with <code class="literal">vendorID=0x1af4</code> it will also have total net queues of 2. This matches what is configured in the performance profile.
						</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example 3</strong></p><p>
						In this example, the net queue count is set to the reserved CPU count (2) for <span class="emphasis"><em>all</em></span> supported network devices that match any of the defined device identifiers.
					</p></div><p>
					The command <code class="literal">udevadm info</code> provides a detailed report on a device. In this example the devices are:
				</p><pre class="programlisting language-terminal"># udevadm info -p /sys/class/net/ens4
...
E: ID_MODEL_ID=0x1000
E: ID_VENDOR_ID=0x1af4
E: INTERFACE=ens4
...</pre><pre class="programlisting language-terminal"># udevadm info -p /sys/class/net/eth0
...
E: ID_MODEL_ID=0x1002
E: ID_VENDOR_ID=0x1001
E: INTERFACE=eth0
...</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Set the net queues to 2 for a device with <code class="literal">interfaceName</code> equal to <code class="literal">eth0</code> and any devices that have a <code class="literal">vendorID=0x1af4</code> with the following performance profile:
						</p><pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
metadata:
  name: performance
spec:
  kind: PerformanceProfile
    spec:
      cpu:
        reserved: 0-1  #total = 2
        isolated: 2-8
      net:
        userLevelNetworking: true
        devices:
        - interfaceName = eth0
        - vendorID = 0x1af4
...</pre></li><li class="listitem"><p class="simpara">
							Verify the queue status after the profile is applied:
						</p><pre class="programlisting language-terminal">$ ethtool -l ens4</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Channel parameters for ens4:
Pre-set maximums:
RX:         0
TX:         0
Other:      0
Combined:   4
Current hardware settings:
RX:         0
TX:         0
Other:      0
Combined:   2 <span id="CO33-1"><!--Empty--></span><span class="callout">1</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO33-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The total count of reserved CPUs for all supported devices with <code class="literal">vendorID=0x1af4</code> is set to 2. For example, if there is another network device <code class="literal">ens2</code> with <code class="literal">vendorID=0x1af4</code>, it will also have the total net queues set to 2. Similarly, a device with <code class="literal">interfaceName</code> equal to <code class="literal">eth0</code> will have total net queues set to 2.
								</div></dd></dl></div></li></ul></div></section><section class="section" id="logging-associated-with-adjusting-nic-queues_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.4.3. Logging associated with adjusting NIC queues</h3></div></div></div><p>
					Log messages detailing the assigned devices are recorded in the respective Tuned daemon logs. The following messages might be recorded to the <code class="literal">/var/log/tuned/tuned.log</code> file:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							An <code class="literal">INFO</code> message is recorded detailing the successfully assigned devices:
						</p><pre class="programlisting language-terminal">INFO tuned.plugins.base: instance net_test (net): assigning devices ens1, ens2, ens3</pre></li><li class="listitem"><p class="simpara">
							A <code class="literal">WARNING</code> message is recorded if none of the devices can be assigned:
						</p><pre class="programlisting language-terminal">WARNING  tuned.plugins.base: instance net_test: no matching devices available</pre></li></ul></div></section></section><section class="section" id="cnf-debugging-low-latency-cnf-tuning-status_cnf-master"><div class="titlepage"><div><div><h2 class="title">10.5. Debugging low latency CNF tuning status</h2></div></div></div><p>
				The <code class="literal">PerformanceProfile</code> custom resource (CR) contains status fields for reporting tuning status and debugging latency degradation issues. These fields report on conditions that describe the state of the operator’s reconciliation functionality.
			</p><p>
				A typical issue can arise when the status of machine config pools that are attached to the performance profile are in a degraded state, causing the <code class="literal">PerformanceProfile</code> status to degrade. In this case, the machine config pool issues a failure message.
			</p><p>
				The Node Tuning Operator contains the <code class="literal">performanceProfile.spec.status.Conditions</code> status field:
			</p><pre class="programlisting language-bash">Status:
  Conditions:
    Last Heartbeat Time:   2020-06-02T10:01:24Z
    Last Transition Time:  2020-06-02T10:01:24Z
    Status:                True
    Type:                  Available
    Last Heartbeat Time:   2020-06-02T10:01:24Z
    Last Transition Time:  2020-06-02T10:01:24Z
    Status:                True
    Type:                  Upgradeable
    Last Heartbeat Time:   2020-06-02T10:01:24Z
    Last Transition Time:  2020-06-02T10:01:24Z
    Status:                False
    Type:                  Progressing
    Last Heartbeat Time:   2020-06-02T10:01:24Z
    Last Transition Time:  2020-06-02T10:01:24Z
    Status:                False
    Type:                  Degraded</pre><p>
				The <code class="literal">Status</code> field contains <code class="literal">Conditions</code> that specify <code class="literal">Type</code> values that indicate the status of the performance profile:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">Available</code></span></dt><dd>
							All machine configs and Tuned profiles have been created successfully and are available for cluster components are responsible to process them (NTO, MCO, Kubelet).
						</dd><dt><span class="term"><code class="literal">Upgradeable</code></span></dt><dd>
							Indicates whether the resources maintained by the Operator are in a state that is safe to upgrade.
						</dd><dt><span class="term"><code class="literal">Progressing</code></span></dt><dd>
							Indicates that the deployment process from the performance profile has started.
						</dd><dt><span class="term"><code class="literal">Degraded</code></span></dt><dd><p class="simpara">
							Indicates an error if:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Validation of the performance profile has failed.
								</li><li class="listitem">
									Creation of all relevant components did not complete successfully.
								</li></ul></div></dd></dl></div><p>
				Each of these types contain the following fields:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">Status</code></span></dt><dd>
							The state for the specific type (<code class="literal">true</code> or <code class="literal">false</code>).
						</dd><dt><span class="term"><code class="literal">Timestamp</code></span></dt><dd>
							The transaction timestamp.
						</dd><dt><span class="term"><code class="literal">Reason string</code></span></dt><dd>
							The machine readable reason.
						</dd><dt><span class="term"><code class="literal">Message string</code></span></dt><dd>
							The human readable reason describing the state and error details, if any.
						</dd></dl></div><section class="section" id="cnf-debugging-low-latency-cnf-tuning-status-machineconfigpools_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.5.1. Machine config pools</h3></div></div></div><p>
					A performance profile and its created products are applied to a node according to an associated machine config pool (MCP). The MCP holds valuable information about the progress of applying the machine configurations created by performance profiles that encompass kernel args, kube config, huge pages allocation, and deployment of rt-kernel. The Performance Profile controller monitors changes in the MCP and updates the performance profile status accordingly.
				</p><p>
					The only conditions returned by the MCP to the performance profile status is when the MCP is <code class="literal">Degraded</code>, which leads to <code class="literal">performanceProfile.status.condition.Degraded = true</code>.
				</p><div class="formalpara"><p class="title"><strong>Example</strong></p><p>
						The following example is for a performance profile with an associated machine config pool (<code class="literal">worker-cnf</code>) that was created for it:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							The associated machine config pool is in a degraded state:
						</p><pre class="programlisting language-terminal"># oc get mcp</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME         CONFIG                                                 UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master       rendered-master-2ee57a93fa6c9181b546ca46e1571d2d       True      False      False      3              3                   3                     0                      2d21h
worker       rendered-worker-d6b2bdc07d9f5a59a6b68950acf25e5f       True      False      False      2              2                   2                     0                      2d21h
worker-cnf   rendered-worker-cnf-6c838641b8a08fff08dbd8b02fb63f7c   False     True       True       2              1                   1                     1                      2d20h</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							The <code class="literal">describe</code> section of the MCP shows the reason:
						</p><pre class="programlisting language-terminal"># oc describe mcp worker-cnf</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">  Message:               Node node-worker-cnf is reporting: "prepping update:
  machineconfig.machineconfiguration.openshift.io \"rendered-worker-cnf-40b9996919c08e335f3ff230ce1d170\" not
  found"
    Reason:                1 nodes are reporting degraded status on sync</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							The degraded state should also appear under the performance profile <code class="literal">status</code> field marked as <code class="literal">degraded = true</code>:
						</p><pre class="programlisting language-terminal"># oc describe performanceprofiles performance</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Message: Machine config pool worker-cnf Degraded Reason: 1 nodes are reporting degraded status on sync.
Machine config pool worker-cnf Degraded Message: Node yquinn-q8s5v-w-b-z5lqn.c.openshift-gce-devel.internal is
reporting: "prepping update: machineconfig.machineconfiguration.openshift.io
\"rendered-worker-cnf-40b9996919c08e335f3ff230ce1d170\" not found".    Reason:  MCPDegraded
   Status:  True
   Type:    Degraded</pre>

							</p></div></li></ol></div></section></section><section class="section" id="cnf-collecting-low-latency-tuning-debugging-data-for-red-hat-support_cnf-master"><div class="titlepage"><div><div><h2 class="title">10.6. Collecting low latency tuning debugging data for Red Hat Support</h2></div></div></div><p>
				When opening a support case, it is helpful to provide debugging information about your cluster to Red Hat Support.
			</p><p>
				The <code class="literal">must-gather</code> tool enables you to collect diagnostic information about your OpenShift Container Platform cluster, including node tuning, NUMA topology, and other information needed to debug issues with low latency setup.
			</p><p>
				For prompt support, supply diagnostic information for both OpenShift Container Platform and low latency tuning.
			</p><section class="section" id="cnf-about-must-gather_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.6.1. About the must-gather tool</h3></div></div></div><p>
					The <code class="literal">oc adm must-gather</code> CLI command collects the information from your cluster that is most likely needed for debugging issues, such as:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Resource definitions
						</li><li class="listitem">
							Audit logs
						</li><li class="listitem">
							Service logs
						</li></ul></div><p>
					You can specify one or more images when you run the command by including the <code class="literal">--image</code> argument. When you specify an image, the tool collects data related to that feature or product. When you run <code class="literal">oc adm must-gather</code>, a new pod is created on the cluster. The data is collected on that pod and saved in a new directory that starts with <code class="literal">must-gather.local</code>. This directory is created in your current working directory.
				</p></section><section class="section" id="cnf-about-collecting-low-latency-data_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.6.2. About collecting low latency tuning data</h3></div></div></div><p>
					Use the <code class="literal">oc adm must-gather</code> CLI command to collect information about your cluster, including features and objects associated with low latency tuning, including:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The Node Tuning Operator namespaces and child objects.
						</li><li class="listitem">
							<code class="literal">MachineConfigPool</code> and associated <code class="literal">MachineConfig</code> objects.
						</li><li class="listitem">
							The Node Tuning Operator and associated Tuned objects.
						</li><li class="listitem">
							Linux Kernel command line options.
						</li><li class="listitem">
							CPU and NUMA topology
						</li><li class="listitem">
							Basic PCI device information and NUMA locality.
						</li></ul></div><p>
					To collect debugging information with <code class="literal">must-gather</code>, you must specify the Performance Addon Operator <code class="literal">must-gather</code> image:
				</p><pre class="programlisting language-terminal">--image=registry.redhat.io/openshift4/performance-addon-operator-must-gather-rhel8:v4.13.</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						In earlier versions of OpenShift Container Platform, the Performance Addon Operator provided automatic, low latency performance tuning for applications. In OpenShift Container Platform 4.11 and later, this functionality is part of the Node Tuning Operator. However, you must still use the <code class="literal">performance-addon-operator-must-gather</code> image when running the <code class="literal">must-gather</code> command.
					</p></div></div></section><section class="section" id="cnf-about-gathering-data_cnf-master"><div class="titlepage"><div><div><h3 class="title">10.6.3. Gathering data about specific features</h3></div></div></div><p>
					You can gather debugging information about specific features by using the <code class="literal">oc adm must-gather</code> CLI command with the <code class="literal">--image</code> or <code class="literal">--image-stream</code> argument. The <code class="literal">must-gather</code> tool supports multiple images, so you can gather data about more than one feature by running a single command.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						To collect the default <code class="literal">must-gather</code> data in addition to specific feature data, add the <code class="literal">--image-stream=openshift/must-gather</code> argument.
					</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						In earlier versions of OpenShift Container Platform, the Performance Addon Operator provided automatic, low latency performance tuning for applications. In OpenShift Container Platform 4.11, these functions are part of the Node Tuning Operator. However, you must still use the <code class="literal">performance-addon-operator-must-gather</code> image when running the <code class="literal">must-gather</code> command.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							The OpenShift Container Platform CLI (oc) installed.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Navigate to the directory where you want to store the <code class="literal">must-gather</code> data.
						</li><li class="listitem"><p class="simpara">
							Run the <code class="literal">oc adm must-gather</code> command with one or more <code class="literal">--image</code> or <code class="literal">--image-stream</code> arguments. For example, the following command gathers both the default cluster data and information specific to the Node Tuning Operator:
						</p><pre class="programlisting language-terminal">$ oc adm must-gather \
 --image-stream=openshift/must-gather \ <span id="CO34-1"><!--Empty--></span><span class="callout">1</span>

 --image=registry.redhat.io/openshift4/performance-addon-operator-must-gather-rhel8:v4.13 <span id="CO34-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO34-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The default OpenShift Container Platform <code class="literal">must-gather</code> image.
								</div></dd><dt><a href="#CO34-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The <code class="literal">must-gather</code> image for low latency tuning diagnostics.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create a compressed file from the <code class="literal">must-gather</code> directory that was created in your working directory. For example, on a computer that uses a Linux operating system, run the following command:
						</p><pre class="programlisting language-terminal"> $ tar cvaf must-gather.tar.gz must-gather.local.5421342344627712289/ <span id="CO35-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO35-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal">must-gather-local.5421342344627712289/</code> with the actual directory name.
								</div></dd></dl></div></li><li class="listitem">
							Attach the compressed file to your support case on the <a class="link" href="https://access.redhat.com/">Red Hat Customer Portal</a>.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For more information about MachineConfig and KubeletConfig, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-managing">Managing nodes</a>.
						</li><li class="listitem">
							For more information about the Node Tuning Operator, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#using-node-tuning-operator">Using the Node Tuning Operator</a>.
						</li><li class="listitem">
							For more information about the PerformanceProfile, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#configuring-huge-pages_huge-pages">Configuring huge pages</a>.
						</li><li class="listitem">
							For more information about consuming huge pages from your containers, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#how-huge-pages-are-consumed-by-apps_huge-pages">How huge pages are consumed by apps</a>.
						</li></ul></div></section></section></section><section class="chapter" id="cnf-performing-platform-verification-latency-tests"><div class="titlepage"><div><div><h1 class="title">Chapter 11. Performing latency tests for platform verification</h1></div></div></div><p>
			You can use the Cloud-native Network Functions (CNF) tests image to run latency tests on a CNF-enabled OpenShift Container Platform cluster, where all the components required for running CNF workloads are installed. Run the latency tests to validate node tuning for your workload.
		</p><p>
			The <code class="literal">cnf-tests</code> container image is available at <code class="literal">registry.redhat.io/openshift4/cnf-tests-rhel8:v4.13</code>.
		</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				The <code class="literal">cnf-tests</code> image also includes several tests that are not supported by Red Hat at this time. Only the latency tests are supported by Red Hat.
			</p></div></div><section class="section" id="cnf-latency-tests-prerequisites_cnf-latency-tests"><div class="titlepage"><div><div><h2 class="title">11.1. Prerequisites for running latency tests</h2></div></div></div><p>
				Your cluster must meet the following requirements before you can run the latency tests:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						You have configured a performance profile with the Node Tuning Operator.
					</li><li class="listitem">
						You have applied all the required CNF configurations in the cluster.
					</li><li class="listitem">
						You have a pre-existing <code class="literal">MachineConfigPool</code> CR applied in the cluster. The default worker pool is <code class="literal">worker-cnf</code>.
					</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						For more information about creating the cluster performance profile, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#node-tuning-operator-provisioning-worker-with-real-time-capabilities_cnf-master">Provisioning a worker with real-time capabilities</a>.
					</li></ul></div></section><section class="section" id="discovery-mode_cnf-latency-tests"><div class="titlepage"><div><div><h2 class="title">11.2. About discovery mode for latency tests</h2></div></div></div><p>
				Use discovery mode to validate the functionality of a cluster without altering its configuration. Existing environment configurations are used for the tests. The tests can find the configuration items needed and use those items to execute the tests. If resources needed to run a specific test are not found, the test is skipped, providing an appropriate message to the user. After the tests are finished, no cleanup of the pre-configured configuration items is done, and the test environment can be immediately used for another test run.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					When running the latency tests, <span class="strong strong"><strong>always</strong></span> run the tests with <code class="literal">-e DISCOVERY_MODE=true</code> and <code class="literal">-ginkgo.focus</code> set to the appropriate latency test. If you do not run the latency tests in discovery mode, your existing live cluster performance profile configuration will be modified by the test run.
				</p></div></div><h5 id="limiting-the-nodes-used-during-tests">Limiting the nodes used during tests</h5><p>
				The nodes on which the tests are executed can be limited by specifying a <code class="literal">NODES_SELECTOR</code> environment variable, for example, <code class="literal">-e NODES_SELECTOR=node-role.kubernetes.io/worker-cnf</code>. Any resources created by the test are limited to nodes with matching labels.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					If you want to override the default worker pool, pass the <code class="literal">-e ROLE_WORKER_CNF=&lt;custom_worker_pool&gt;</code> variable to the command specifying an appropriate label.
				</p></div></div></section><section class="section" id="cnf-measuring-latency_cnf-latency-tests"><div class="titlepage"><div><div><h2 class="title">11.3. Measuring latency</h2></div></div></div><p>
				The <code class="literal">cnf-tests</code> image uses three tools to measure the latency of the system:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">hwlatdetect</code>
					</li><li class="listitem">
						<code class="literal">cyclictest</code>
					</li><li class="listitem">
						<code class="literal">oslat</code>
					</li></ul></div><p>
				Each tool has a specific use. Use the tools in sequence to achieve reliable test results.
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">hwlatdetect</span></dt><dd>
							Measures the baseline that the bare-metal hardware can achieve. Before proceeding with the next latency test, ensure that the latency reported by <code class="literal">hwlatdetect</code> meets the required threshold because you cannot fix hardware latency spikes by operating system tuning.
						</dd><dt><span class="term">cyclictest</span></dt><dd>
							Verifies the real-time kernel scheduler latency after <code class="literal">hwlatdetect</code> passes validation. The <code class="literal">cyclictest</code> tool schedules a repeated timer and measures the difference between the desired and the actual trigger times. The difference can uncover basic issues with the tuning caused by interrupts or process priorities. The tool must run on a real-time kernel.
						</dd><dt><span class="term">oslat</span></dt><dd>
							Behaves similarly to a CPU-intensive DPDK application and measures all the interruptions and disruptions to the busy loop that simulates CPU heavy data processing.
						</dd></dl></div><p>
				The tests introduce the following environment variables:
			</p><div class="table" id="idm139735333113984"><p class="title"><strong>Table 11.1. Latency test environment variables</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 75%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735342772448" scope="col">Environment variables</th><th align="left" valign="top" id="idm139735342771472" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735342772448"> <p>
								<code class="literal">LATENCY_TEST_DELAY</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139735342771472"> <p>
								Specifies the amount of time in seconds after which the test starts running. You can use the variable to allow the CPU manager reconcile loop to update the default CPU pool. The default value is 0.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342772448"> <p>
								<code class="literal">LATENCY_TEST_CPUS</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139735342771472"> <p>
								Specifies the number of CPUs that the pod running the latency tests uses. If you do not set the variable, the default configuration includes all isolated CPUs.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342772448"> <p>
								<code class="literal">LATENCY_TEST_RUNTIME</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139735342771472"> <p>
								Specifies the amount of time in seconds that the latency test must run. The default value is 300 seconds.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342772448"> <p>
								<code class="literal">HWLATDETECT_MAXIMUM_LATENCY</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139735342771472"> <p>
								Specifies the maximum acceptable hardware latency in microseconds for the workload and operating system. If you do not set the value of <code class="literal">HWLATDETECT_MAXIMUM_LATENCY</code> or <code class="literal">MAXIMUM_LATENCY</code>, the tool compares the default expected threshold (20μs) and the actual maximum latency in the tool itself. Then, the test fails or succeeds accordingly.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342772448"> <p>
								<code class="literal">CYCLICTEST_MAXIMUM_LATENCY</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139735342771472"> <p>
								Specifies the maximum latency in microseconds that all threads expect before waking up during the <code class="literal">cyclictest</code> run. If you do not set the value of <code class="literal">CYCLICTEST_MAXIMUM_LATENCY</code> or <code class="literal">MAXIMUM_LATENCY</code>, the tool skips the comparison of the expected and the actual maximum latency.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342772448"> <p>
								<code class="literal">OSLAT_MAXIMUM_LATENCY</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139735342771472"> <p>
								Specifies the maximum acceptable latency in microseconds for the <code class="literal">oslat</code> test results. If you do not set the value of <code class="literal">OSLAT_MAXIMUM_LATENCY</code> or <code class="literal">MAXIMUM_LATENCY</code>, the tool skips the comparison of the expected and the actual maximum latency.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342772448"> <p>
								<code class="literal">MAXIMUM_LATENCY</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139735342771472"> <p>
								Unified variable that specifies the maximum acceptable latency in microseconds. Applicable for all available latency tools.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139735342772448"> <p>
								<code class="literal">LATENCY_TEST_RUN</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139735342771472"> <p>
								Boolean parameter that indicates whether the tests should run. <code class="literal">LATENCY_TEST_RUN</code> is set to <code class="literal">false</code> by default. To run the latency tests, set this value to <code class="literal">true</code>.
							</p>
							 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Variables that are specific to a latency tool take precedence over unified variables. For example, if <code class="literal">OSLAT_MAXIMUM_LATENCY</code> is set to 30 microseconds and <code class="literal">MAXIMUM_LATENCY</code> is set to 10 microseconds, the <code class="literal">oslat</code> test will run with maximum acceptable latency of 30 microseconds.
				</p></div></div></section><section class="section" id="cnf-performing-end-to-end-tests-running-the-tests_cnf-latency-tests"><div class="titlepage"><div><div><h2 class="title">11.4. Running the latency tests</h2></div></div></div><p>
				Run the cluster latency tests to validate node tuning for your Cloud-native Network Functions (CNF) workload.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					<span class="strong strong"><strong>Always</strong></span> run the latency tests with <code class="literal">DISCOVERY_MODE=true</code> set. If you don’t, the test suite will make changes to the running cluster configuration.
				</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					When executing <code class="literal">podman</code> commands as a non-root or non-privileged user, mounting paths can fail with <code class="literal">permission denied</code> errors. To make the <code class="literal">podman</code> command work, append <code class="literal">:Z</code> to the volumes creation; for example, <code class="literal">-v $(pwd)/:/kubeconfig:Z</code>. This allows <code class="literal">podman</code> to do the proper SELinux relabeling.
				</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Open a shell prompt in the directory containing the <code class="literal">kubeconfig</code> file.
					</p><p class="simpara">
						You provide the test image with a <code class="literal">kubeconfig</code> file in current directory and its related <code class="literal">$KUBECONFIG</code> environment variable, mounted through a volume. This allows the running container to use the <code class="literal">kubeconfig</code> file from inside the container.
					</p></li><li class="listitem"><p class="simpara">
						Run the latency tests by entering the following command:
					</p><pre class="programlisting language-terminal">$ podman run -v $(pwd)/:/kubeconfig:Z -e KUBECONFIG=/kubeconfig/kubeconfig \
-e LATENCY_TEST_RUN=true -e DISCOVERY_MODE=true -e FEATURES=performance registry.redhat.io/openshift4/cnf-tests-rhel8:v4.13 \
/usr/bin/test-run.sh -ginkgo.focus="\[performance\]\ Latency\ Test"</pre></li><li class="listitem">
						Optional: Append <code class="literal">-ginkgo.dryRun</code> to run the latency tests in dry-run mode. This is useful for checking what the tests run.
					</li><li class="listitem">
						Optional: Append <code class="literal">-ginkgo.v</code> to run the tests with increased verbosity.
					</li><li class="listitem"><p class="simpara">
						Optional: To run the latency tests against a specific performance profile, run the following command, substituting appropriate values:
					</p><pre class="programlisting language-terminal">$ podman run -v $(pwd)/:/kubeconfig:Z -e KUBECONFIG=/kubeconfig/kubeconfig \
-e LATENCY_TEST_RUN=true -e FEATURES=performance -e LATENCY_TEST_RUNTIME=600 -e MAXIMUM_LATENCY=20 \
-e PERF_TEST_PROFILE=&lt;performance_profile&gt; registry.redhat.io/openshift4/cnf-tests-rhel8:v4.13 \
/usr/bin/test-run.sh -ginkgo.focus="[performance]\ Latency\ Test"</pre><p class="simpara">
						where:
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;performance_profile&gt; </span></dt><dd>
									Is the name of the performance profile you want to run the latency tests against.
								</dd></dl></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							For valid latency test results, run the tests for at least 12 hours.
						</p></div></div></li></ol></div><section class="section" id="cnf-performing-end-to-end-tests-running-hwlatdetect_cnf-latency-tests"><div class="titlepage"><div><div><h3 class="title">11.4.1. Running hwlatdetect</h3></div></div></div><p>
					The <code class="literal">hwlatdetect</code> tool is available in the <code class="literal">rt-kernel</code> package with a regular subscription of Red Hat Enterprise Linux (RHEL) 8.x.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						<span class="strong strong"><strong>Always</strong></span> run the latency tests with <code class="literal">DISCOVERY_MODE=true</code> set. If you don’t, the test suite will make changes to the running cluster configuration.
					</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						When executing <code class="literal">podman</code> commands as a non-root or non-privileged user, mounting paths can fail with <code class="literal">permission denied</code> errors. To make the <code class="literal">podman</code> command work, append <code class="literal">:Z</code> to the volumes creation; for example, <code class="literal">-v $(pwd)/:/kubeconfig:Z</code>. This allows <code class="literal">podman</code> to do the proper SELinux relabeling.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the real-time kernel in the cluster.
						</li><li class="listitem">
							You have logged in to <code class="literal">registry.redhat.io</code> with your Customer Portal credentials.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To run the <code class="literal">hwlatdetect</code> tests, run the following command, substituting variable values as appropriate:
						</p><pre class="programlisting language-terminal">$ podman run -v $(pwd)/:/kubeconfig:Z -e KUBECONFIG=/kubeconfig/kubeconfig \
-e LATENCY_TEST_RUN=true -e DISCOVERY_MODE=true -e FEATURES=performance -e ROLE_WORKER_CNF=worker-cnf \
-e LATENCY_TEST_RUNTIME=600 -e MAXIMUM_LATENCY=20 \
registry.redhat.io/openshift4/cnf-tests-rhel8:v4.13 \
/usr/bin/test-run.sh -ginkgo.v -ginkgo.focus="hwlatdetect"</pre><p class="simpara">
							The <code class="literal">hwlatdetect</code> test runs for 10 minutes (600 seconds). The test runs successfully when the maximum observed latency is lower than <code class="literal">MAXIMUM_LATENCY</code> (20 μs).
						</p><p class="simpara">
							If the results exceed the latency threshold, the test fails.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								For valid results, the test should run for at least 12 hours.
							</p></div></div><div class="formalpara"><p class="title"><strong>Example failure output</strong></p><p>
								
<pre class="programlisting language-terminal">running /usr/bin/cnftests -ginkgo.v -ginkgo.focus=hwlatdetect
I0908 15:25:20.023712      27 request.go:601] Waited for 1.046586367s due to client-side throttling, not priority and fairness, request: GET:https://api.hlxcl6.lab.eng.tlv2.redhat.com:6443/apis/imageregistry.operator.openshift.io/v1?timeout=32s
Running Suite: CNF Features e2e integration tests
=================================================
Random Seed: 1662650718
Will run 1 of 194 specs

[...]

• Failure [283.574 seconds]
[performance] Latency Test
/remote-source/app/vendor/github.com/openshift/cluster-node-tuning-operator/test/e2e/performanceprofile/functests/4_latency/latency.go:62
  with the hwlatdetect image
  /remote-source/app/vendor/github.com/openshift/cluster-node-tuning-operator/test/e2e/performanceprofile/functests/4_latency/latency.go:228
    should succeed [It]
    /remote-source/app/vendor/github.com/openshift/cluster-node-tuning-operator/test/e2e/performanceprofile/functests/4_latency/latency.go:236

    Log file created at: 2022/09/08 15:25:27
    Running on machine: hwlatdetect-b6n4n
    Binary: Built with gc go1.17.12 for linux/amd64
    Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
    I0908 15:25:27.160620       1 node.go:39] Environment information: /proc/cmdline: BOOT_IMAGE=(hd1,gpt3)/ostree/rhcos-c6491e1eedf6c1f12ef7b95e14ee720bf48359750ac900b7863c625769ef5fb9/vmlinuz-4.18.0-372.19.1.el8_6.x86_64 random.trust_cpu=on console=tty0 console=ttyS0,115200n8 ignition.platform.id=metal ostree=/ostree/boot.1/rhcos/c6491e1eedf6c1f12ef7b95e14ee720bf48359750ac900b7863c625769ef5fb9/0 ip=dhcp root=UUID=5f80c283-f6e6-4a27-9b47-a287157483b2 rw rootflags=prjquota boot=UUID=773bf59a-bafd-48fc-9a87-f62252d739d3 skew_tick=1 nohz=on rcu_nocbs=0-3 tuned.non_isolcpus=0000ffff,ffffffff,fffffff0 systemd.cpu_affinity=4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79 intel_iommu=on iommu=pt isolcpus=managed_irq,0-3 nohz_full=0-3 tsc=nowatchdog nosoftlockup nmi_watchdog=0 mce=off skew_tick=1 rcutree.kthread_prio=11 + +
    I0908 15:25:27.160830       1 node.go:46] Environment information: kernel version 4.18.0-372.19.1.el8_6.x86_64
    I0908 15:25:27.160857       1 main.go:50] running the hwlatdetect command with arguments [/usr/bin/hwlatdetect --threshold 1 --hardlimit 1 --duration 100 --window 10000000us --width 950000us]
    F0908 15:27:10.603523       1 main.go:53] failed to run hwlatdetect command; out: hwlatdetect:  test duration 100 seconds
       detector: tracer
       parameters:
            Latency threshold: 1us <span id="CO36-1"><!--Empty--></span><span class="callout">1</span>
            Sample window:     10000000us
            Sample width:      950000us
         Non-sampling period:  9050000us
            Output File:       None

    Starting test
    test finished
    Max Latency: 326us <span id="CO36-2"><!--Empty--></span><span class="callout">2</span>
    Samples recorded: 5
    Samples exceeding threshold: 5
    ts: 1662650739.017274507, inner:6, outer:6
    ts: 1662650749.257272414, inner:14, outer:326
    ts: 1662650779.977272835, inner:314, outer:12
    ts: 1662650800.457272384, inner:3, outer:9
    ts: 1662650810.697273520, inner:3, outer:2

[...]

JUnit report was created: /junit.xml/cnftests-junit.xml


Summarizing 1 Failure:

[Fail] [performance] Latency Test with the hwlatdetect image [It] should succeed
/remote-source/app/vendor/github.com/openshift/cluster-node-tuning-operator/test/e2e/performanceprofile/functests/4_latency/latency.go:476

Ran 1 of 194 Specs in 365.797 seconds
FAIL! -- 0 Passed | 1 Failed | 0 Pending | 193 Skipped
--- FAIL: TestTest (366.08s)
FAIL</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO36-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									You can configure the latency threshold by using the <code class="literal">MAXIMUM_LATENCY</code> or the <code class="literal">HWLATDETECT_MAXIMUM_LATENCY</code> environment variables.
								</div></dd><dt><a href="#CO36-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The maximum latency value measured during the test.
								</div></dd></dl></div></li></ul></div><h5 id="cnf-performing-end-to-end-tests-example-results-hwlatdetect_cnf-latency-tests">Example hwlatdetect test results</h5><p>
					You can capture the following types of results:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Rough results that are gathered after each run to create a history of impact on any changes made throughout the test.
						</li><li class="listitem">
							The combined set of the rough tests with the best results and configuration settings.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Example of good results</strong></p><p>
						
<pre class="programlisting language-terminal">hwlatdetect: test duration 3600 seconds
detector: tracer
parameters:
Latency threshold: 10us
Sample window: 1000000us
Sample width: 950000us
Non-sampling period: 50000us
Output File: None

Starting test
test finished
Max Latency: Below threshold
Samples recorded: 0</pre>

					</p></div><p>
					The <code class="literal">hwlatdetect</code> tool only provides output if the sample exceeds the specified threshold.
				</p><div class="formalpara"><p class="title"><strong>Example of bad results</strong></p><p>
						
<pre class="programlisting language-terminal">hwlatdetect: test duration 3600 seconds
detector: tracer
parameters:Latency threshold: 10usSample window: 1000000us
Sample width: 950000usNon-sampling period: 50000usOutput File: None

Starting tests:1610542421.275784439, inner:78, outer:81
ts: 1610542444.330561619, inner:27, outer:28
ts: 1610542445.332549975, inner:39, outer:38
ts: 1610542541.568546097, inner:47, outer:32
ts: 1610542590.681548531, inner:13, outer:17
ts: 1610543033.818801482, inner:29, outer:30
ts: 1610543080.938801990, inner:90, outer:76
ts: 1610543129.065549639, inner:28, outer:39
ts: 1610543474.859552115, inner:28, outer:35
ts: 1610543523.973856571, inner:52, outer:49
ts: 1610543572.089799738, inner:27, outer:30
ts: 1610543573.091550771, inner:34, outer:28
ts: 1610543574.093555202, inner:116, outer:63</pre>

					</p></div><p>
					The output of <code class="literal">hwlatdetect</code> shows that multiple samples exceed the threshold. However, the same output can indicate different results based on the following factors:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The duration of the test
						</li><li class="listitem">
							The number of CPU cores
						</li><li class="listitem">
							The host firmware settings
						</li></ul></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Before proceeding with the next latency test, ensure that the latency reported by <code class="literal">hwlatdetect</code> meets the required threshold. Fixing latencies introduced by hardware might require you to contact the system vendor support.
					</p><p>
						Not all latency spikes are hardware related. Ensure that you tune the host firmware to meet your workload requirements. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/9/html-single/optimizing_rhel_9_for_real_time_for_low_latency_operation/index#setting-bios-parameters-for-system-tuning_optimizing-RHEL9-for-real-time-for-low-latency-operation">Setting firmware parameters for system tuning</a>.
					</p></div></div></section><section class="section" id="cnf-performing-end-to-end-tests-running-cyclictest_cnf-latency-tests"><div class="titlepage"><div><div><h3 class="title">11.4.2. Running cyclictest</h3></div></div></div><p>
					The <code class="literal">cyclictest</code> tool measures the real-time kernel scheduler latency on the specified CPUs.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						<span class="strong strong"><strong>Always</strong></span> run the latency tests with <code class="literal">DISCOVERY_MODE=true</code> set. If you don’t, the test suite will make changes to the running cluster configuration.
					</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						When executing <code class="literal">podman</code> commands as a non-root or non-privileged user, mounting paths can fail with <code class="literal">permission denied</code> errors. To make the <code class="literal">podman</code> command work, append <code class="literal">:Z</code> to the volumes creation; for example, <code class="literal">-v $(pwd)/:/kubeconfig:Z</code>. This allows <code class="literal">podman</code> to do the proper SELinux relabeling.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have logged in to <code class="literal">registry.redhat.io</code> with your Customer Portal credentials.
						</li><li class="listitem">
							You have installed the real-time kernel in the cluster.
						</li><li class="listitem">
							You have applied a cluster performance profile by using Node Tuning Operator.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To perform the <code class="literal">cyclictest</code>, run the following command, substituting variable values as appropriate:
						</p><pre class="programlisting language-terminal">$ podman run -v $(pwd)/:/kubeconfig:Z -e KUBECONFIG=/kubeconfig/kubeconfig \
-e LATENCY_TEST_RUN=true -e DISCOVERY_MODE=true -e FEATURES=performance -e ROLE_WORKER_CNF=worker-cnf \
-e LATENCY_TEST_CPUS=10 -e LATENCY_TEST_RUNTIME=600 -e MAXIMUM_LATENCY=20 \
registry.redhat.io/openshift4/cnf-tests-rhel8:v4.13 \
/usr/bin/test-run.sh -ginkgo.v -ginkgo.focus="cyclictest"</pre><p class="simpara">
							The command runs the <code class="literal">cyclictest</code> tool for 10 minutes (600 seconds). The test runs successfully when the maximum observed latency is lower than <code class="literal">MAXIMUM_LATENCY</code> (in this example, 20 μs). Latency spikes of 20 μs and above are generally not acceptable for telco RAN workloads.
						</p><p class="simpara">
							If the results exceed the latency threshold, the test fails.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								For valid results, the test should run for at least 12 hours.
							</p></div></div><div class="formalpara"><p class="title"><strong>Example failure output</strong></p><p>
								
<pre class="programlisting language-terminal">running /usr/bin/cnftests -ginkgo.v -ginkgo.focus=cyclictest
I0908 13:01:59.193776      27 request.go:601] Waited for 1.046228824s due to client-side throttling, not priority and fairness, request: GET:https://api.compute-1.example.com:6443/apis/packages.operators.coreos.com/v1?timeout=32s
Running Suite: CNF Features e2e integration tests
=================================================
Random Seed: 1662642118
Will run 1 of 194 specs

[...]

Summarizing 1 Failure:

[Fail] [performance] Latency Test with the cyclictest image [It] should succeed
/remote-source/app/vendor/github.com/openshift/cluster-node-tuning-operator/test/e2e/performanceprofile/functests/4_latency/latency.go:220

Ran 1 of 194 Specs in 161.151 seconds
FAIL! -- 0 Passed | 1 Failed | 0 Pending | 193 Skipped
--- FAIL: TestTest (161.48s)
FAIL</pre>

							</p></div></li></ul></div><h5 id="cnf-performing-end-to-end-tests-example-results-cyclictest_cnf-latency-tests">Example cyclictest results</h5><p>
					The same output can indicate different results for different workloads. For example, spikes up to 18μs are acceptable for 4G DU workloads, but not for 5G DU workloads.
				</p><div class="formalpara"><p class="title"><strong>Example of good results</strong></p><p>
						
<pre class="programlisting language-terminal">running cmd: cyclictest -q -D 10m -p 1 -t 16 -a 2,4,6,8,10,12,14,16,54,56,58,60,62,64,66,68 -h 30 -i 1000 -m
# Histogram
000000 000000   000000  000000  000000  000000  000000  000000  000000  000000  000000  000000  000000  000000  000000  000000  000000
000001 000000   000000  000000  000000  000000  000000  000000  000000  000000  000000  000000  000000  000000  000000  000000  000000
000002 579506   535967  418614  573648  532870  529897  489306  558076  582350  585188  583793  223781  532480  569130  472250  576043
More histogram entries ...
# Total: 000600000 000600000 000600000 000599999 000599999 000599999 000599998 000599998 000599998 000599997 000599997 000599996 000599996 000599995 000599995 000599995
# Min Latencies: 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002
# Avg Latencies: 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002
# Max Latencies: 00005 00005 00004 00005 00004 00004 00005 00005 00006 00005 00004 00005 00004 00004 00005 00004
# Histogram Overflows: 00000 00000 00000 00000 00000 00000 00000 00000 00000 00000 00000 00000 00000 00000 00000 00000
# Histogram Overflow at cycle number:
# Thread 0:
# Thread 1:
# Thread 2:
# Thread 3:
# Thread 4:
# Thread 5:
# Thread 6:
# Thread 7:
# Thread 8:
# Thread 9:
# Thread 10:
# Thread 11:
# Thread 12:
# Thread 13:
# Thread 14:
# Thread 15:</pre>

					</p></div><div class="formalpara"><p class="title"><strong>Example of bad results</strong></p><p>
						
<pre class="programlisting language-terminal">running cmd: cyclictest -q -D 10m -p 1 -t 16 -a 2,4,6,8,10,12,14,16,54,56,58,60,62,64,66,68 -h 30 -i 1000 -m
# Histogram
000000 000000   000000  000000  000000  000000  000000  000000  000000  000000  000000  000000  000000  000000  000000  000000  000000
000001 000000   000000  000000  000000  000000  000000  000000  000000  000000  000000  000000  000000  000000  000000  000000  000000
000002 564632   579686  354911  563036  492543  521983  515884  378266  592621  463547  482764  591976  590409  588145  589556  353518
More histogram entries ...
# Total: 000599999 000599999 000599999 000599997 000599997 000599998 000599998 000599997 000599997 000599996 000599995 000599996 000599995 000599995 000599995 000599993
# Min Latencies: 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002
# Avg Latencies: 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002 00002
# Max Latencies: 00493 00387 00271 00619 00541 00513 00009 00389 00252 00215 00539 00498 00363 00204 00068 00520
# Histogram Overflows: 00001 00001 00001 00002 00002 00001 00000 00001 00001 00001 00002 00001 00001 00001 00001 00002
# Histogram Overflow at cycle number:
# Thread 0: 155922
# Thread 1: 110064
# Thread 2: 110064
# Thread 3: 110063 155921
# Thread 4: 110063 155921
# Thread 5: 155920
# Thread 6:
# Thread 7: 110062
# Thread 8: 110062
# Thread 9: 155919
# Thread 10: 110061 155919
# Thread 11: 155918
# Thread 12: 155918
# Thread 13: 110060
# Thread 14: 110060
# Thread 15: 110059 155917</pre>

					</p></div></section><section class="section" id="cnf-performing-end-to-end-tests-running-oslat_cnf-latency-tests"><div class="titlepage"><div><div><h3 class="title">11.4.3. Running oslat</h3></div></div></div><p>
					The <code class="literal">oslat</code> test simulates a CPU-intensive DPDK application and measures all the interruptions and disruptions to test how the cluster handles CPU heavy data processing.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						<span class="strong strong"><strong>Always</strong></span> run the latency tests with <code class="literal">DISCOVERY_MODE=true</code> set. If you don’t, the test suite will make changes to the running cluster configuration.
					</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						When executing <code class="literal">podman</code> commands as a non-root or non-privileged user, mounting paths can fail with <code class="literal">permission denied</code> errors. To make the <code class="literal">podman</code> command work, append <code class="literal">:Z</code> to the volumes creation; for example, <code class="literal">-v $(pwd)/:/kubeconfig:Z</code>. This allows <code class="literal">podman</code> to do the proper SELinux relabeling.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have logged in to <code class="literal">registry.redhat.io</code> with your Customer Portal credentials.
						</li><li class="listitem">
							You have applied a cluster performance profile by using the Node Tuning Operator.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To perform the <code class="literal">oslat</code> test, run the following command, substituting variable values as appropriate:
						</p><pre class="programlisting language-terminal">$ podman run -v $(pwd)/:/kubeconfig:Z -e KUBECONFIG=/kubeconfig/kubeconfig \
-e LATENCY_TEST_RUN=true -e DISCOVERY_MODE=true -e FEATURES=performance -e ROLE_WORKER_CNF=worker-cnf \
-e LATENCY_TEST_CPUS=10 -e LATENCY_TEST_RUNTIME=600 -e MAXIMUM_LATENCY=20 \
registry.redhat.io/openshift4/cnf-tests-rhel8:v4.13 \
/usr/bin/test-run.sh -ginkgo.v -ginkgo.focus="oslat"</pre><p class="simpara">
							<code class="literal">LATENCY_TEST_CPUS</code> specifies the list of CPUs to test with the <code class="literal">oslat</code> command.
						</p><p class="simpara">
							The command runs the <code class="literal">oslat</code> tool for 10 minutes (600 seconds). The test runs successfully when the maximum observed latency is lower than <code class="literal">MAXIMUM_LATENCY</code> (20 μs).
						</p><p class="simpara">
							If the results exceed the latency threshold, the test fails.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								For valid results, the test should run for at least 12 hours.
							</p></div></div><div class="formalpara"><p class="title"><strong>Example failure output</strong></p><p>
								
<pre class="programlisting language-terminal">running /usr/bin/cnftests -ginkgo.v -ginkgo.focus=oslat
I0908 12:51:55.999393      27 request.go:601] Waited for 1.044848101s due to client-side throttling, not priority and fairness, request: GET:https://compute-1.example.com:6443/apis/machineconfiguration.openshift.io/v1?timeout=32s
Running Suite: CNF Features e2e integration tests
=================================================
Random Seed: 1662641514
Will run 1 of 194 specs

[...]

• Failure [77.833 seconds]
[performance] Latency Test
/remote-source/app/vendor/github.com/openshift/cluster-node-tuning-operator/test/e2e/performanceprofile/functests/4_latency/latency.go:62
  with the oslat image
  /remote-source/app/vendor/github.com/openshift/cluster-node-tuning-operator/test/e2e/performanceprofile/functests/4_latency/latency.go:128
    should succeed [It]
    /remote-source/app/vendor/github.com/openshift/cluster-node-tuning-operator/test/e2e/performanceprofile/functests/4_latency/latency.go:153

    The current latency 304 is bigger than the expected one 1 : <span id="CO37-1"><!--Empty--></span><span class="callout">1</span>

[...]

Summarizing 1 Failure:

[Fail] [performance] Latency Test with the oslat image [It] should succeed
/remote-source/app/vendor/github.com/openshift/cluster-node-tuning-operator/test/e2e/performanceprofile/functests/4_latency/latency.go:177

Ran 1 of 194 Specs in 161.091 seconds
FAIL! -- 0 Passed | 1 Failed | 0 Pending | 193 Skipped
--- FAIL: TestTest (161.42s)
FAIL</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO37-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									In this example, the measured latency is outside the maximum allowed value.
								</div></dd></dl></div></li></ul></div></section></section><section class="section" id="cnf-performing-end-to-end-tests-test-failure-report_cnf-latency-tests"><div class="titlepage"><div><div><h2 class="title">11.5. Generating a latency test failure report</h2></div></div></div><p>
				Use the following procedures to generate a JUnit latency test output and test failure report.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						You have logged in as a user with <code class="literal">cluster-admin</code> privileges.
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Create a test failure report with information about the cluster state and resources for troubleshooting by passing the <code class="literal">--report</code> parameter with the path to where the report is dumped:
					</p><pre class="programlisting language-terminal">$ podman run -v $(pwd)/:/kubeconfig:Z -v $(pwd)/reportdest:&lt;report_folder_path&gt; \
-e KUBECONFIG=/kubeconfig/kubeconfig  -e DISCOVERY_MODE=true -e FEATURES=performance \
registry.redhat.io/openshift4/cnf-tests-rhel8:v4.13 \
/usr/bin/test-run.sh --report &lt;report_folder_path&gt; \
-ginkgo.focus="\[performance\]\ Latency\ Test"</pre><p class="simpara">
						where:
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;report_folder_path&gt; </span></dt><dd>
									Is the path to the folder where the report is generated.
								</dd></dl></div></li></ul></div></section><section class="section" id="cnf-performing-end-to-end-tests-junit-test-output_cnf-latency-tests"><div class="titlepage"><div><div><h2 class="title">11.6. Generating a JUnit latency test report</h2></div></div></div><p>
				Use the following procedures to generate a JUnit latency test output and test failure report.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						You have logged in as a user with <code class="literal">cluster-admin</code> privileges.
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Create a JUnit-compliant XML report by passing the <code class="literal">--junit</code> parameter together with the path to where the report is dumped:
					</p><pre class="programlisting language-terminal">$ podman run -v $(pwd)/:/kubeconfig:Z -v $(pwd)/junitdest:&lt;junit_folder_path&gt; \
-e KUBECONFIG=/kubeconfig/kubeconfig -e DISCOVERY_MODE=true -e FEATURES=performance \
registry.redhat.io/openshift4/cnf-tests-rhel8:v4.13 \
/usr/bin/test-run.sh --junit &lt;junit_folder_path&gt; \
-ginkgo.focus="\[performance\]\ Latency\ Test"</pre><p class="simpara">
						where:
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;junit_folder_path&gt; </span></dt><dd>
									Is the path to the folder where the junit report is generated
								</dd></dl></div></li></ul></div></section><section class="section" id="cnf-performing-end-to-end-tests-running-in-single-node-cluster_cnf-latency-tests"><div class="titlepage"><div><div><h2 class="title">11.7. Running latency tests on a single-node OpenShift cluster</h2></div></div></div><p>
				You can run latency tests on single-node OpenShift clusters.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					<span class="strong strong"><strong>Always</strong></span> run the latency tests with <code class="literal">DISCOVERY_MODE=true</code> set. If you don’t, the test suite will make changes to the running cluster configuration.
				</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					When executing <code class="literal">podman</code> commands as a non-root or non-privileged user, mounting paths can fail with <code class="literal">permission denied</code> errors. To make the <code class="literal">podman</code> command work, append <code class="literal">:Z</code> to the volumes creation; for example, <code class="literal">-v $(pwd)/:/kubeconfig:Z</code>. This allows <code class="literal">podman</code> to do the proper SELinux relabeling.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						You have logged in as a user with <code class="literal">cluster-admin</code> privileges.
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						To run the latency tests on a single-node OpenShift cluster, run the following command:
					</p><pre class="programlisting language-terminal">$ podman run -v $(pwd)/:/kubeconfig:Z -e KUBECONFIG=/kubeconfig/kubeconfig \
-e DISCOVERY_MODE=true -e FEATURES=performance -e ROLE_WORKER_CNF=master \
registry.redhat.io/openshift4/cnf-tests-rhel8:v4.13 \
/usr/bin/test-run.sh -ginkgo.focus="\[performance\]\ Latency\ Test"</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							<code class="literal">ROLE_WORKER_CNF=master</code> is required because master is the only machine pool to which the node belongs. For more information about setting the required <code class="literal">MachineConfigPool</code> for the latency tests, see "Prerequisites for running latency tests".
						</p></div></div><p class="simpara">
						After running the test suite, all the dangling resources are cleaned up.
					</p></li></ul></div></section><section class="section" id="cnf-performing-end-to-end-tests-disconnected-mode_cnf-latency-tests"><div class="titlepage"><div><div><h2 class="title">11.8. Running latency tests in a disconnected cluster</h2></div></div></div><p>
				The CNF tests image can run tests in a disconnected cluster that is not able to reach external registries. This requires two steps:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Mirroring the <code class="literal">cnf-tests</code> image to the custom disconnected registry.
					</li><li class="listitem">
						Instructing the tests to consume the images from the custom disconnected registry.
					</li></ol></div><h4 id="cnf-performing-end-to-end-tests-mirroring-images-to-custom-registry_cnf-latency-tests">Mirroring the images to a custom registry accessible from the cluster</h4><p>
				A <code class="literal">mirror</code> executable is shipped in the image to provide the input required by <code class="literal">oc</code> to mirror the test image to a local registry.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Run this command from an intermediate machine that has access to the cluster and <a class="link" href="https://catalog.redhat.com/software/containers/explore">registry.redhat.io</a>:
					</p><pre class="programlisting language-terminal">$ podman run -v $(pwd)/:/kubeconfig:Z -e KUBECONFIG=/kubeconfig/kubeconfig \
registry.redhat.io/openshift4/cnf-tests-rhel8:v4.13 \
/usr/bin/mirror -registry &lt;disconnected_registry&gt; | oc image mirror -f -</pre><p class="simpara">
						where:
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;disconnected_registry&gt; </span></dt><dd>
									Is the disconnected mirror registry you have configured, for example, <code class="literal">my.local.registry:5000/</code>.
								</dd></dl></div></li><li class="listitem"><p class="simpara">
						When you have mirrored the <code class="literal">cnf-tests</code> image into the disconnected registry, you must override the original registry used to fetch the images when running the tests, for example:
					</p><pre class="programlisting language-terminal">$ podman run -v $(pwd)/:/kubeconfig:Z -e KUBECONFIG=/kubeconfig/kubeconfig \
-e DISCOVERY_MODE=true -e FEATURES=performance -e IMAGE_REGISTRY="&lt;disconnected_registry&gt;" \
-e CNF_TESTS_IMAGE="cnf-tests-rhel8:v4.13" \
/usr/bin/test-run.sh -ginkgo.focus="\[performance\]\ Latency\ Test"</pre></li></ol></div><h4 id="cnf-performing-end-to-end-tests-image-parameters_cnf-latency-tests">Configuring the tests to consume images from a custom registry</h4><p>
				You can run the latency tests using a custom test image and image registry using <code class="literal">CNF_TESTS_IMAGE</code> and <code class="literal">IMAGE_REGISTRY</code> variables.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						To configure the latency tests to use a custom test image and image registry, run the following command:
					</p><pre class="programlisting language-terminal">$ podman run -v $(pwd)/:/kubeconfig:Z -e KUBECONFIG=/kubeconfig/kubeconfig \
-e IMAGE_REGISTRY="&lt;custom_image_registry&gt;" \
-e CNF_TESTS_IMAGE="&lt;custom_cnf-tests_image&gt;" \
-e FEATURES=performance \
registry.redhat.io/openshift4/cnf-tests-rhel8:v4.13 /usr/bin/test-run.sh</pre><p class="simpara">
						where:
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;custom_image_registry&gt; </span></dt><dd>
									is the custom image registry, for example, <code class="literal">custom.registry:5000/</code>.
								</dd><dt><span class="term">&lt;custom_cnf-tests_image&gt; </span></dt><dd>
									is the custom cnf-tests image, for example, <code class="literal">custom-cnf-tests-image:latest</code>.
								</dd></dl></div></li></ul></div><h4 id="cnf-performing-end-to-end-tests-mirroring-to-cluster-internal-registry_cnf-latency-tests">Mirroring images to the cluster OpenShift image registry</h4><p>
				OpenShift Container Platform provides a built-in container image registry, which runs as a standard workload on the cluster.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Gain external access to the registry by exposing it with a route:
					</p><pre class="programlisting language-terminal">$ oc patch configs.imageregistry.operator.openshift.io/cluster --patch '{"spec":{"defaultRoute":true}}' --type=merge</pre></li><li class="listitem"><p class="simpara">
						Fetch the registry endpoint by running the following command:
					</p><pre class="programlisting language-terminal">$ REGISTRY=$(oc get route default-route -n openshift-image-registry --template='{{ .spec.host }}')</pre></li><li class="listitem"><p class="simpara">
						Create a namespace for exposing the images:
					</p><pre class="programlisting language-terminal">$ oc create ns cnftests</pre></li><li class="listitem"><p class="simpara">
						Make the image stream available to all the namespaces used for tests. This is required to allow the tests namespaces to fetch the images from the <code class="literal">cnf-tests</code> image stream. Run the following commands:
					</p><pre class="programlisting language-terminal">$ oc policy add-role-to-user system:image-puller system:serviceaccount:cnf-features-testing:default --namespace=cnftests</pre><pre class="programlisting language-terminal">$ oc policy add-role-to-user system:image-puller system:serviceaccount:performance-addon-operators-testing:default --namespace=cnftests</pre></li><li class="listitem"><p class="simpara">
						Retrieve the docker secret name and auth token by running the following commands:
					</p><pre class="programlisting language-terminal">$ SECRET=$(oc -n cnftests get secret | grep builder-docker | awk {'print $1'}</pre><pre class="programlisting language-terminal">$ TOKEN=$(oc -n cnftests get secret $SECRET -o jsonpath="{.data['\.dockercfg']}" | base64 --decode | jq '.["image-registry.openshift-image-registry.svc:5000"].auth')</pre></li><li class="listitem"><p class="simpara">
						Create a <code class="literal">dockerauth.json</code> file, for example:
					</p><pre class="programlisting language-bash">$ echo "{\"auths\": { \"$REGISTRY\": { \"auth\": $TOKEN } }}" &gt; dockerauth.json</pre></li><li class="listitem"><p class="simpara">
						Do the image mirroring:
					</p><pre class="programlisting language-terminal">$ podman run -v $(pwd)/:/kubeconfig:Z -e KUBECONFIG=/kubeconfig/kubeconfig \
registry.redhat.io/openshift4/cnf-tests-rhel8:4.13 \
/usr/bin/mirror -registry $REGISTRY/cnftests |  oc image mirror --insecure=true \
-a=$(pwd)/dockerauth.json -f -</pre></li><li class="listitem"><p class="simpara">
						Run the tests:
					</p><pre class="programlisting language-terminal">$ podman run -v $(pwd)/:/kubeconfig:Z -e KUBECONFIG=/kubeconfig/kubeconfig \
-e DISCOVERY_MODE=true -e FEATURES=performance -e IMAGE_REGISTRY=image-registry.openshift-image-registry.svc:5000/cnftests \
cnf-tests-local:latest /usr/bin/test-run.sh -ginkgo.focus="\[performance\]\ Latency\ Test"</pre></li></ol></div><h4 id="mirroring-different-set-of-images_cnf-latency-tests">Mirroring a different set of test images</h4><p>
				You can optionally change the default upstream images that are mirrored for the latency tests.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						The <code class="literal">mirror</code> command tries to mirror the upstream images by default. This can be overridden by passing a file with the following format to the image:
					</p><pre class="programlisting language-yaml">[
    {
        "registry": "public.registry.io:5000",
        "image": "imageforcnftests:4.13"
    }
]</pre></li><li class="listitem"><p class="simpara">
						Pass the file to the <code class="literal">mirror</code> command, for example saving it locally as <code class="literal">images.json</code>. With the following command, the local path is mounted in <code class="literal">/kubeconfig</code> inside the container and that can be passed to the mirror command.
					</p><pre class="programlisting language-terminal">$ podman run -v $(pwd)/:/kubeconfig:Z -e KUBECONFIG=/kubeconfig/kubeconfig \
registry.redhat.io/openshift4/cnf-tests-rhel8:v4.13 /usr/bin/mirror \
--registry "my.local.registry:5000/" --images "/kubeconfig/images.json" \
|  oc image mirror -f -</pre></li></ol></div></section><section class="section" id="cnf-performing-end-to-end-tests-troubleshooting_cnf-latency-tests"><div class="titlepage"><div><div><h2 class="title">11.9. Troubleshooting errors with the cnf-tests container</h2></div></div></div><p>
				To run latency tests, the cluster must be accessible from within the <code class="literal">cnf-tests</code> container.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						You have logged in as a user with <code class="literal">cluster-admin</code> privileges.
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Verify that the cluster is accessible from inside the <code class="literal">cnf-tests</code> container by running the following command:
					</p><pre class="programlisting language-terminal">$ podman run -v $(pwd)/:/kubeconfig:Z -e KUBECONFIG=/kubeconfig/kubeconfig \
registry.redhat.io/openshift4/cnf-tests-rhel8:v4.13 \
oc get nodes</pre><p class="simpara">
						If this command does not work, an error related to spanning across DNS, MTU size, or firewall access might be occurring.
					</p></li></ul></div></section></section><section class="chapter" id="scaling-worker-latency-profiles"><div class="titlepage"><div><div><h1 class="title">Chapter 12. Improving cluster stability in high latency environments using worker latency profiles</h1></div></div></div><p>
			All nodes send heartbeats to the Kubernetes Controller Manager Operator (kube controller) in the OpenShift Container Platform cluster every 10 seconds, by default. If the cluster does not receive heartbeats from a node, OpenShift Container Platform responds using several default mechanisms.
		</p><p>
			For example, if the Kubernetes Controller Manager Operator loses contact with a node after a configured period:
		</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
					The node controller on the control plane updates the node health to <code class="literal">Unhealthy</code> and marks the node <code class="literal">Ready</code> condition as <code class="literal">Unknown</code>.
				</li><li class="listitem">
					In response, the scheduler stops scheduling pods to that node.
				</li><li class="listitem">
					The on-premise node controller adds a <code class="literal">node.kubernetes.io/unreachable</code> taint with a <code class="literal">NoExecute</code> effect to the node and schedules any pods on the node for eviction after five minutes, by default.
				</li></ol></div><p>
			This behavior can cause problems if your network is prone to latency issues, especially if you have nodes at the network edge. In some cases, the Kubernetes Controller Manager Operator might not receive an update from a healthy node due to network latency. The Kubernetes Controller Manager Operator would then evict pods from the node even though the node is healthy. To avoid this problem, you can use <span class="emphasis"><em>worker latency profiles</em></span> to adjust the frequency that the kubelet and the Kubernetes Controller Manager Operator wait for status updates before taking action. These adjustments help to ensure that your cluster runs properly in the event that network latency between the control plane and the worker nodes is not optimal.
		</p><p>
			These worker latency profiles are three sets of parameters that are pre-defined with carefully tuned values that let you control the reaction of the cluster to latency issues without needing to determine the best values manually.
		</p><p>
			You can configure worker latency profiles when installing a cluster or at any time you notice increased latency in your cluster network.
		</p><section class="section" id="nodes-cluster-worker-latency-profiles-about_scaling-worker-latency-profiles"><div class="titlepage"><div><div><h2 class="title">12.1. Understanding worker latency profiles</h2></div></div></div><p>
				Worker latency profiles are multiple sets of carefully-tuned values for the <code class="literal">node-status-update-frequency</code>, <code class="literal">node-monitor-grace-period</code>, <code class="literal">default-not-ready-toleration-seconds</code> and <code class="literal">default-unreachable-toleration-seconds</code> parameters. These parameters let you control the reaction of the cluster to latency issues without needing to determine the best values manually.
			</p><p>
				All worker latency profiles configure the following parameters:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">node-status-update-frequency</code>. Specifies the amount of time in seconds that a kubelet updates its status to the Kubernetes Controller Manager Operator.
					</li><li class="listitem">
						<code class="literal">node-monitor-grace-period</code>. Specifies the amount of time in seconds that the Kubernetes Controller Manager Operator waits for an update from a kubelet before marking the node unhealthy and adding the <code class="literal">node.kubernetes.io/not-ready</code> or <code class="literal">node.kubernetes.io/unreachable</code> taint to the node.
					</li><li class="listitem">
						<code class="literal">default-not-ready-toleration-seconds</code>. Specifies the amount of time in seconds after marking a node unhealthy that the Kubernetes Controller Manager Operator waits before evicting pods from that node.
					</li><li class="listitem">
						<code class="literal">default-unreachable-toleration-seconds</code>. Specifies the amount of time in seconds after marking a node unreachable that the Kubernetes Controller Manager Operator waits before evicting pods from that node.
					</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Manually modifying the <code class="literal">node-monitor-grace-period</code> parameter is not supported.
				</p></div></div><p>
				The following Operators monitor the changes to the worker latency profiles and respond accordingly:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The Machine Config Operator (MCO) updates the <code class="literal">node-status-update-frequency</code> parameter on the worker nodes.
					</li><li class="listitem">
						The Kubernetes Controller Manager Operator updates the <code class="literal">node-monitor-grace-period</code> parameter on the control plane nodes.
					</li><li class="listitem">
						The Kubernetes API Server Operator updates the <code class="literal">default-not-ready-toleration-seconds</code> and <code class="literal">default-unreachable-toleration-seconds</code> parameters on the control plance nodes.
					</li></ul></div><p>
				While the default configuration works in most cases, OpenShift Container Platform offers two other worker latency profiles for situations where the network is experiencing higher latency than usual. The three worker latency profiles are described in the following sections:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Default worker latency profile</span></dt><dd><p class="simpara">
							With the <code class="literal">Default</code> profile, each kubelet reports its node status to the Kubelet Controller Manager Operator (kube controller) every 10 seconds. The Kubelet Controller Manager Operator checks the kubelet for a status every 5 seconds.
						</p><p class="simpara">
							The Kubernetes Controller Manager Operator waits 40 seconds for a status update before considering that node unhealthy. It marks the node with the <code class="literal">node.kubernetes.io/not-ready</code> or <code class="literal">node.kubernetes.io/unreachable</code> taint and evicts the pods on that node. If a pod on that node has the <code class="literal">NoExecute</code> toleration, the pod gets evicted in 300 seconds. If the pod has the <code class="literal">tolerationSeconds</code> parameter, the eviction waits for the period specified by that parameter.
						</p><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 17%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col><col style="width: 17%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th style="text-align: left; vertical-align: top; " id="idm139735322968112" scope="col">Profile</th><th style="text-align: left; vertical-align: top; " id="idm139735322967024" scope="col">Component</th><th style="text-align: left; vertical-align: top; " id="idm139735339528192" scope="col">Parameter</th><th style="text-align: left; vertical-align: top; " id="idm139735339527216" scope="col">Value</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; " rowspan="4" headers="idm139735322968112"> <p>
											Default
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735322967024"> <p>
											kubelet
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735339528192"> <p>
											<code class="literal">node-status-update-frequency</code>
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735339527216"> <p>
											10s
										</p>
										 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm139735322967024"> <p>
											Kubelet Controller Manager
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735339528192"> <p>
											<code class="literal">node-monitor-grace-period</code>
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735339527216"> <p>
											40s
										</p>
										 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm139735322967024"> <p>
											Kubernetes API Server
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735339528192"> <p>
											<code class="literal">default-not-ready-toleration-seconds</code>
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735339527216"> <p>
											300s
										</p>
										 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm139735322967024"> <p>
											Kubernetes API Server
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735339528192"> <p>
											<code class="literal">default-unreachable-toleration-seconds</code>
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735339527216"> <p>
											300s
										</p>
										 </td></tr></tbody></table></div></dd><dt><span class="term">Medium worker latency profile</span></dt><dd><p class="simpara">
							Use the <code class="literal">MediumUpdateAverageReaction</code> profile if the network latency is slightly higher than usual.
						</p><p class="simpara">
							The <code class="literal">MediumUpdateAverageReaction</code> profile reduces the frequency of kubelet updates to 20 seconds and changes the period that the Kubernetes Controller Manager Operator waits for those updates to 2 minutes. The pod eviction period for a pod on that node is reduced to 60 seconds. If the pod has the <code class="literal">tolerationSeconds</code> parameter, the eviction waits for the period specified by that parameter.
						</p><p class="simpara">
							The Kubernetes Controller Manager Operator waits for 2 minutes to consider a node unhealthy. In another minute, the eviction process starts.
						</p><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 17%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col><col style="width: 17%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th style="text-align: left; vertical-align: top; " id="idm139735323531168" scope="col">Profile</th><th style="text-align: left; vertical-align: top; " id="idm139735323530080" scope="col">Component</th><th style="text-align: left; vertical-align: top; " id="idm139735323528992" scope="col">Parameter</th><th style="text-align: left; vertical-align: top; " id="idm139735323527904" scope="col">Value</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; " rowspan="4" headers="idm139735323531168"> <p>
											MediumUpdateAverageReaction
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735323530080"> <p>
											kubelet
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735323528992"> <p>
											<code class="literal">node-status-update-frequency</code>
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735323527904"> <p>
											20s
										</p>
										 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm139735323530080"> <p>
											Kubelet Controller Manager
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735323528992"> <p>
											<code class="literal">node-monitor-grace-period</code>
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735323527904"> <p>
											2m
										</p>
										 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm139735323530080"> <p>
											Kubernetes API Server
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735323528992"> <p>
											<code class="literal">default-not-ready-toleration-seconds</code>
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735323527904"> <p>
											60s
										</p>
										 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm139735323530080"> <p>
											Kubernetes API Server
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735323528992"> <p>
											<code class="literal">default-unreachable-toleration-seconds</code>
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735323527904"> <p>
											60s
										</p>
										 </td></tr></tbody></table></div></dd><dt><span class="term">Low worker latency profile</span></dt><dd><p class="simpara">
							Use the <code class="literal">LowUpdateSlowReaction</code> profile if the network latency is extremely high.
						</p><p class="simpara">
							The <code class="literal">LowUpdateSlowReaction</code> profile reduces the frequency of kubelet updates to 1 minute and changes the period that the Kubernetes Controller Manager Operator waits for those updates to 5 minutes. The pod eviction period for a pod on that node is reduced to 60 seconds. If the pod has the <code class="literal">tolerationSeconds</code> parameter, the eviction waits for the period specified by that parameter.
						</p><p class="simpara">
							The Kubernetes Controller Manager Operator waits for 5 minutes to consider a node unhealthy. In another minute, the eviction process starts.
						</p><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 17%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col><col style="width: 17%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th style="text-align: left; vertical-align: top; " id="idm139735325240992" scope="col">Profile</th><th style="text-align: left; vertical-align: top; " id="idm139735325239904" scope="col">Component</th><th style="text-align: left; vertical-align: top; " id="idm139735325238816" scope="col">Parameter</th><th style="text-align: left; vertical-align: top; " id="idm139735325237728" scope="col">Value</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; " rowspan="4" headers="idm139735325240992"> <p>
											LowUpdateSlowReaction
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735325239904"> <p>
											kubelet
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735325238816"> <p>
											<code class="literal">node-status-update-frequency</code>
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735325237728"> <p>
											1m
										</p>
										 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm139735325239904"> <p>
											Kubelet Controller Manager
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735325238816"> <p>
											<code class="literal">node-monitor-grace-period</code>
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735325237728"> <p>
											5m
										</p>
										 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm139735325239904"> <p>
											Kubernetes API Server
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735325238816"> <p>
											<code class="literal">default-not-ready-toleration-seconds</code>
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735325237728"> <p>
											60s
										</p>
										 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm139735325239904"> <p>
											Kubernetes API Server
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735325238816"> <p>
											<code class="literal">default-unreachable-toleration-seconds</code>
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139735325237728"> <p>
											60s
										</p>
										 </td></tr></tbody></table></div></dd></dl></div></section><section class="section" id="nodes-cluster-worker-latency-profiles-using_scaling-worker-latency-profiles"><div class="titlepage"><div><div><h2 class="title">12.2. Using worker latency profiles</h2></div></div></div><p>
				To implement a worker latency profile to deal with network latency, edit the <code class="literal">node.config</code> object to add the name of the profile. You can change the profile at any time as latency increases or decreases.
			</p><p>
				You must move one worker latency profile at a time. For example, you cannot move directly from the <code class="literal">Default</code> profile to the <code class="literal">LowUpdateSlowReaction</code> worker latency profile. You must move from the <code class="literal">default</code> worker latency profile to the <code class="literal">MediumUpdateAverageReaction</code> profile first, then to <code class="literal">LowUpdateSlowReaction</code>. Similarly, when returning to the default profile, you must move from the low profile to the medium profile first, then to the default.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					You can also configure worker latency profiles upon installing an OpenShift Container Platform cluster.
				</p></div></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					To move from the default worker latency profile:
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Move to the medium worker latency profile:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">node.config</code> object:
							</p><pre class="programlisting language-terminal">$ oc edit nodes.config/cluster</pre></li><li class="listitem"><p class="simpara">
								Add <code class="literal">spec.workerLatencyProfile: MediumUpdateAverageReaction</code>:
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">node.config</code> object</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Node
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2022-07-08T16:02:51Z"
  generation: 1
  name: cluster
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    kind: ClusterVersion
    name: version
    uid: 36282574-bf9f-409e-a6cd-3032939293eb
  resourceVersion: "1865"
  uid: 0c0f7a4c-4307-4187-b591-6155695ac85b
spec:
  workerLatencyProfile: MediumUpdateAverageReaction <span id="CO38-1"><!--Empty--></span><span class="callout">1</span>

# ...</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO38-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specifies the medium worker latency policy.
									</div></dd></dl></div><p class="simpara">
								Scheduling on each worker node is disabled as the change is being applied.
							</p></li></ol></div></li><li class="listitem"><p class="simpara">
						Optional: Move to the low worker latency profile:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">node.config</code> object:
							</p><pre class="programlisting language-terminal">$ oc edit nodes.config/cluster</pre></li><li class="listitem"><p class="simpara">
								Change the <code class="literal">spec.workerLatencyProfile</code> value to <code class="literal">LowUpdateSlowReaction</code>:
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">node.config</code> object</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Node
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2022-07-08T16:02:51Z"
  generation: 1
  name: cluster
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    kind: ClusterVersion
    name: version
    uid: 36282574-bf9f-409e-a6cd-3032939293eb
  resourceVersion: "1865"
  uid: 0c0f7a4c-4307-4187-b591-6155695ac85b
spec:
  workerLatencyProfile: LowUpdateSlowReaction <span id="CO39-1"><!--Empty--></span><span class="callout">1</span>

# ...</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO39-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specifies to use the low worker latency policy.
									</div></dd></dl></div><p class="simpara">
								Scheduling on each worker node is disabled as the change is being applied.
							</p></li></ol></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						When all nodes return to the <code class="literal">Ready</code> condition, you can use the following command to look in the Kubernetes Controller Manager to ensure it was applied:
					</p><pre class="programlisting language-terminal">$ oc get KubeControllerManager -o yaml | grep -i workerlatency -A 5 -B 5</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal"># ...
    - lastTransitionTime: "2022-07-11T19:47:10Z"
      reason: ProfileUpdated
      status: "False"
      type: WorkerLatencyProfileProgressing
    - lastTransitionTime: "2022-07-11T19:47:10Z" <span id="CO40-1"><!--Empty--></span><span class="callout">1</span>
      message: all static pod revision(s) have updated latency profile
      reason: ProfileUpdated
      status: "True"
      type: WorkerLatencyProfileComplete
    - lastTransitionTime: "2022-07-11T19:20:11Z"
      reason: AsExpected
      status: "False"
      type: WorkerLatencyProfileDegraded
    - lastTransitionTime: "2022-07-11T19:20:36Z"
      status: "False"
# ...</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO40-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specifies that the profile is applied and active.
							</div></dd></dl></div></li></ul></div><p>
				To change the low profile to medium or change the medium to low, edit the <code class="literal">node.config</code> object and set the <code class="literal">spec.workerLatencyProfile</code> parameter to the appropriate value.
			</p></section></section><section class="chapter" id="cnf-create-performance-profiles"><div class="titlepage"><div><div><h1 class="title">Chapter 13. Creating a performance profile</h1></div></div></div><p>
			Learn about the Performance Profile Creator (PPC) and how you can use it to create a performance profile.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				Currently, disabling CPU load balancing is not supported by cgroup v2. As a result, you might not get the desired behavior from performance profiles if you have cgroup v2 enabled. Enabling cgroup v2 is not recommended if you are using performance profiles.
			</p></div></div><section class="section" id="cnf-about-the-profile-creator-tool_cnf-create-performance-profiles"><div class="titlepage"><div><div><h2 class="title">13.1. About the Performance Profile Creator</h2></div></div></div><p>
				The Performance Profile Creator (PPC) is a command-line tool, delivered with the Node Tuning Operator, used to create the performance profile. The tool consumes <code class="literal">must-gather</code> data from the cluster and several user-supplied profile arguments. The PPC generates a performance profile that is appropriate for your hardware and topology.
			</p><p>
				The tool is run by one of the following methods:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Invoking <code class="literal">podman</code>
					</li><li class="listitem">
						Calling a wrapper script
					</li></ul></div><section class="section" id="gathering-data-about-your-cluster-using-must-gather_cnf-create-performance-profiles"><div class="titlepage"><div><div><h3 class="title">13.1.1. Gathering data about your cluster using the must-gather command</h3></div></div></div><p>
					The Performance Profile Creator (PPC) tool requires <code class="literal">must-gather</code> data. As a cluster administrator, run the <code class="literal">must-gather</code> command to capture information about your cluster.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						In earlier versions of OpenShift Container Platform, the Performance Addon Operator provided automatic, low latency performance tuning for applications. In OpenShift Container Platform 4.11 and later, this functionality is part of the Node Tuning Operator. However, you must still use the <code class="literal">performance-addon-operator-must-gather</code> image when running the <code class="literal">must-gather</code> command.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Access to the Performance Addon Operator <code class="literal">must gather</code> image.
						</li><li class="listitem">
							The OpenShift CLI (<code class="literal">oc</code>) installed.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Optional: Verify that a matching machine config pool exists with a label:
						</p><pre class="programlisting language-terminal">$ oc describe mcp/worker-rt</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Name:         worker-rt
Namespace:
Labels:       machineconfiguration.openshift.io/role=worker-rt</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							If a matching label does not exist add a label for a machine config pool (MCP) that matches with the MCP name:
						</p><pre class="programlisting language-terminal">$ oc label mcp &lt;mcp_name&gt; &lt;mcp_name&gt;=""</pre></li><li class="listitem">
							Navigate to the directory where you want to store the <code class="literal">must-gather</code> data.
						</li><li class="listitem"><p class="simpara">
							Run <code class="literal">must-gather</code> on your cluster:
						</p><pre class="programlisting language-terminal">$ oc adm must-gather --image=&lt;PAO_must_gather_image&gt; --dest-dir=&lt;dir&gt;</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The <code class="literal">must-gather</code> command must be run with the <code class="literal">performance-addon-operator-must-gather</code> image. The output can optionally be compressed. Compressed output is required if you are running the Performance Profile Creator wrapper script.
							</p></div></div><div class="formalpara"><p class="title"><strong>Example</strong></p><p>
								
<pre class="programlisting language-terminal">$ oc adm must-gather --image=registry.redhat.io/openshift4/performance-addon-operator-must-gather-rhel8:v4.13 --dest-dir=&lt;path_to_must-gather&gt;/must-gather</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create a compressed file from the <code class="literal">must-gather</code> directory:
						</p><pre class="programlisting language-terminal">$ tar cvaf must-gather.tar.gz must-gather/</pre></li></ol></div></section><section class="section" id="running-the-performance-profile-profile-cluster-using-podman_cnf-create-performance-profiles"><div class="titlepage"><div><div><h3 class="title">13.1.2. Running the Performance Profile Creator using podman</h3></div></div></div><p>
					As a cluster administrator, you can run <code class="literal">podman</code> and the Performance Profile Creator to create a performance profile.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							A cluster installed on bare-metal hardware.
						</li><li class="listitem">
							A node with <code class="literal">podman</code> and OpenShift CLI (<code class="literal">oc</code>) installed.
						</li><li class="listitem">
							Access to the Node Tuning Operator image.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check the machine config pool:
						</p><pre class="programlisting language-terminal">$ oc get mcp</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME         CONFIG                                                 UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master       rendered-master-acd1358917e9f98cbdb599aea622d78b       True      False      False      3              3                   3                     0                      22h
worker-cnf   rendered-worker-cnf-1d871ac76e1951d32b2fe92369879826   False     True       False      2              1                   1                     0                      22h</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Use Podman to authenticate to <code class="literal">registry.redhat.io</code>:
						</p><pre class="programlisting language-terminal">$ podman login registry.redhat.io</pre><pre class="programlisting language-bash">Username: &lt;username&gt;
Password: &lt;password&gt;</pre></li><li class="listitem"><p class="simpara">
							Optional: Display help for the PPC tool:
						</p><pre class="programlisting language-terminal">$ podman run --rm --entrypoint performance-profile-creator registry.redhat.io/openshift4/ose-cluster-node-tuning-operator:v4.13 -h</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">A tool that automates creation of Performance Profiles

Usage:
  performance-profile-creator [flags]

Flags:
      --disable-ht                        Disable Hyperthreading
  -h, --help                              help for performance-profile-creator
      --info string                       Show cluster information; requires --must-gather-dir-path, ignore the other arguments. [Valid values: log, json] (default "log")
      --mcp-name string                   MCP name corresponding to the target machines (required)
      --must-gather-dir-path string       Must gather directory path (default "must-gather")
      --offlined-cpu-count int            Number of offlined CPUs
      --power-consumption-mode string     The power consumption mode.  [Valid values: default, low-latency, ultra-low-latency] (default "default")
      --profile-name string               Name of the performance profile to be created (default "performance")
      --reserved-cpu-count int            Number of reserved CPUs (required)
      --rt-kernel                         Enable Real Time Kernel (required)
      --split-reserved-cpus-across-numa   Split the Reserved CPUs across NUMA nodes
      --topology-manager-policy string    Kubelet Topology Manager Policy of the performance profile to be created. [Valid values: single-numa-node, best-effort, restricted] (default "restricted")
      --user-level-networking             Run with User level Networking(DPDK) enabled</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Run the Performance Profile Creator tool in discovery mode:
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Discovery mode inspects your cluster using the output from <code class="literal">must-gather</code>. The output produced includes information on:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										The NUMA cell partitioning with the allocated CPU ids
									</li><li class="listitem">
										Whether hyperthreading is enabled
									</li></ul></div><p>
								Using this information you can set appropriate values for some of the arguments supplied to the Performance Profile Creator tool.
							</p></div></div><pre class="programlisting language-terminal">$ podman run --entrypoint performance-profile-creator -v &lt;path_to_must-gather&gt;/must-gather:/must-gather:z registry.redhat.io/openshift4/ose-cluster-node-tuning-operator:v4.13 --info log --must-gather-dir-path /must-gather</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								This command uses the performance profile creator as a new entry point to <code class="literal">podman</code>. It maps the <code class="literal">must-gather</code> data for the host into the container image and invokes the required user-supplied profile arguments to produce the <code class="literal">my-performance-profile.yaml</code> file.
							</p><p>
								The <code class="literal">-v</code> option can be the path to either:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										The <code class="literal">must-gather</code> output directory
									</li><li class="listitem">
										An existing directory containing the <code class="literal">must-gather</code> decompressed tarball
									</li></ul></div><p>
								The <code class="literal">info</code> option requires a value which specifies the output format. Possible values are log and JSON. The JSON format is reserved for debugging.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Run <code class="literal">podman</code>:
						</p><pre class="programlisting language-terminal">$ podman run --entrypoint performance-profile-creator -v /must-gather:/must-gather:z registry.redhat.io/openshift4/ose-cluster-node-tuning-operator:v4.13 --mcp-name=worker-cnf --reserved-cpu-count=4 --rt-kernel=true --split-reserved-cpus-across-numa=false --must-gather-dir-path /must-gather --power-consumption-mode=ultra-low-latency --offlined-cpu-count=6 &gt; my-performance-profile.yaml</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The Performance Profile Creator arguments are shown in the Performance Profile Creator arguments table. The following arguments are required:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">reserved-cpu-count</code>
									</li><li class="listitem">
										<code class="literal">mcp-name</code>
									</li><li class="listitem">
										<code class="literal">rt-kernel</code>
									</li></ul></div><p>
								The <code class="literal">mcp-name</code> argument in this example is set to <code class="literal">worker-cnf</code> based on the output of the command <code class="literal">oc get mcp</code>. For single-node OpenShift use <code class="literal">--mcp-name=master</code>.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Review the created YAML file:
						</p><pre class="programlisting language-terminal">$ cat my-performance-profile.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: performance
spec:
  cpu:
    isolated: 2-39,48-79
    offlined: 42-47
    reserved: 0-1,40-41
  machineConfigPoolSelector:
    machineconfiguration.openshift.io/role: worker-cnf
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""
  numa:
    topologyPolicy: restricted
  realTimeKernel:
    enabled: true
  workloadHints:
    highPowerConsumption: true
    realTime: true</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Apply the generated profile:
						</p><pre class="programlisting language-terminal">$ oc apply -f my-performance-profile.yaml</pre></li></ol></div><section class="section" id="how-to-run-podman-to-create-a-profile_cnf-create-performance-profiles"><div class="titlepage"><div><div><h4 class="title">13.1.2.1. How to run <code class="literal">podman</code> to create a performance profile</h4></div></div></div><p>
						The following example illustrates how to run <code class="literal">podman</code> to create a performance profile with 20 reserved CPUs that are to be split across the NUMA nodes.
					</p><p>
						Node hardware configuration:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								80 CPUs
							</li><li class="listitem">
								Hyperthreading enabled
							</li><li class="listitem">
								Two NUMA nodes
							</li><li class="listitem">
								Even numbered CPUs run on NUMA node 0 and odd numbered CPUs run on NUMA node 1
							</li></ul></div><p>
						Run <code class="literal">podman</code> to create the performance profile:
					</p><pre class="programlisting language-terminal">$ podman run --entrypoint performance-profile-creator -v /must-gather:/must-gather:z registry.redhat.io/openshift4/ose-cluster-node-tuning-operator:v4.13 --mcp-name=worker-cnf --reserved-cpu-count=20 --rt-kernel=true --split-reserved-cpus-across-numa=true --must-gather-dir-path /must-gather &gt; my-performance-profile.yaml</pre><p>
						The created profile is described in the following YAML:
					</p><pre class="programlisting language-yaml">  apiVersion: performance.openshift.io/v2
  kind: PerformanceProfile
  metadata:
    name: performance
  spec:
    cpu:
      isolated: 10-39,50-79
      reserved: 0-9,40-49
    nodeSelector:
      node-role.kubernetes.io/worker-cnf: ""
    numa:
      topologyPolicy: restricted
    realTimeKernel:
      enabled: true</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							In this case, 10 CPUs are reserved on NUMA node 0 and 10 are reserved on NUMA node 1.
						</p></div></div></section></section><section class="section" id="running-the-performance-profile-creator-wrapper-script_cnf-create-performance-profiles"><div class="titlepage"><div><div><h3 class="title">13.1.3. Running the Performance Profile Creator wrapper script</h3></div></div></div><p>
					The performance profile wrapper script simplifies the running of the Performance Profile Creator (PPC) tool. It hides the complexities associated with running <code class="literal">podman</code> and specifying the mapping directories and it enables the creation of the performance profile.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Access to the Node Tuning Operator image.
						</li><li class="listitem">
							Access to the <code class="literal">must-gather</code> tarball.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a file on your local machine named, for example, <code class="literal">run-perf-profile-creator.sh</code>:
						</p><pre class="programlisting language-terminal">$ vi run-perf-profile-creator.sh</pre></li><li class="listitem"><p class="simpara">
							Paste the following code into the file:
						</p><pre class="programlisting language-bash">#!/bin/bash

readonly CONTAINER_RUNTIME=${CONTAINER_RUNTIME:-podman}
readonly CURRENT_SCRIPT=$(basename "$0")
readonly CMD="${CONTAINER_RUNTIME} run --entrypoint performance-profile-creator"
readonly IMG_EXISTS_CMD="${CONTAINER_RUNTIME} image exists"
readonly IMG_PULL_CMD="${CONTAINER_RUNTIME} image pull"
readonly MUST_GATHER_VOL="/must-gather"

NTO_IMG="registry.redhat.io/openshift4/ose-cluster-node-tuning-operator:v4.13"
MG_TARBALL=""
DATA_DIR=""

usage() {
  print "Wrapper usage:"
  print "  ${CURRENT_SCRIPT} [-h] [-p image][-t path] -- [performance-profile-creator flags]"
  print ""
  print "Options:"
  print "   -h                 help for ${CURRENT_SCRIPT}"
  print "   -p                 Node Tuning Operator image"
  print "   -t                 path to a must-gather tarball"

  ${IMG_EXISTS_CMD} "${NTO_IMG}" &amp;&amp; ${CMD} "${NTO_IMG}" -h
}

function cleanup {
  [ -d "${DATA_DIR}" ] &amp;&amp; rm -rf "${DATA_DIR}"
}
trap cleanup EXIT

exit_error() {
  print "error: $*"
  usage
  exit 1
}

print() {
  echo  "$*" &gt;&amp;2
}

check_requirements() {
  ${IMG_EXISTS_CMD} "${NTO_IMG}" || ${IMG_PULL_CMD} "${NTO_IMG}" || \
      exit_error "Node Tuning Operator image not found"

  [ -n "${MG_TARBALL}" ] || exit_error "Must-gather tarball file path is mandatory"
  [ -f "${MG_TARBALL}" ] || exit_error "Must-gather tarball file not found"

  DATA_DIR=$(mktemp -d -t "${CURRENT_SCRIPT}XXXX") || exit_error "Cannot create the data directory"
  tar -zxf "${MG_TARBALL}" --directory "${DATA_DIR}" || exit_error "Cannot decompress the must-gather tarball"
  chmod a+rx "${DATA_DIR}"

  return 0
}

main() {
  while getopts ':hp:t:' OPT; do
    case "${OPT}" in
      h)
        usage
        exit 0
        ;;
      p)
        NTO_IMG="${OPTARG}"
        ;;
      t)
        MG_TARBALL="${OPTARG}"
        ;;
      ?)
        exit_error "invalid argument: ${OPTARG}"
        ;;
    esac
  done
  shift $((OPTIND - 1))

  check_requirements || exit 1

  ${CMD} -v "${DATA_DIR}:${MUST_GATHER_VOL}:z" "${NTO_IMG}" "$@" --must-gather-dir-path "${MUST_GATHER_VOL}"
  echo "" 1&gt;&amp;2
}

main "$@"</pre></li><li class="listitem"><p class="simpara">
							Add execute permissions for everyone on this script:
						</p><pre class="programlisting language-terminal">$ chmod a+x run-perf-profile-creator.sh</pre></li><li class="listitem"><p class="simpara">
							Optional: Display the <code class="literal">run-perf-profile-creator.sh</code> command usage:
						</p><pre class="programlisting language-terminal">$ ./run-perf-profile-creator.sh -h</pre><div class="formalpara"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">Wrapper usage:
  run-perf-profile-creator.sh [-h] [-p image][-t path] -- [performance-profile-creator flags]

Options:
   -h                 help for run-perf-profile-creator.sh
   -p                 Node Tuning Operator image <span id="CO41-1"><!--Empty--></span><span class="callout">1</span>
   -t                 path to a must-gather tarball <span id="CO41-2"><!--Empty--></span><span class="callout">2</span>
A tool that automates creation of Performance Profiles

Usage:
  performance-profile-creator [flags]

Flags:
      --disable-ht                        Disable Hyperthreading
  -h, --help                              help for performance-profile-creator
      --info string                       Show cluster information; requires --must-gather-dir-path, ignore the other arguments. [Valid values: log, json] (default "log")
      --mcp-name string                   MCP name corresponding to the target machines (required)
      --must-gather-dir-path string       Must gather directory path (default "must-gather")
      --offlined-cpu-count int            Number of offlined CPUs
      --power-consumption-mode string     The power consumption mode.  [Valid values: default, low-latency, ultra-low-latency] (default "default")
      --profile-name string               Name of the performance profile to be created (default "performance")
      --reserved-cpu-count int            Number of reserved CPUs (required)
      --rt-kernel                         Enable Real Time Kernel (required)
      --split-reserved-cpus-across-numa   Split the Reserved CPUs across NUMA nodes
      --topology-manager-policy string    Kubelet Topology Manager Policy of the performance profile to be created. [Valid values: single-numa-node, best-effort, restricted] (default "restricted")
      --user-level-networking             Run with User level Networking(DPDK) enabled</pre>

							</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								There two types of arguments:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Wrapper arguments namely <code class="literal">-h</code>, <code class="literal">-p</code> and <code class="literal">-t</code>
									</li><li class="listitem">
										PPC arguments
									</li></ul></div></div></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO41-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Optional: Specify the Node Tuning Operator image. If not set, the default upstream image is used: <code class="literal">registry.redhat.io/openshift4/ose-cluster-node-tuning-operator:v4.13</code>.
								</div></dd><dt><a href="#CO41-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									<code class="literal">-t</code> is a required wrapper script argument and specifies the path to a <code class="literal">must-gather</code> tarball.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Run the performance profile creator tool in discovery mode:
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Discovery mode inspects your cluster using the output from <code class="literal">must-gather</code>. The output produced includes information on:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										The NUMA cell partitioning with the allocated CPU IDs
									</li><li class="listitem">
										Whether hyperthreading is enabled
									</li></ul></div><p>
								Using this information you can set appropriate values for some of the arguments supplied to the Performance Profile Creator tool.
							</p></div></div><pre class="programlisting language-terminal">$ ./run-perf-profile-creator.sh -t /must-gather/must-gather.tar.gz -- --info=log</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The <code class="literal">info</code> option requires a value which specifies the output format. Possible values are log and JSON. The JSON format is reserved for debugging.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Check the machine config pool:
						</p><pre class="programlisting language-terminal">$ oc get mcp</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME         CONFIG                                                 UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master       rendered-master-acd1358917e9f98cbdb599aea622d78b       True      False      False      3              3                   3                     0                      22h
worker-cnf   rendered-worker-cnf-1d871ac76e1951d32b2fe92369879826   False     True       False      2              1                   1                     0                      22h</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create a performance profile:
						</p><pre class="programlisting language-terminal">$ ./run-perf-profile-creator.sh -t /must-gather/must-gather.tar.gz -- --mcp-name=worker-cnf --reserved-cpu-count=2 --rt-kernel=true &gt; my-performance-profile.yaml</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The Performance Profile Creator arguments are shown in the Performance Profile Creator arguments table. The following arguments are required:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">reserved-cpu-count</code>
									</li><li class="listitem">
										<code class="literal">mcp-name</code>
									</li><li class="listitem">
										<code class="literal">rt-kernel</code>
									</li></ul></div><p>
								The <code class="literal">mcp-name</code> argument in this example is set to <code class="literal">worker-cnf</code> based on the output of the command <code class="literal">oc get mcp</code>. For single-node OpenShift use <code class="literal">--mcp-name=master</code>.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Review the created YAML file:
						</p><pre class="programlisting language-terminal">$ cat my-performance-profile.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: performance
spec:
  cpu:
    isolated: 1-39,41-79
    reserved: 0,40
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""
  numa:
    topologyPolicy: restricted
  realTimeKernel:
    enabled: false</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Apply the generated profile:
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Install the Node Tuning Operator before applying the profile.
							</p></div></div><pre class="programlisting language-terminal">$ oc apply -f my-performance-profile.yaml</pre></li></ol></div></section><section class="section" id="performance-profile-creator-arguments_cnf-create-performance-profiles"><div class="titlepage"><div><div><h3 class="title">13.1.4. Performance Profile Creator arguments</h3></div></div></div><div class="table" id="idm139735333847616"><p class="title"><strong>Table 13.1. Performance Profile Creator arguments</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 30%; " class="col_1"><!--Empty--></col><col style="width: 70%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735327340960" scope="col">Argument</th><th align="left" valign="top" id="idm139735327339984" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735327340960"> <p>
									<code class="literal">disable-ht</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735327339984"> <p>
									Disable hyperthreading.
								</p>
								 <p>
									Possible values: <code class="literal">true</code> or <code class="literal">false</code>.
								</p>
								 <p>
									Default: <code class="literal">false</code>.
								</p>
								 <div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
										If this argument is set to <code class="literal">true</code> you should not disable hyperthreading in the BIOS. Disabling hyperthreading is accomplished with a kernel command line argument.
									</p></div></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735327340960"> <p>
									<code class="literal">info</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735327339984"> <p>
									This captures cluster information and is used in discovery mode only. Discovery mode also requires the <code class="literal">must-gather-dir-path</code> argument. If any other arguments are set they are ignored.
								</p>
								 <p>
									Possible values:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">log</code>
										</li><li class="listitem"><p class="simpara">
											<code class="literal">JSON</code>
										</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												These options define the output format with the JSON format being reserved for debugging.
											</p></div></div></li></ul></div>
								 <p>
									Default: <code class="literal">log</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735327340960"> <p>
									<code class="literal">mcp-name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735327339984"> <p>
									MCP name for example <code class="literal">worker-cnf</code> corresponding to the target machines. This parameter is required.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735327340960"> <p>
									<code class="literal">must-gather-dir-path</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735327339984"> <p>
									Must gather directory path. This parameter is required.
								</p>
								 <p>
									When the user runs the tool with the wrapper script <code class="literal">must-gather</code> is supplied by the script itself and the user must not specify it.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735327340960"> <p>
									<code class="literal">offlined-cpu-count</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735327339984"> <p>
									Number of offlined CPUs.
								</p>
								 <div class="admonition note"><div class="admonition_header">Note</div><div><p>
										This must be a natural number greater than 0. If not enough logical processors are offlined then error messages are logged. The messages are:
									</p><pre class="programlisting language-terminal">Error: failed to compute the reserved and isolated CPUs: please ensure that reserved-cpu-count plus offlined-cpu-count should be in the range [0,1]</pre><pre class="programlisting language-terminal">Error: failed to compute the reserved and isolated CPUs: please specify the offlined CPU count in the range [0,1]</pre></div></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735327340960"> <p>
									<code class="literal">power-consumption-mode</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735327339984"> <p>
									The power consumption mode.
								</p>
								 <p>
									Possible values:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">default</code>: CPU partitioning with enabled power management and basic low-latency.
										</li><li class="listitem">
											<code class="literal">low-latency</code>: Enhanced measures to improve latency figures.
										</li><li class="listitem">
											<code class="literal">ultra-low-latency</code>: Priority given to optimal latency, at the expense of power management.
										</li></ul></div>
								 <p>
									Default: <code class="literal">default</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735327340960"> <p>
									<code class="literal">per-pod-power-management</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735327339984"> <p>
									Enable per-pod power management. You cannot use this argument if you configured <code class="literal">ultra-low-latency</code> as the power consumption mode.
								</p>
								 <p>
									Possible values: <code class="literal">true</code> or <code class="literal">false</code>.
								</p>
								 <p>
									Default: <code class="literal">false</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735327340960"> <p>
									<code class="literal">profile-name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735327339984"> <p>
									Name of the performance profile to create. Default: <code class="literal">performance</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735327340960"> <p>
									<code class="literal">reserved-cpu-count</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735327339984"> <p>
									Number of reserved CPUs. This parameter is required.
								</p>
								 <div class="admonition note"><div class="admonition_header">Note</div><div><p>
										This must be a natural number. A value of 0 is not allowed.
									</p></div></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735327340960"> <p>
									<code class="literal">rt-kernel</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735327339984"> <p>
									Enable real-time kernel. This parameter is required.
								</p>
								 <p>
									Possible values: <code class="literal">true</code> or <code class="literal">false</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735327340960"> <p>
									<code class="literal">split-reserved-cpus-across-numa</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735327339984"> <p>
									Split the reserved CPUs across NUMA nodes.
								</p>
								 <p>
									Possible values: <code class="literal">true</code> or <code class="literal">false</code>.
								</p>
								 <p>
									Default: <code class="literal">false</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735327340960"> <p>
									<code class="literal">topology-manager-policy</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735327339984"> <p>
									Kubelet Topology Manager policy of the performance profile to be created.
								</p>
								 <p>
									Possible values:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">single-numa-node</code>
										</li><li class="listitem">
											<code class="literal">best-effort</code>
										</li><li class="listitem">
											<code class="literal">restricted</code>
										</li></ul></div>
								 <p>
									Default: <code class="literal">restricted</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735327340960"> <p>
									<code class="literal">user-level-networking</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735327339984"> <p>
									Run with user level networking (DPDK) enabled.
								</p>
								 <p>
									Possible values: <code class="literal">true</code> or <code class="literal">false</code>.
								</p>
								 <p>
									Default: <code class="literal">false</code>.
								</p>
								 </td></tr></tbody></table></div></div></section></section><section class="section" id="cnf-create-performance-profiles-reference"><div class="titlepage"><div><div><h2 class="title">13.2. Reference performance profiles</h2></div></div></div><section class="section" id="installation-openstack-ovs-dpdk-performance-profile_cnf-create-performance-profiles"><div class="titlepage"><div><div><h3 class="title">13.2.1. A performance profile template for clusters that use OVS-DPDK on OpenStack</h3></div></div></div><p>
					To maximize machine performance in a cluster that uses Open vSwitch with the Data Plane Development Kit (OVS-DPDK) on Red Hat OpenStack Platform (RHOSP), you can use a performance profile.
				</p><p>
					You can use the following performance profile template to create a profile for your deployment.
				</p><div class="formalpara"><p class="title"><strong>A performance profile template for clusters that use OVS-DPDK</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: cnf-performanceprofile
spec:
  additionalKernelArgs:
    - nmi_watchdog=0
    - audit=0
    - mce=off
    - processor.max_cstate=1
    - idle=poll
    - intel_idle.max_cstate=0
    - default_hugepagesz=1GB
    - hugepagesz=1G
    - intel_iommu=on
  cpu:
    isolated: &lt;CPU_ISOLATED&gt;
    reserved: &lt;CPU_RESERVED&gt;
  hugepages:
    defaultHugepagesSize: 1G
    pages:
      - count: &lt;HUGEPAGES_COUNT&gt;
        node: 0
        size: 1G
  nodeSelector:
    node-role.kubernetes.io/worker: ''
  realTimeKernel:
    enabled: false
    globallyDisableIrqLoadBalancing: true</pre>

					</p></div><p>
					Insert values that are appropriate for your configuration for the <code class="literal">CPU_ISOLATED</code>, <code class="literal">CPU_RESERVED</code>, and <code class="literal">HUGEPAGES_COUNT</code> keys.
				</p><p>
					To learn how to create and use performance profiles, see the "Creating a performance profile" page in the "Scalability and performance" section of the OpenShift Container Platform documentation.
				</p></section></section><section class="section _additional-resources" id="cnf-create-performance-profiles-additional-resources"><div class="titlepage"><div><div><h2 class="title">13.3. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						For more information about the <code class="literal">must-gather</code> tool, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#nodes-nodes-managing">Gathering data about your cluster</a>.
					</li></ul></div></section></section><section class="chapter" id="enabling-workload-partitioning"><div class="titlepage"><div><div><h1 class="title">Chapter 14. Workload partitioning</h1></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				Workload partitioning is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
			</p><p>
				For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
			</p></div></div><p>
			In resource-constrained environments, you can use workload partitioning to isolate OpenShift Container Platform services, cluster management workloads, and infrastructure pods to run on a reserved set of CPUs.
		</p><p>
			The minimum number of reserved CPUs required for the cluster management is four CPU Hyper-Threads (HTs). With workload partitioning, you annotate the set of cluster management pods and a set of typical add-on Operators for inclusion in the cluster management workload partition. These pods operate normally within the minimum size CPU configuration. Additional Operators or workloads outside of the set of minimum cluster management pods require additional CPUs to be added to the workload partition.
		</p><p>
			Workload partitioning isolates user workloads from platform workloads using standard Kubernetes scheduling capabilities.
		</p><p>
			The following changes are required for workload partitioning:
		</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
					In the <code class="literal">install-config.yaml</code> file, add the additional field: <code class="literal">cpuPartitioningMode</code>.
				</p><pre class="programlisting language-yaml">apiVersion: v1
baseDomain: devcluster.openshift.com
cpuPartitioningMode: AllNodes <span id="CO42-1"><!--Empty--></span><span class="callout">1</span>
compute:
  - architecture: amd64
    hyperthreading: Enabled
    name: worker
    platform: {}
    replicas: 3
controlPlane:
  architecture: amd64
  hyperthreading: Enabled
  name: master
  platform: {}
  replicas: 3</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO42-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Sets up a cluster for CPU partitioning at install time. The default value is <code class="literal">None</code>.
						</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Workload partitioning can only be enabled during cluster installation. You cannot disable workload partitioning post-installation.
					</p></div></div></li><li class="listitem"><p class="simpara">
					In the performance profile, specify the <code class="literal">isolated</code> and <code class="literal">reserved</code> CPUs.
				</p><div class="formalpara"><p class="title"><strong>Recommended performance profile configuration</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: openshift-node-performance-profile
spec:
  additionalKernelArgs:
  - "rcupdate.rcu_normal_after_boot=0"
  - "efi=runtime"
  - "module_blacklist=irdma"
  cpu:
    isolated: 2-51,54-103
    reserved: 0-1,52-53
  hugepages:
    defaultHugepagesSize: 1G
    pages:
      - count: 32
        size: 1G
        node: 0
  machineConfigPoolSelector:
    pools.operator.machineconfiguration.openshift.io/master: ""
  nodeSelector:
    node-role.kubernetes.io/master: ''
  numa:
    topologyPolicy: "restricted"
  realTimeKernel:
    enabled: true
  workloadHints:
    realTime: true
    highPowerConsumption: false
    perPodPowerManagement: false</pre>

					</p></div><div class="table" id="idm139735326367808"><p class="title"><strong>Table 14.1. PerformanceProfile CR options for single-node OpenShift clusters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735327271952" scope="col">PerformanceProfile CR field</th><th align="left" valign="top" id="idm139735327270848" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735327271952"> <p>
									<code class="literal">metadata.name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735327270848"> <p>
									Ensure that <code class="literal">name</code> matches the following fields set in related GitOps ZTP custom resources (CRs):
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">include=openshift-node-performance-${PerformanceProfile.metadata.name}</code> in <code class="literal">TunedPerformancePatch.yaml</code>
										</li><li class="listitem">
											<code class="literal">name: 50-performance-${PerformanceProfile.metadata.name}</code> in <code class="literal">validatorCRs/informDuValidator.yaml</code>
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735327271952"> <p>
									<code class="literal">spec.additionalKernelArgs</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735327270848"> <p>
									<code class="literal">"efi=runtime"</code> Configures UEFI secure boot for the cluster host.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735327271952"> <p>
									<code class="literal">spec.cpu.isolated</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735327270848"> <p>
									Set the isolated CPUs. Ensure all of the Hyper-Threading pairs match.
								</p>
								 <div class="admonition important"><div class="admonition_header">Important</div><div><p>
										The reserved and isolated CPU pools must not overlap and together must span all available cores. CPU cores that are not accounted for cause an undefined behaviour in the system.
									</p></div></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735327271952"> <p>
									<code class="literal">spec.cpu.reserved</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735327270848"> <p>
									Set the reserved CPUs. When workload partitioning is enabled, system processes, kernel threads, and system container threads are restricted to these CPUs. All CPUs that are not isolated should be reserved.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735327271952"> <p>
									<code class="literal">spec.hugepages.pages</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735327270848"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											Set the number of huge pages (<code class="literal">count</code>)
										</li><li class="listitem">
											Set the huge pages size (<code class="literal">size</code>).
										</li><li class="listitem">
											Set <code class="literal">node</code> to the NUMA node where the <code class="literal">hugepages</code> are allocated (<code class="literal">node</code>)
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735327271952"> <p>
									<code class="literal">spec.realTimeKernel</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735327270848"> <p>
									Set <code class="literal">enabled</code> to <code class="literal">true</code> to use the realtime kernel.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735327271952"> <p>
									<code class="literal">spec.workloadHints</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735327270848"> <p>
									Use <code class="literal">workloadHints</code> to define the set of top level flags for different type of workloads. The example configuration configures the cluster for low latency and high performance.
								</p>
								 </td></tr></tbody></table></div></div></li></ol></div><p>
			Workload partitioning introduces an extended <code class="literal">management.workload.openshift.io/cores</code> resource type for platform pods. kubelet advertises the resources and CPU requests by pods allocated to the pool within the corresponding resource. When workload partitioning is enabled, the <code class="literal">management.workload.openshift.io/cores</code> resource allows the scheduler to correctly assign pods based on the <code class="literal">cpushares</code> capacity of the host, not just the default <code class="literal">cpuset</code>.
		</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
					For the recommended workload partitioning configuration for single-node OpenShift clusters, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-sno-du-enabling-workload-partitioning_sno-configure-for-vdu">Workload partitioning</a>.
				</li></ul></div></section><section class="chapter" id="using-node-observability-operator"><div class="titlepage"><div><div><h1 class="title">Chapter 15. Requesting CRI-O and Kubelet profiling data by using the Node Observability Operator</h1></div></div></div><p>
			The Node Observability Operator collects and stores the CRI-O and Kubelet profiling data of worker nodes. You can query the profiling data to analyze the CRI-O and Kubelet performance trends and debug the performance-related issues.
		</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				The Node Observability Operator is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
			</p><p>
				For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
			</p></div></div><section class="section" id="workflow-node-observability-operator_node-observability-operator"><div class="titlepage"><div><div><h2 class="title">15.1. Workflow of the Node Observability Operator</h2></div></div></div><p>
				The following workflow outlines on how to query the profiling data using the Node Observability Operator:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Install the Node Observability Operator in the OpenShift Container Platform cluster.
					</li><li class="listitem">
						Create a NodeObservability custom resource to enable the CRI-O profiling on the worker nodes of your choice.
					</li><li class="listitem">
						Run the profiling query to generate the profiling data.
					</li></ol></div></section><section class="section" id="install-node-observability-operator_node-observability-operator"><div class="titlepage"><div><div><h2 class="title">15.2. Installing the Node Observability Operator</h2></div></div></div><p>
				The Node Observability Operator is not installed in OpenShift Container Platform by default. You can install the Node Observability Operator by using the OpenShift Container Platform CLI or the web console.
			</p><section class="section" id="install-node-observability-using-cli_node-observability-operator"><div class="titlepage"><div><div><h3 class="title">15.2.1. Installing the Node Observability Operator using the CLI</h3></div></div></div><p>
					You can install the Node Observability Operator by using the OpenShift CLI (oc).
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (oc).
						</li><li class="listitem">
							You have access to the cluster with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Confirm that the Node Observability Operator is available by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get packagemanifests -n openshift-marketplace node-observability-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                            CATALOG                AGE
node-observability-operator     Red Hat Operators      9h</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">node-observability-operator</code> namespace by running the following command:
						</p><pre class="programlisting language-terminal">$ oc new-project node-observability-operator</pre></li><li class="listitem"><p class="simpara">
							Create an <code class="literal">OperatorGroup</code> object YAML file:
						</p><pre class="programlisting language-yaml">cat &lt;&lt;EOF | oc apply -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: node-observability-operator
  namespace: node-observability-operator
spec:
  targetNamespaces: []
EOF</pre></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">Subscription</code> object YAML file to subscribe a namespace to an Operator:
						</p><pre class="programlisting language-yaml">cat &lt;&lt;EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: node-observability-operator
  namespace: node-observability-operator
spec:
  channel: alpha
  name: node-observability-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF</pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the install plan name by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n node-observability-operator get sub node-observability-operator -o yaml | yq '.status.installplan.name'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">install-dt54w</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Verify the install plan status by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n node-observability-operator get ip &lt;install_plan_name&gt; -o yaml | yq '.status.phase'</pre><p class="simpara">
							<code class="literal">&lt;install_plan_name&gt;</code> is the install plan name that you obtained from the output of the previous command.
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">COMPLETE</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Verify that the Node Observability Operator is up and running:
						</p><pre class="programlisting language-terminal">$ oc get deploy -n node-observability-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                            READY   UP-TO-DATE  AVAILABLE   AGE
node-observability-operator-controller-manager  1/1     1           1           40h</pre>

							</p></div></li></ol></div></section><section class="section" id="install-node-observability-using-web-console_node-observability-operator"><div class="titlepage"><div><div><h3 class="title">15.2.2. Installing the Node Observability Operator using the web console</h3></div></div></div><p>
					You can install the Node Observability Operator from the OpenShift Container Platform web console.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							You have access to the OpenShift Container Platform web console.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to the OpenShift Container Platform web console.
						</li><li class="listitem">
							In the Administrator’s navigation panel, expand <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span>.
						</li><li class="listitem">
							In the <span class="strong strong"><strong>All items</strong></span> field, enter <span class="strong strong"><strong>Node Observability Operator</strong></span> and select the <span class="strong strong"><strong>Node Observability Operator</strong></span> tile.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Install</strong></span>.
						</li><li class="listitem"><p class="simpara">
							On the <span class="strong strong"><strong>Install Operator</strong></span> page, configure the following settings:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									In the <span class="strong strong"><strong>Update channel</strong></span> area, click <span class="strong strong"><strong>alpha</strong></span>.
								</li><li class="listitem">
									In the <span class="strong strong"><strong>Installation mode</strong></span> area, click <span class="strong strong"><strong>A specific namespace on the cluster</strong></span>.
								</li><li class="listitem">
									From the <span class="strong strong"><strong>Installed Namespace</strong></span> list, select <span class="strong strong"><strong>node-observability-operator</strong></span> from the list.
								</li><li class="listitem">
									In the <span class="strong strong"><strong>Update approval</strong></span> area, select <span class="strong strong"><strong>Automatic</strong></span>.
								</li><li class="listitem">
									Click <span class="strong strong"><strong>Install</strong></span>.
								</li></ol></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the Administrator’s navigation panel, expand <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
						</li><li class="listitem">
							Verify that the Node Observability Operator is listed in the Operators list.
						</li></ol></div></section></section><section class="section" id="creating-node-observability-custom-resource_node-observability-operator"><div class="titlepage"><div><div><h2 class="title">15.3. Creating the Node Observability custom resource</h2></div></div></div><p>
				You must create and run the <code class="literal">NodeObservability</code> custom resource (CR) before you run the profiling query. When you run the <code class="literal">NodeObservability</code> CR, it creates the necessary machine config and machine config pool CRs to enable the CRI-O profiling on the worker nodes matching the <code class="literal">nodeSelector</code>.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					If CRI-O profiling is not enabled on the worker nodes, the <code class="literal">NodeObservabilityMachineConfig</code> resource gets created. Worker nodes matching the <code class="literal">nodeSelector</code> specified in <code class="literal">NodeObservability</code> CR restarts. This might take 10 or more minutes to complete.
				</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Kubelet profiling is enabled by default.
				</p></div></div><p>
				The CRI-O unix socket of the node is mounted on the agent pod, which allows the agent to communicate with CRI-O to run the pprof request. Similarly, the <code class="literal">kubelet-serving-ca</code> certificate chain is mounted on the agent pod, which allows secure communication between the agent and node’s kubelet endpoint.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have installed the Node Observability Operator.
					</li><li class="listitem">
						You have installed the OpenShift CLI (oc).
					</li><li class="listitem">
						You have access to the cluster with <code class="literal">cluster-admin</code> privileges.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Log in to the OpenShift Container Platform CLI by running the following command:
					</p><pre class="programlisting language-terminal">$ oc login -u kubeadmin https://&lt;HOSTNAME&gt;:6443</pre></li><li class="listitem"><p class="simpara">
						Switch back to the <code class="literal">node-observability-operator</code> namespace by running the following command:
					</p><pre class="programlisting language-terminal">$ oc project node-observability-operator</pre></li><li class="listitem"><p class="simpara">
						Create a CR file named <code class="literal">nodeobservability.yaml</code> that contains the following text:
					</p><pre class="programlisting language-yaml">    apiVersion: nodeobservability.olm.openshift.io/v1alpha2
    kind: NodeObservability
    metadata:
      name: cluster <span id="CO43-1"><!--Empty--></span><span class="callout">1</span>
    spec:
      nodeSelector:
        kubernetes.io/hostname: &lt;node_hostname&gt; <span id="CO43-2"><!--Empty--></span><span class="callout">2</span>
      type: crio-kubelet</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO43-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								You must specify the name as <code class="literal">cluster</code> because there should be only one <code class="literal">NodeObservability</code> CR per cluster.
							</div></dd><dt><a href="#CO43-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specify the nodes on which the Node Observability agent must be deployed.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Run the <code class="literal">NodeObservability</code> CR:
					</p><pre class="programlisting language-terminal">oc apply -f nodeobservability.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">nodeobservability.olm.openshift.io/cluster created</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Review the status of the <code class="literal">NodeObservability</code> CR by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get nob/cluster -o yaml | yq '.status.conditions'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">conditions:
  conditions:
  - lastTransitionTime: "2022-07-05T07:33:54Z"
    message: 'DaemonSet node-observability-ds ready: true NodeObservabilityMachineConfig
      ready: true'
    reason: Ready
    status: "True"
    type: Ready</pre>

						</p></div><p class="simpara">
						<code class="literal">NodeObservability</code> CR run is completed when the reason is <code class="literal">Ready</code> and the status is <code class="literal">True</code>.
					</p></li></ol></div></section><section class="section" id="running-profiling-query_node-observability-operator"><div class="titlepage"><div><div><h2 class="title">15.4. Running the profiling query</h2></div></div></div><p>
				To run the profiling query, you must create a <code class="literal">NodeObservabilityRun</code> resource. The profiling query is a blocking operation that fetches CRI-O and Kubelet profiling data for a duration of 30 seconds. After the profiling query is complete, you must retrieve the profiling data inside the container file system <code class="literal">/run/node-observability</code> directory. The lifetime of data is bound to the agent pod through the <code class="literal">emptyDir</code> volume, so you can access the profiling data while the agent pod is in the <code class="literal">running</code> status.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You can request only one profiling query at any point of time.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have installed the Node Observability Operator.
					</li><li class="listitem">
						You have created the <code class="literal">NodeObservability</code> custom resource (CR).
					</li><li class="listitem">
						You have access to the cluster with <code class="literal">cluster-admin</code> privileges.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a <code class="literal">NodeObservabilityRun</code> resource file named <code class="literal">nodeobservabilityrun.yaml</code> that contains the following text:
					</p><pre class="programlisting language-yaml">apiVersion: nodeobservability.olm.openshift.io/v1alpha2
kind: NodeObservabilityRun
metadata:
  name: nodeobservabilityrun
spec:
  nodeObservabilityRef:
    name: cluster</pre></li><li class="listitem"><p class="simpara">
						Trigger the profiling query by running the <code class="literal">NodeObservabilityRun</code> resource:
					</p><pre class="programlisting language-terminal">$ oc apply -f nodeobservabilityrun.yaml</pre></li><li class="listitem"><p class="simpara">
						Review the status of the <code class="literal">NodeObservabilityRun</code> by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get nodeobservabilityrun nodeobservabilityrun -o yaml  | yq '.status.conditions'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">conditions:
- lastTransitionTime: "2022-07-07T14:57:34Z"
  message: Ready to start profiling
  reason: Ready
  status: "True"
  type: Ready
- lastTransitionTime: "2022-07-07T14:58:10Z"
  message: Profiling query done
  reason: Finished
  status: "True"
  type: Finished</pre>

						</p></div><p class="simpara">
						The profiling query is complete once the status is <code class="literal">True</code> and type is <code class="literal">Finished</code>.
					</p></li><li class="listitem"><p class="simpara">
						Retrieve the profiling data from the container’s <code class="literal">/run/node-observability</code> path by running the following bash script:
					</p><pre class="programlisting language-bash">for a in $(oc get nodeobservabilityrun nodeobservabilityrun -o yaml | yq .status.agents[].name); do
  echo "agent ${a}"
  mkdir -p "/tmp/${a}"
  for p in $(oc exec "${a}" -c node-observability-agent -- bash -c "ls /run/node-observability/*.pprof"); do
    f="$(basename ${p})"
    echo "copying ${f} to /tmp/${a}/${f}"
    oc exec "${a}" -c node-observability-agent -- cat "${p}" &gt; "/tmp/${a}/${f}"
  done
done</pre></li></ol></div></section></section><section class="chapter" id="clusters-at-the-network-far-edge"><div class="titlepage"><div><div><h1 class="title">Chapter 16. Clusters at the network far edge</h1></div></div></div><section class="section" id="ztp-deploying-far-edge-clusters-at-scale"><div class="titlepage"><div><div><h2 class="title">16.1. Challenges of the network far edge</h2></div></div></div><p>
				Edge computing presents complex challenges when managing many sites in geographically displaced locations. Use GitOps Zero Touch Provisioning (ZTP) to provision and manage sites at the far edge of the network.
			</p><section class="section" id="ztp-challenges-of-far-edge-deployments_ztp-deploying-far-edge-clusters-at-scale"><div class="titlepage"><div><div><h3 class="title">16.1.1. Overcoming the challenges of the network far edge</h3></div></div></div><p>
					Today, service providers want to deploy their infrastructure at the edge of the network. This presents significant challenges:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							How do you handle deployments of many edge sites in parallel?
						</li><li class="listitem">
							What happens when you need to deploy sites in disconnected environments?
						</li><li class="listitem">
							How do you manage the lifecycle of large fleets of clusters?
						</li></ul></div><p>
					GitOps Zero Touch Provisioning (ZTP) and <span class="emphasis"><em>GitOps</em></span> meets these challenges by allowing you to provision remote edge sites at scale with declarative site definitions and configurations for bare-metal equipment. Template or overlay configurations install OpenShift Container Platform features that are required for CNF workloads. The full lifecycle of installation and upgrades is handled through the GitOps ZTP pipeline.
				</p><p>
					GitOps ZTP uses GitOps for infrastructure deployments. With GitOps, you use declarative YAML files and other defined patterns stored in Git repositories. Red Hat Advanced Cluster Management (RHACM) uses your Git repositories to drive the deployment of your infrastructure.
				</p><p>
					GitOps provides traceability, role-based access control (RBAC), and a single source of truth for the desired state of each site. Scalability issues are addressed by Git methodologies and event driven operations through webhooks.
				</p><p>
					You start the GitOps ZTP workflow by creating declarative site definition and configuration custom resources (CRs) that the GitOps ZTP pipeline delivers to the edge nodes.
				</p><p>
					The following diagram shows how GitOps ZTP works within the far edge framework.
				</p><div class="informalfigure"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Scalability_and_performance-en-US/images/e01c06c618b6911917ac01992c4d7163/217_OpenShift_Zero_Touch_Provisioning_updates_1022_1.png" alt="GitOps ZTP at the network far edge"/></div></div></section><section class="section" id="about-ztp_ztp-deploying-far-edge-clusters-at-scale"><div class="titlepage"><div><div><h3 class="title">16.1.2. Using GitOps ZTP to provision clusters at the network far edge</h3></div></div></div><p>
					Red Hat Advanced Cluster Management (RHACM) manages clusters in a hub-and-spoke architecture, where a single hub cluster manages many spoke clusters. Hub clusters running RHACM provision and deploy the managed clusters by using GitOps Zero Touch Provisioning (ZTP) and the assisted service that is deployed when you install RHACM.
				</p><p>
					The assisted service handles provisioning of OpenShift Container Platform on single node clusters, three-node clusters, or standard clusters running on bare metal.
				</p><p>
					A high-level overview of using GitOps ZTP to provision and maintain bare-metal hosts with OpenShift Container Platform is as follows:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							A hub cluster running RHACM manages an OpenShift image registry that mirrors the OpenShift Container Platform release images. RHACM uses the OpenShift image registry to provision the managed clusters.
						</li><li class="listitem">
							You manage the bare-metal hosts in a YAML format inventory file, versioned in a Git repository.
						</li><li class="listitem">
							You make the hosts ready for provisioning as managed clusters, and use RHACM and the assisted service to install the bare-metal hosts on site.
						</li></ul></div><p>
					Installing and deploying the clusters is a two-stage process, involving an initial installation phase, and a subsequent configuration phase. The following diagram illustrates this workflow:
				</p><div class="informalfigure"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Scalability_and_performance-en-US/images/9e02f04517d56a01e98004500d3e619c/217_OpenShift_Zero_Touch_Provisioning_updates_1022_2.png" alt="Using GitOps and GitOps ZTP to install and deploy managed clusters"/></div></div></section><section class="section" id="ztp-creating-ztp-crs-for-multiple-managed-clusters_ztp-deploying-far-edge-clusters-at-scale"><div class="titlepage"><div><div><h3 class="title">16.1.3. Installing managed clusters with SiteConfig resources and RHACM</h3></div></div></div><p>
					GitOps Zero Touch Provisioning (ZTP) uses <code class="literal">SiteConfig</code> custom resources (CRs) in a Git repository to manage the processes that install OpenShift Container Platform clusters. The <code class="literal">SiteConfig</code> CR contains cluster-specific parameters required for installation. It has options for applying select configuration CRs during installation including user defined extra manifests.
				</p><p>
					The GitOps ZTP plugin processes <code class="literal">SiteConfig</code> CRs to generate a collection of CRs on the hub cluster. This triggers the assisted service in Red Hat Advanced Cluster Management (RHACM) to install OpenShift Container Platform on the bare-metal host. You can find installation status and error messages in these CRs on the hub cluster.
				</p><p>
					You can provision single clusters manually or in batches with GitOps ZTP:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Provisioning a single cluster</span></dt><dd>
								Create a single <code class="literal">SiteConfig</code> CR and related installation and configuration CRs for the cluster, and apply them in the hub cluster to begin cluster provisioning. This is a good way to test your CRs before deploying on a larger scale.
							</dd><dt><span class="term">Provisioning many clusters</span></dt><dd>
								Install managed clusters in batches of up to 400 by defining <code class="literal">SiteConfig</code> and related CRs in a Git repository. ArgoCD uses the <code class="literal">SiteConfig</code> CRs to deploy the sites. The RHACM policy generator creates the manifests and applies them to the hub cluster. This starts the cluster provisioning process.
							</dd></dl></div></section><section class="section" id="ztp-configuring-cluster-policies_ztp-deploying-far-edge-clusters-at-scale"><div class="titlepage"><div><div><h3 class="title">16.1.4. Configuring managed clusters with policies and PolicyGenTemplate resources</h3></div></div></div><p>
					GitOps Zero Touch Provisioning (ZTP) uses Red Hat Advanced Cluster Management (RHACM) to configure clusters by using a policy-based governance approach to applying the configuration.
				</p><p>
					The policy generator or <code class="literal">PolicyGen</code> is a plugin for the GitOps Operator that enables the creation of RHACM policies from a concise template. The tool can combine multiple CRs into a single policy, and you can generate multiple policies that apply to various subsets of clusters in your fleet.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						For scalability and to reduce the complexity of managing configurations across the fleet of clusters, use configuration CRs with as much commonality as possible.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Where possible, apply configuration CRs using a fleet-wide common policy.
							</li><li class="listitem">
								The next preference is to create logical groupings of clusters to manage as much of the remaining configurations as possible under a group policy.
							</li><li class="listitem">
								When a configuration is unique to an individual site, use RHACM templating on the hub cluster to inject the site-specific data into a common or group policy. Alternatively, apply an individual site policy for the site.
							</li></ul></div></div></div><p>
					The following diagram shows how the policy generator interacts with GitOps and RHACM in the configuration phase of cluster deployment.
				</p><div class="informalfigure"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Scalability_and_performance-en-US/images/20ee6312e63aade50a51c024c2a6608c/217_OpenShift_Zero_Touch_Provisioning_updates_1022_3.png" alt="Policy generator"/></div></div><p>
					For large fleets of clusters, it is typical for there to be a high-level of consistency in the configuration of those clusters.
				</p><p>
					The following recommended structuring of policies combines configuration CRs to meet several goals:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Describe common configurations once and apply to the fleet.
						</li><li class="listitem">
							Minimize the number of maintained and managed policies.
						</li><li class="listitem">
							Support flexibility in common configurations for cluster variants.
						</li></ul></div><div class="table" id="idm139735326621712"><p class="title"><strong>Table 16.1. Recommended PolicyGenTemplate policy categories</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 17%; " class="col_1"><!--Empty--></col><col style="width: 83%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735326616176" scope="col">Policy category</th><th align="left" valign="top" id="idm139735326615088" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735326616176"> <p>
									Common
								</p>
								 </td><td align="left" valign="top" headers="idm139735326615088"> <p>
									A policy that exists in the common category is applied to all clusters in the fleet. Use common <code class="literal">PolicyGenTemplate</code> CRs to apply common installation settings across all cluster types.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735326616176"> <p>
									Groups
								</p>
								 </td><td align="left" valign="top" headers="idm139735326615088"> <p>
									A policy that exists in the groups category is applied to a group of clusters in the fleet. Use group <code class="literal">PolicyGenTemplate</code> CRs to manage specific aspects of single-node, three-node, and standard cluster installations. Cluster groups can also follow geographic region, hardware variant, etc.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735326616176"> <p>
									Sites
								</p>
								 </td><td align="left" valign="top" headers="idm139735326615088"> <p>
									A policy that exists in the sites category is applied to a specific cluster site. Any cluster can have its own specific policies maintained.
								</p>
								 </td></tr></tbody></table></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For more information about extracting the reference <code class="literal">SiteConfig</code> and <code class="literal">PolicyGenTemplate</code> CRs from the <code class="literal">ztp-site-generate</code> container image, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-preparing-the-ztp-git-repository_ztp-preparing-the-hub-cluster">Preparing the ZTP Git repository</a>.
						</li></ul></div></section></section><section class="section" id="ztp-preparing-the-hub-cluster"><div class="titlepage"><div><div><h2 class="title">16.2. Preparing the hub cluster for ZTP</h2></div></div></div><p>
				To use RHACM in a disconnected environment, create a mirror registry that mirrors the OpenShift Container Platform release images and Operator Lifecycle Manager (OLM) catalog that contains the required Operator images. OLM manages, installs, and upgrades Operators and their dependencies in the cluster. You can also use a disconnected mirror host to serve the RHCOS ISO and RootFS disk images that are used to provision the bare-metal hosts.
			</p><section class="section" id="ztp-telco-ran-software-versions_ztp-preparing-the-hub-cluster"><div class="titlepage"><div><div><h3 class="title">16.2.1. Telco RAN 4.13 validated solution software versions</h3></div></div></div><p>
					The Red Hat Telco Radio Access Network (RAN) version 4.13 solution has been validated using the following Red Hat software products.
				</p><div class="table" id="idm139735326868160"><p class="title"><strong>Table 16.2. Telco RAN 4.13 validated solution software</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735326650304" scope="col">Product</th><th align="left" valign="top" id="idm139735326649216" scope="col">Software version</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735326650304"> <p>
									Hub cluster OpenShift Container Platform version
								</p>
								 </td><td align="left" valign="top" headers="idm139735326649216"> <p>
									4.13
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735326650304"> <p>
									GitOps ZTP plugin
								</p>
								 </td><td align="left" valign="top" headers="idm139735326649216"> <p>
									4.11, 4.12, or 4.13
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735326650304"> <p>
									Red Hat Advanced Cluster Management (RHACM)
								</p>
								 </td><td align="left" valign="top" headers="idm139735326649216"> <p>
									2.7
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735326650304"> <p>
									Red Hat OpenShift GitOps
								</p>
								 </td><td align="left" valign="top" headers="idm139735326649216"> <p>
									1.9
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735326650304"> <p>
									Topology Aware Lifecycle Manager (TALM)
								</p>
								 </td><td align="left" valign="top" headers="idm139735326649216"> <p>
									4.11, 4.12, or 4.13
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section" id="ztp-gitops-ztp-max-spoke-clusters_ztp-preparing-the-hub-cluster"><div class="titlepage"><div><div><h3 class="title">16.2.2. Recommended hub cluster specifications and managed cluster limits for GitOps ZTP</h3></div></div></div><p>
					With GitOps Zero Touch Provisioning (ZTP), you can manage thousands of clusters in geographically dispersed regions and networks. The Red Hat Performance and Scale lab successfully created and managed 3500 virtual single-node OpenShift clusters with a reduced DU profile from a single Red Hat Advanced Cluster Management (RHACM) hub cluster in a lab environment.
				</p><p>
					In real-world situations, the scaling limits for the number of clusters that you can manage will vary depending on various factors affecting the hub cluster. For example:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Hub cluster resources</span></dt><dd>
								Available hub cluster host resources (CPU, memory, storage) are an important factor in determining how many clusters the hub cluster can manage. The more resources allocated to the hub cluster, the more managed clusters it can accommodate.
							</dd><dt><span class="term">Hub cluster storage</span></dt><dd>
								The hub cluster host storage IOPS rating and whether the hub cluster hosts use NVMe storage can affect hub cluster performance and the number of clusters it can manage.
							</dd><dt><span class="term">Network bandwidth and latency</span></dt><dd>
								Slow or high-latency network connections between the hub cluster and managed clusters can impact how the hub cluster manages multiple clusters.
							</dd><dt><span class="term">Managed cluster size and complexity</span></dt><dd>
								The size and complexity of the managed clusters also affects the capacity of the hub cluster. Larger managed clusters with more nodes, namespaces, and resources require additional processing and management resources. Similarly, clusters with complex configurations such as the RAN DU profile or diverse workloads can require more resources from the hub cluster.
							</dd><dt><span class="term">Number of managed policies</span></dt><dd>
								The number of policies managed by the hub cluster scaled over the number of managed clusters bound to those policies is an important factor that determines how many clusters can be managed.
							</dd><dt><span class="term">Monitoring and management workloads</span></dt><dd>
								RHACM continuously monitors and manages the managed clusters. The number and complexity of monitoring and management workloads running on the hub cluster can affect its capacity. Intensive monitoring or frequent reconciliation operations can require additional resources, potentially limiting the number of manageable clusters.
							</dd><dt><span class="term">RHACM version and configuration</span></dt><dd>
								Different versions of RHACM can have varying performance characteristics and resource requirements. Additionally, the configuration settings of RHACM, such as the number of concurrent reconciliations or the frequency of health checks, can affect the managed cluster capacity of the hub cluster.
							</dd></dl></div><p>
					Use the following representative configuration and network specifications to develop your own Hub cluster and network specifications.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The following guidelines are based on internal lab benchmark testing only and do not represent a complete real-world host specification.
					</p></div></div><div class="table" id="idm139735333923728"><p class="title"><strong>Table 16.3. Representative three-node hub cluster machine specifications</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735325602208" scope="col">Requirement</th><th align="left" valign="top" id="idm139735325601120" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735325602208"> <p>
									OpenShift Container Platform version
								</p>
								 </td><td align="left" valign="top" headers="idm139735325601120"> <p>
									version 4.13
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735325602208"> <p>
									RHACM version
								</p>
								 </td><td align="left" valign="top" headers="idm139735325601120"> <p>
									version 2.7
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735325602208"> <p>
									Topology Aware Lifecycle Manager (TALM)
								</p>
								 </td><td align="left" valign="top" headers="idm139735325601120"> <p>
									version 4.13
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735325602208"> <p>
									Server hardware
								</p>
								 </td><td align="left" valign="top" headers="idm139735325601120"> <p>
									3 x Dell PowerEdge R650 rack servers
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735325602208"> <p>
									NVMe hard disks
								</p>
								 </td><td align="left" valign="top" headers="idm139735325601120"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											50 GB disk for <code class="literal">/var/lib/etcd</code>
										</li><li class="listitem">
											2.9 TB disk for <code class="literal">/var/lib/containers</code>
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735325602208"> <p>
									SSD hard disks
								</p>
								 </td><td align="left" valign="top" headers="idm139735325601120"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											1 SSD split into 15 200GB thin-provisioned logical volumes provisioned as <code class="literal">PV</code> CRs
										</li><li class="listitem">
											1 SSD serving as an extra large <code class="literal">PV</code> resource
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735325602208"> <p>
									Number of applied DU profile policies
								</p>
								 </td><td align="left" valign="top" headers="idm139735325601120"> <p>
									5
								</p>
								 </td></tr></tbody></table></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The following network specifications are representative of a typical real-world RAN network and were applied to the scale lab environment during testing.
					</p></div></div><div class="table" id="idm139735327703952"><p class="title"><strong>Table 16.4. Simulated lab environment network specifications</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735327698352" scope="col">Specification</th><th align="left" valign="top" id="idm139735327697264" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735327698352"> <p>
									Round-trip time (RTT) latency
								</p>
								 </td><td align="left" valign="top" headers="idm139735327697264"> <p>
									50 ms
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735327698352"> <p>
									Packet loss
								</p>
								 </td><td align="left" valign="top" headers="idm139735327697264"> <p>
									0.02% packet loss
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735327698352"> <p>
									Network bandwidth limit
								</p>
								 </td><td align="left" valign="top" headers="idm139735327697264"> <p>
									20 Mbps
								</p>
								 </td></tr></tbody></table></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.7/html/install/installing#single-node">Creating and managing single-node OpenShift clusters with RHACM</a>
						</li></ul></div></section><section class="section" id="installing-disconnected-rhacm_ztp-preparing-the-hub-cluster"><div class="titlepage"><div><div><h3 class="title">16.2.3. Installing GitOps ZTP in a disconnected environment</h3></div></div></div><p>
					Use Red Hat Advanced Cluster Management (RHACM), Red Hat OpenShift GitOps, and Topology Aware Lifecycle Manager (TALM) on the hub cluster in the disconnected environment to manage the deployment of multiple managed clusters.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift Container Platform CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have logged in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem"><p class="simpara">
							You have configured a disconnected mirror registry for use in the cluster.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The disconnected mirror registry that you create must contain a version of TALM backup and pre-cache images that matches the version of TALM running in the hub cluster. The spoke cluster must be able to resolve these images in the disconnected mirror registry.
							</p></div></div></li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install RHACM in the hub cluster. See <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.8/html/install/installing#install-on-disconnected-networks">Installing RHACM in a disconnected environment</a>.
						</li><li class="listitem">
							Install GitOps and TALM in the hub cluster.
						</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://docs.openshift.com/gitops/latest/installing_gitops/installing-openshift-gitops.html#installing-openshift-gitops">Installing OpenShift GitOps</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#installing-topology-aware-lifecycle-manager-using-cli_cnf-topology-aware-lifecycle-manager">Installing TALM</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-mirror-catalog_olm-restricted-networks">Mirroring an Operator catalog</a>
						</li></ul></div></section><section class="section" id="ztp-acm-adding-images-to-mirror-registry_ztp-preparing-the-hub-cluster"><div class="titlepage"><div><div><h3 class="title">16.2.4. Adding RHCOS ISO and RootFS images to the disconnected mirror host</h3></div></div></div><p>
					Before you begin installing clusters in the disconnected environment with Red Hat Advanced Cluster Management (RHACM), you must first host Red Hat Enterprise Linux CoreOS (RHCOS) images for it to use. Use a disconnected mirror to host the RHCOS images.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Deploy and configure an HTTP server to host the RHCOS image resources on the network. You must be able to access the HTTP server from your computer, and from the machines that you create.
						</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The RHCOS images might not change with every release of OpenShift Container Platform. You must download images with the highest version that is less than or equal to the version that you install. Use the image versions that match your OpenShift Container Platform version if they are available. You require ISO and RootFS images to install RHCOS on the hosts. RHCOS QCOW2 images are not supported for this installation type.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to the mirror host.
						</li><li class="listitem"><p class="simpara">
							Obtain the RHCOS ISO and RootFS images from <a class="link" href="https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/">mirror.openshift.com</a>, for example:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Export the required image names and OpenShift Container Platform version as environment variables:
								</p><pre class="programlisting language-terminal">$ export ISO_IMAGE_NAME=&lt;iso_image_name&gt; <span id="CO44-1"><!--Empty--></span><span class="callout">1</span></pre><pre class="programlisting language-terminal">$ export ROOTFS_IMAGE_NAME=&lt;rootfs_image_name&gt; <span id="CO44-2"><!--Empty--></span><span class="callout">1</span></pre><pre class="programlisting language-terminal">$ export OCP_VERSION=&lt;ocp_version&gt; <span id="CO44-3"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO44-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											ISO image name, for example, <code class="literal">rhcos-4.13.1-x86_64-live.x86_64.iso</code>
										</div></dd><dt><a href="#CO44-2"><span class="callout">1</span></a> </dt><dd><div class="para">
											RootFS image name, for example, <code class="literal">rhcos-4.13.1-x86_64-live-rootfs.x86_64.img</code>
										</div></dd><dt><a href="#CO44-3"><span class="callout">1</span></a> </dt><dd><div class="para">
											OpenShift Container Platform version, for example, <code class="literal">4.13.1</code>
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Download the required images:
								</p><pre class="programlisting language-terminal">$ sudo wget https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.13/${OCP_VERSION}/${ISO_IMAGE_NAME} -O /var/www/html/${ISO_IMAGE_NAME}</pre><pre class="programlisting language-terminal">$ sudo wget https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.13/${OCP_VERSION}/${ROOTFS_IMAGE_NAME} -O /var/www/html/${ROOTFS_IMAGE_NAME}</pre></li></ol></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Verify that the images downloaded successfully and are being served on the disconnected mirror host, for example:
						</p><pre class="programlisting language-terminal">$ wget http://$(hostname)/${ISO_IMAGE_NAME}</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Saving to: rhcos-4.13.1-x86_64-live.x86_64.iso
rhcos-4.13.1-x86_64-live.x86_64.iso-  11%[====&gt;    ]  10.01M  4.71MB/s</pre>

							</p></div></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-mirroring-creating-registry">Creating a mirror registry</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-mirroring-installation-images">Mirroring images for a disconnected installation</a>
						</li></ul></div></section><section class="section" id="enabling-assisted-installer-service-on-bare-metal_ztp-preparing-the-hub-cluster"><div class="titlepage"><div><div><h3 class="title">16.2.5. Enabling the assisted service</h3></div></div></div><p>
					Red Hat Advanced Cluster Management (RHACM) uses the assisted service to deploy OpenShift Container Platform clusters. The assisted service is deployed automatically when you enable the MultiClusterHub Operator on Red Hat Advanced Cluster Management (RHACM). After that, you need to configure the <code class="literal">Provisioning</code> resource to watch all namespaces and to update the <code class="literal">AgentServiceConfig</code> custom resource (CR) with references to the ISO and RootFS images that are hosted on the mirror registry HTTP server.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have logged in to the hub cluster as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							You have RHACM with MultiClusterHub enabled.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Enable the <code class="literal">Provisioning</code> resource to watch all namespaces and configure mirrors for disconnected environments. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.8/html/clusters/cluster_mce_overview#enable-cim">Enabling the Central Infrastructure Management service</a>.
						</li><li class="listitem"><p class="simpara">
							Update the <code class="literal">AgentServiceConfig</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc edit AgentServiceConfig</pre></li><li class="listitem"><p class="simpara">
							Add the following entry to the <code class="literal">items.spec.osImages</code> field in the CR:
						</p><pre class="programlisting language-yaml">- cpuArchitecture: x86_64
    openshiftVersion: "4.13"
    rootFSUrl: https://&lt;host&gt;/&lt;path&gt;/rhcos-live-rootfs.x86_64.img
    url: https://&lt;mirror-registry&gt;/&lt;path&gt;/rhcos-live.x86_64.iso</pre><p class="simpara">
							where:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;host&gt; </span></dt><dd>
										Is the fully qualified domain name (FQDN) for the target mirror registry HTTP server.
									</dd><dt><span class="term">&lt;path&gt; </span></dt><dd>
										Is the path to the image on the target mirror registry.
									</dd></dl></div><p class="simpara">
							Save and quit the editor to apply the changes.
						</p></li></ol></div></section><section class="section" id="ztp-configuring-the-cluster-for-a-disconnected-environment_ztp-preparing-the-hub-cluster"><div class="titlepage"><div><div><h3 class="title">16.2.6. Configuring the hub cluster to use a disconnected mirror registry</h3></div></div></div><p>
					You can configure the hub cluster to use a disconnected mirror registry for a disconnected environment.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have a disconnected hub cluster installation with Red Hat Advanced Cluster Management (RHACM) 2.8 installed.
						</li><li class="listitem">
							You have hosted the <code class="literal">rootfs</code> and <code class="literal">iso</code> images on an HTTP server.
						</li></ul></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						If you enable TLS for the HTTP server, you must confirm the root certificate is signed by an authority trusted by the client and verify the trusted certificate chain between your OpenShift Container Platform hub and managed clusters and the HTTP server. Using a server configured with an untrusted certificate prevents the images from being downloaded to the image creation service. Using untrusted HTTPS servers is not supported.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">ConfigMap</code> containing the mirror registry config:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: assisted-installer-config-map
  namespace: "&lt;infrastructure_operator_namespace&gt;" <span id="CO45-1"><!--Empty--></span><span class="callout">1</span>
  labels:
    app: assisted-service
data:
  ca-bundle.crt: | <span id="CO45-2"><!--Empty--></span><span class="callout">2</span>
    -----BEGIN CERTIFICATE-----
    &lt;certificate_contents&gt;
    -----END CERTIFICATE-----

  registries.conf: | <span id="CO45-3"><!--Empty--></span><span class="callout">3</span>
    unqualified-search-registries = ["registry.access.redhat.com", "docker.io"]

    [[registry]]
       prefix = ""
       location = "quay.io/example-repository" <span id="CO45-4"><!--Empty--></span><span class="callout">4</span>
       mirror-by-digest-only = true

       [[registry.mirror]]
       location = "mirror1.registry.corp.com:5000/example-repository" <span id="CO45-5"><!--Empty--></span><span class="callout">5</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO45-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal">ConfigMap</code> namespace must be the same as the namespace of the Infrastructure Operator.
								</div></dd><dt><a href="#CO45-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The mirror registry’s certificate that is used when creating the mirror registry.
								</div></dd><dt><a href="#CO45-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The configuration file for the mirror registry. The mirror registry configuration adds mirror information to the <code class="literal">/etc/containers/registries.conf</code> file in the discovery image. The mirror information is stored in the <code class="literal">imageContentSources</code> section of the <code class="literal">install-config.yaml</code> file when the information is passed to the installation program. The Assisted Service pod that runs on the hub cluster fetches the container images from the configured mirror registry.
								</div></dd><dt><a href="#CO45-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The URL of the mirror registry. You must use the URL from the <code class="literal">imageContentSources</code> section by running the <code class="literal">oc adm release mirror</code> command when you configure the mirror registry. For more information, see the <span class="emphasis"><em>Mirroring the OpenShift Container Platform image repository</em></span> section.
								</div></dd><dt><a href="#CO45-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									The registries defined in the <code class="literal">registries.conf</code> file must be scoped by repository, not by registry. In this example, both the <code class="literal">quay.io/example-repository</code> and the <code class="literal">mirror1.registry.corp.com:5000/example-repository</code> repositories are scoped by the <code class="literal">example-repository</code> repository.
								</div></dd></dl></div><p class="simpara">
							This updates <code class="literal">mirrorRegistryRef</code> in the <code class="literal">AgentServiceConfig</code> custom resource, as shown below:
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: agent-install.openshift.io/v1beta1
kind: AgentServiceConfig
metadata:
  name: agent
spec:
  databaseStorage:
    volumeName: &lt;db_pv_name&gt;
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: &lt;db_storage_size&gt;
  filesystemStorage:
    volumeName: &lt;fs_pv_name&gt;
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: &lt;fs_storage_size&gt;
  mirrorRegistryRef:
    name: 'assisted-installer-mirror-config'
  osImages:
    - openshiftVersion: &lt;ocp_version&gt;
      url: &lt;iso_url&gt; <span id="CO46-1"><!--Empty--></span><span class="callout">1</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO46-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Must match the URL of the HTTPD server.
								</div></dd></dl></div></li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						A valid NTP server is required during cluster installation. Ensure that a suitable NTP server is available and can be reached from the installed clusters through the disconnected network.
					</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-mirror-repository_installing-mirroring-installation-images">Mirroring the OpenShift Container Platform image repository</a>
						</li></ul></div></section><section class="section" id="ztp-configuring-the-hub-cluster-to-use-unauthenticated-registries_ztp-preparing-the-hub-cluster"><div class="titlepage"><div><div><h3 class="title">16.2.7. Configuring the hub cluster to use unauthenticated registries</h3></div></div></div><p>
					You can configure the hub cluster to use unauthenticated registries. Unauthenticated registries does not require authentication to access and download images.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed and configured a hub cluster and installed Red Hat Advanced Cluster Management (RHACM) on the hub cluster.
						</li><li class="listitem">
							You have installed the OpenShift Container Platform CLI (oc).
						</li><li class="listitem">
							You have logged in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							You have configured an unauthenticated registry for use with the hub cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Update the <code class="literal">AgentServiceConfig</code> custom resource (CR) by running the following command:
						</p><pre class="programlisting language-terminal">$ oc edit AgentServiceConfig agent</pre></li><li class="listitem"><p class="simpara">
							Add the <code class="literal">unauthenticatedRegistries</code> field in the CR:
						</p><pre class="programlisting language-yaml">apiVersion: agent-install.openshift.io/v1beta1
kind: AgentServiceConfig
metadata:
  name: agent
spec:
  unauthenticatedRegistries:
  - example.registry.com
  - example.registry2.com
  ...</pre><p class="simpara">
							Unauthenticated registries are listed under <code class="literal">spec.unauthenticatedRegistries</code> in the <code class="literal">AgentServiceConfig</code> resource. Any registry on this list is not required to have an entry in the pull secret used for the spoke cluster installation. <code class="literal">assisted-service</code> validates the pull secret by making sure it contains the authentication information for every image registry used for installation.
						</p></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Mirror registries are automatically added to the ignore list and do not need to be added under <code class="literal">spec.unauthenticatedRegistries</code>. Specifying the <code class="literal">PUBLIC_CONTAINER_REGISTRIES</code> environment variable in the <code class="literal">ConfigMap</code> overrides the default values with the specified value. The <code class="literal">PUBLIC_CONTAINER_REGISTRIES</code> defaults are <a class="link" href="https://quay.io">quay.io</a> and <a class="link" href="https://registry.svc.ci.openshift.org">registry.svc.ci.openshift.org</a>.
					</p></div></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						Verify that you can access the newly added registry from the hub cluster by running the following commands:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Open a debug shell prompt to the hub cluster:
						</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Test access to the unauthenticated registry by running the following command:
						</p><pre class="programlisting language-terminal">sh-4.4# podman login -u kubeadmin -p $(oc whoami -t) &lt;unauthenticated_registry&gt;</pre><p class="simpara">
							where:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;unauthenticated_registry&gt;</span></dt><dd>
										Is the new registry, for example, <code class="literal">unauthenticated-image-registry.openshift-image-registry.svc:5000</code>.
									</dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Login Succeeded!</pre>

							</p></div></li></ol></div></section><section class="section" id="ztp-configuring-hub-cluster-with-argocd_ztp-preparing-the-hub-cluster"><div class="titlepage"><div><div><h3 class="title">16.2.8. Configuring the hub cluster with ArgoCD</h3></div></div></div><p>
					You can configure the hub cluster with a set of ArgoCD applications that generate the required installation and policy custom resources (CRs) for each site with GitOps Zero Touch Provisioning (ZTP).
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Red Hat Advanced Cluster Management (RHACM) uses <code class="literal">SiteConfig</code> CRs to generate the Day 1 managed cluster installation CRs for ArgoCD. Each ArgoCD application can manage a maximum of 300 <code class="literal">SiteConfig</code> CRs.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have a OpenShift Container Platform hub cluster with Red Hat Advanced Cluster Management (RHACM) and Red Hat OpenShift GitOps installed.
						</li><li class="listitem">
							You have extracted the reference deployment from the GitOps ZTP plugin container as described in the "Preparing the GitOps ZTP site configuration repository" section. Extracting the reference deployment creates the <code class="literal">out/argocd/deployment</code> directory referenced in the following procedure.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Prepare the ArgoCD pipeline configuration:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Create a Git repository with the directory structure similar to the example directory. For more information, see "Preparing the GitOps ZTP site configuration repository".
								</li><li class="listitem"><p class="simpara">
									Configure access to the repository using the ArgoCD UI. Under <span class="strong strong"><strong>Settings</strong></span> configure the following:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<span class="strong strong"><strong>Repositories</strong></span> - Add the connection information. The URL must end in <code class="literal">.git</code>, for example, <code class="literal"><a class="link" href="https://repo.example.com/repo.git">https://repo.example.com/repo.git</a></code> and credentials.
										</li><li class="listitem">
											<span class="strong strong"><strong>Certificates</strong></span> - Add the public certificate for the repository, if needed.
										</li></ul></div></li><li class="listitem"><p class="simpara">
									Modify the two ArgoCD applications, <code class="literal">out/argocd/deployment/clusters-app.yaml</code> and <code class="literal">out/argocd/deployment/policies-app.yaml</code>, based on your Git repository:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											Update the URL to point to the Git repository. The URL ends with <code class="literal">.git</code>, for example, <code class="literal"><a class="link" href="https://repo.example.com/repo.git">https://repo.example.com/repo.git</a></code>.
										</li><li class="listitem">
											The <code class="literal">targetRevision</code> indicates which Git repository branch to monitor.
										</li><li class="listitem">
											<code class="literal">path</code> specifies the path to the <code class="literal">SiteConfig</code> and <code class="literal">PolicyGenTemplate</code> CRs, respectively.
										</li></ul></div></li></ol></div></li><li class="listitem"><p class="simpara">
							To install the GitOps ZTP plugin you must patch the ArgoCD instance in the hub cluster by using the patch file previously extracted into the <code class="literal">out/argocd/deployment/</code> directory. Run the following command:
						</p><pre class="programlisting language-terminal">$ oc patch argocd openshift-gitops \
-n openshift-gitops --type=merge \
--patch-file out/argocd/deployment/argocd-openshift-gitops-patch.json</pre></li><li class="listitem">
							In RHACM 2.7 and later, the multicluster engine enables the <code class="literal">cluster-proxy-addon</code> feature by default. To disable this feature, apply the following patch to disable and remove the relevant hub cluster and managed cluster pods that are responsible for this add-on.
						</li></ol></div><pre class="programlisting language-terminal">$ oc patch multiclusterengines.multicluster.openshift.io multiclusterengine --type=merge --patch-file out/argocd/deployment/disable-cluster-proxy-addon.json</pre><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Apply the pipeline configuration to your hub cluster by using the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -k out/argocd/deployment</pre></li></ol></div></section><section class="section" id="ztp-preparing-the-ztp-git-repository_ztp-preparing-the-hub-cluster"><div class="titlepage"><div><div><h3 class="title">16.2.9. Preparing the GitOps ZTP site configuration repository</h3></div></div></div><p>
					Before you can use the GitOps Zero Touch Provisioning (ZTP) pipeline, you need to prepare the Git repository to host the site configuration data.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have configured the hub cluster GitOps applications for generating the required installation and policy custom resources (CRs).
						</li><li class="listitem">
							You have deployed the managed clusters using GitOps ZTP.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Create a directory structure with separate paths for the <code class="literal">SiteConfig</code> and <code class="literal">PolicyGenTemplate</code> CRs.
						</li><li class="listitem"><p class="simpara">
							Export the <code class="literal">argocd</code> directory from the <code class="literal">ztp-site-generate</code> container image using the following commands:
						</p><pre class="programlisting language-terminal">$ podman pull registry.redhat.io/openshift4/ztp-site-generate-rhel8:v4.13</pre><pre class="programlisting language-terminal">$ mkdir -p ./out</pre><pre class="programlisting language-terminal">$ podman run --log-driver=none --rm registry.redhat.io/openshift4/ztp-site-generate-rhel8:v4.13 extract /home/ztp --tar | tar x -C ./out</pre></li><li class="listitem"><p class="simpara">
							Check that the <code class="literal">out</code> directory contains the following subdirectories:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">out/extra-manifest</code> contains the source CR files that <code class="literal">SiteConfig</code> uses to generate extra manifest <code class="literal">configMap</code>.
								</li><li class="listitem">
									<code class="literal">out/source-crs</code> contains the source CR files that <code class="literal">PolicyGenTemplate</code> uses to generate the Red Hat Advanced Cluster Management (RHACM) policies.
								</li><li class="listitem">
									<code class="literal">out/argocd/deployment</code> contains patches and YAML files to apply on the hub cluster for use in the next step of this procedure.
								</li><li class="listitem">
									<code class="literal">out/argocd/example</code> contains the examples for <code class="literal">SiteConfig</code> and <code class="literal">PolicyGenTemplate</code> files that represent the recommended configuration.
								</li></ul></div></li></ol></div><p>
					The directory structure under <code class="literal">out/argocd/example</code> serves as a reference for the structure and content of your Git repository. The example includes <code class="literal">SiteConfig</code> and <code class="literal">PolicyGenTemplate</code> reference CRs for single-node, three-node, and standard clusters. Remove references to cluster types that you are not using. The following example describes a set of CRs for a network of single-node clusters:
				</p><pre class="programlisting language-text">example
├── policygentemplates
│   ├── common-ranGen.yaml
│   ├── example-sno-site.yaml
│   ├── group-du-sno-ranGen.yaml
│   ├── group-du-sno-validator-ranGen.yaml
│   ├── kustomization.yaml
│   └── ns.yaml
└── siteconfig
    ├── example-sno.yaml
    ├── KlusterletAddonConfigOverride.yaml
    └── kustomization.yaml</pre><p>
					Keep <code class="literal">SiteConfig</code> and <code class="literal">PolicyGenTemplate</code> CRs in separate directories. Both the <code class="literal">SiteConfig</code> and <code class="literal">PolicyGenTemplate</code> directories must contain a <code class="literal">kustomization.yaml</code> file that explicitly includes the files in that directory.
				</p><p>
					This directory structure and the <code class="literal">kustomization.yaml</code> files must be committed and pushed to your Git repository. The initial push to Git should include the <code class="literal">kustomization.yaml</code> files. The <code class="literal">SiteConfig</code> (<code class="literal">example-sno.yaml</code>) and <code class="literal">PolicyGenTemplate</code> (<code class="literal">common-ranGen.yaml</code>, <code class="literal">group-du-sno*.yaml</code>, and <code class="literal">example-sno-site.yaml</code>) files can be omitted and pushed at a later time as required when deploying a site.
				</p><p>
					The <code class="literal">KlusterletAddonConfigOverride.yaml</code> file is only required if one or more <code class="literal">SiteConfig</code> CRs which make reference to it are committed and pushed to Git. See <code class="literal">example-sno.yaml</code> for an example of how this is used.
				</p></section></section><section class="section" id="ztp-deploying-far-edge-sites"><div class="titlepage"><div><div><h2 class="title">16.3. Installing managed clusters with RHACM and SiteConfig resources</h2></div></div></div><p>
				You can provision OpenShift Container Platform clusters at scale with Red Hat Advanced Cluster Management (RHACM) using the assisted service and the GitOps plugin policy generator with core-reduction technology enabled. The GitOps Zero Touch Provisioning (ZTP) pipeline performs the cluster installations. GitOps ZTP can be used in a disconnected environment.
			</p><section class="section" id="ztp-talo-integration_ztp-deploying-far-edge-sites"><div class="titlepage"><div><div><h3 class="title">16.3.1. GitOps ZTP and Topology Aware Lifecycle Manager</h3></div></div></div><p>
					GitOps Zero Touch Provisioning (ZTP) generates installation and configuration CRs from manifests stored in Git. These artifacts are applied to a centralized hub cluster where Red Hat Advanced Cluster Management (RHACM), the assisted service, and the Topology Aware Lifecycle Manager (TALM) use the CRs to install and configure the managed cluster. The configuration phase of the GitOps ZTP pipeline uses the TALM to orchestrate the application of the configuration CRs to the cluster. There are several key integration points between GitOps ZTP and the TALM.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Inform policies</span></dt><dd>
								By default, GitOps ZTP creates all policies with a remediation action of <code class="literal">inform</code>. These policies cause RHACM to report on compliance status of clusters relevant to the policies but does not apply the desired configuration. During the GitOps ZTP process, after OpenShift installation, the TALM steps through the created <code class="literal">inform</code> policies and enforces them on the target managed cluster(s). This applies the configuration to the managed cluster. Outside of the GitOps ZTP phase of the cluster lifecycle, this allows you to change policies without the risk of immediately rolling those changes out to affected managed clusters. You can control the timing and the set of remediated clusters by using TALM.
							</dd><dt><span class="term">Automatic creation of ClusterGroupUpgrade CRs</span></dt><dd><p class="simpara">
								To automate the initial configuration of newly deployed clusters, TALM monitors the state of all <code class="literal">ManagedCluster</code> CRs on the hub cluster. Any <code class="literal">ManagedCluster</code> CR that does not have a <code class="literal">ztp-done</code> label applied, including newly created <code class="literal">ManagedCluster</code> CRs, causes the TALM to automatically create a <code class="literal">ClusterGroupUpgrade</code> CR with the following characteristics:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										The <code class="literal">ClusterGroupUpgrade</code> CR is created and enabled in the <code class="literal">ztp-install</code> namespace.
									</li><li class="listitem">
										<code class="literal">ClusterGroupUpgrade</code> CR has the same name as the <code class="literal">ManagedCluster</code> CR.
									</li><li class="listitem">
										The cluster selector includes only the cluster associated with that <code class="literal">ManagedCluster</code> CR.
									</li><li class="listitem">
										The set of managed policies includes all policies that RHACM has bound to the cluster at the time the <code class="literal">ClusterGroupUpgrade</code> is created.
									</li><li class="listitem">
										Pre-caching is disabled.
									</li><li class="listitem">
										Timeout set to 4 hours (240 minutes).
									</li></ul></div><p class="simpara">
								The automatic creation of an enabled <code class="literal">ClusterGroupUpgrade</code> ensures that initial zero-touch deployment of clusters proceeds without the need for user intervention. Additionally, the automatic creation of a <code class="literal">ClusterGroupUpgrade</code> CR for any <code class="literal">ManagedCluster</code> without the <code class="literal">ztp-done</code> label allows a failed GitOps ZTP installation to be restarted by simply deleting the <code class="literal">ClusterGroupUpgrade</code> CR for the cluster.
							</p></dd><dt><span class="term">Waves</span></dt><dd><p class="simpara">
								Each policy generated from a <code class="literal">PolicyGenTemplate</code> CR includes a <code class="literal">ztp-deploy-wave</code> annotation. This annotation is based on the same annotation from each CR which is included in that policy. The wave annotation is used to order the policies in the auto-generated <code class="literal">ClusterGroupUpgrade</code> CR. The wave annotation is not used other than for the auto-generated <code class="literal">ClusterGroupUpgrade</code> CR.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									All CRs in the same policy must have the same setting for the <code class="literal">ztp-deploy-wave</code> annotation. The default value of this annotation for each CR can be overridden in the <code class="literal">PolicyGenTemplate</code>. The wave annotation in the source CR is used for determining and setting the policy wave annotation. This annotation is removed from each built CR which is included in the generated policy at runtime.
								</p></div></div><p class="simpara">
								The TALM applies the configuration policies in the order specified by the wave annotations. The TALM waits for each policy to be compliant before moving to the next policy. It is important to ensure that the wave annotation for each CR takes into account any prerequisites for those CRs to be applied to the cluster. For example, an Operator must be installed before or concurrently with the configuration for the Operator. Similarly, the <code class="literal">CatalogSource</code> for an Operator must be installed in a wave before or concurrently with the Operator Subscription. The default wave value for each CR takes these prerequisites into account.
							</p><p class="simpara">
								Multiple CRs and policies can share the same wave number. Having fewer policies can result in faster deployments and lower CPU usage. It is a best practice to group many CRs into relatively few waves.
							</p></dd></dl></div><p>
					To check the default wave value in each source CR, run the following command against the <code class="literal">out/source-crs</code> directory that is extracted from the <code class="literal">ztp-site-generate</code> container image:
				</p><pre class="programlisting language-terminal">$ grep -r "ztp-deploy-wave" out/source-crs</pre><div class="variablelist"><dl class="variablelist"><dt><span class="term">Phase labels</span></dt><dd><p class="simpara">
								The <code class="literal">ClusterGroupUpgrade</code> CR is automatically created and includes directives to annotate the <code class="literal">ManagedCluster</code> CR with labels at the start and end of the GitOps ZTP process.
							</p><p class="simpara">
								When GitOps ZTP configuration post-installation commences, the <code class="literal">ManagedCluster</code> has the <code class="literal">ztp-running</code> label applied. When all policies are remediated to the cluster and are fully compliant, these directives cause the TALM to remove the <code class="literal">ztp-running</code> label and apply the <code class="literal">ztp-done</code> label.
							</p><p class="simpara">
								For deployments that make use of the <code class="literal">informDuValidator</code> policy, the <code class="literal">ztp-done</code> label is applied when the cluster is fully ready for deployment of applications. This includes all reconciliation and resulting effects of the GitOps ZTP applied configuration CRs. The <code class="literal">ztp-done</code> label affects automatic <code class="literal">ClusterGroupUpgrade</code> CR creation by TALM. Do not manipulate this label after the initial GitOps ZTP installation of the cluster.
							</p></dd><dt><span class="term">Linked CRs</span></dt><dd>
								The automatically created <code class="literal">ClusterGroupUpgrade</code> CR has the owner reference set as the <code class="literal">ManagedCluster</code> from which it was derived. This reference ensures that deleting the <code class="literal">ManagedCluster</code> CR causes the instance of the <code class="literal">ClusterGroupUpgrade</code> to be deleted along with any supporting resources.
							</dd></dl></div></section><section class="section" id="ztp-ztp-building-blocks_ztp-deploying-far-edge-sites"><div class="titlepage"><div><div><h3 class="title">16.3.2. Overview of deploying managed clusters with GitOps ZTP</h3></div></div></div><p>
					Red Hat Advanced Cluster Management (RHACM) uses GitOps Zero Touch Provisioning (ZTP) to deploy single-node OpenShift Container Platform clusters, three-node clusters, and standard clusters. You manage site configuration data as OpenShift Container Platform custom resources (CRs) in a Git repository. GitOps ZTP uses a declarative GitOps approach for a develop once, deploy anywhere model to deploy the managed clusters.
				</p><p>
					The deployment of the clusters includes:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Installing the host operating system (RHCOS) on a blank server
						</li><li class="listitem">
							Deploying OpenShift Container Platform
						</li><li class="listitem">
							Creating cluster policies and site subscriptions
						</li><li class="listitem">
							Making the necessary network configurations to the server operating system
						</li><li class="listitem">
							Deploying profile Operators and performing any needed software-related configuration, such as performance profile, PTP, and SR-IOV
						</li></ul></div><h5 id="ztp-overview-managed-site-installation-process_ztp-deploying-far-edge-sites">Overview of the managed site installation process</h5><p>
					After you apply the managed site custom resources (CRs) on the hub cluster, the following actions happen automatically:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							A Discovery image ISO file is generated and booted on the target host.
						</li><li class="listitem">
							When the ISO file successfully boots on the target host it reports the host hardware information to RHACM.
						</li><li class="listitem">
							After all hosts are discovered, OpenShift Container Platform is installed.
						</li><li class="listitem">
							When OpenShift Container Platform finishes installing, the hub installs the <code class="literal">klusterlet</code> service on the target cluster.
						</li><li class="listitem">
							The requested add-on services are installed on the target cluster.
						</li></ol></div><p>
					The Discovery image ISO process is complete when the <code class="literal">Agent</code> CR for the managed cluster is created on the hub cluster.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The target bare-metal host must meet the networking, firmware, and hardware requirements listed in <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#sno-configure-for-vdu">Recommended single-node OpenShift cluster configuration for vDU application workloads</a>.
					</p></div></div></section><section class="section" id="ztp-creating-the-site-secrets_ztp-deploying-far-edge-sites"><div class="titlepage"><div><div><h3 class="title">16.3.3. Creating the managed bare-metal host secrets</h3></div></div></div><p>
					Add the required <code class="literal">Secret</code> custom resources (CRs) for the managed bare-metal host to the hub cluster. You need a secret for the GitOps Zero Touch Provisioning (ZTP) pipeline to access the Baseboard Management Controller (BMC) and a secret for the assisted installer service to pull cluster installation images from the registry.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The secrets are referenced from the <code class="literal">SiteConfig</code> CR by name. The namespace must match the <code class="literal">SiteConfig</code> namespace.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a YAML secret file containing credentials for the host Baseboard Management Controller (BMC) and a pull secret required for installing OpenShift and all add-on cluster Operators:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Save the following YAML as the file <code class="literal">example-sno-secret.yaml</code>:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: example-sno-bmc-secret
  namespace: example-sno <span id="CO47-1"><!--Empty--></span><span class="callout">1</span>
data: <span id="CO47-2"><!--Empty--></span><span class="callout">2</span>
  password: &lt;base64_password&gt;
  username: &lt;base64_username&gt;
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  name: pull-secret
  namespace: example-sno  <span id="CO47-3"><!--Empty--></span><span class="callout">3</span>
data:
  .dockerconfigjson: &lt;pull_secret&gt; <span id="CO47-4"><!--Empty--></span><span class="callout">4</span>
type: kubernetes.io/dockerconfigjson</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO47-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Must match the namespace configured in the related <code class="literal">SiteConfig</code> CR
										</div></dd><dt><a href="#CO47-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Base64-encoded values for <code class="literal">password</code> and <code class="literal">username</code>
										</div></dd><dt><a href="#CO47-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Must match the namespace configured in the related <code class="literal">SiteConfig</code> CR
										</div></dd><dt><a href="#CO47-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Base64-encoded pull secret
										</div></dd></dl></div></li></ol></div></li><li class="listitem">
							Add the relative path to <code class="literal">example-sno-secret.yaml</code> to the <code class="literal">kustomization.yaml</code> file that you use to install the cluster.
						</li></ol></div></section><section class="section" id="setting-managed-bare-metal-host-kernel-arguments_ztp-deploying-far-edge-sites"><div class="titlepage"><div><div><h3 class="title">16.3.4. Configuring Discovery ISO kernel arguments for installations using GitOps ZTP</h3></div></div></div><p>
					The GitOps Zero Touch Provisioning (ZTP) workflow uses the Discovery ISO as part of the OpenShift Container Platform installation process on managed bare-metal hosts. You can edit the <code class="literal">InfraEnv</code> resource to specify kernel arguments for the Discovery ISO. This is useful for cluster installations with specific environmental requirements. For example, configure the <code class="literal">rd.net.timeout.carrier</code> kernel argument for the Discovery ISO to facilitate static networking for the cluster or to receive a DHCP address before downloading the root file system during installation.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						In OpenShift Container Platform 4.13, you can only add kernel arguments. You can not replace or delete kernel arguments.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (oc).
						</li><li class="listitem">
							You have logged in to the hub cluster as a user with cluster-admin privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the <code class="literal">InfraEnv</code> CR and edit the <code class="literal">spec.kernelArguments</code> specification to configure kernel arguments.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Save the following YAML in an <code class="literal">InfraEnv-example.yaml</code> file:
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										The <code class="literal">InfraEnv</code> CR in this example uses template syntax such as <code class="literal">{{ .Cluster.ClusterName }}</code> that is populated based on values in the <code class="literal">SiteConfig</code> CR. The <code class="literal">SiteConfig</code> CR automatically populates values for these templates during deployment. Do not edit the templates manually.
									</p></div></div><pre class="programlisting language-yaml">apiVersion: agent-install.openshift.io/v1beta1
kind: InfraEnv
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "1"
  name: "{{ .Cluster.ClusterName }}"
  namespace: "{{ .Cluster.ClusterName }}"
spec:
  clusterRef:
    name: "{{ .Cluster.ClusterName }}"
    namespace: "{{ .Cluster.ClusterName }}"
  kernelArguments:
    - operation: append <span id="CO48-1"><!--Empty--></span><span class="callout">1</span>
      value: audit=0 <span id="CO48-2"><!--Empty--></span><span class="callout">2</span>
    - operation: append
      value: trace=1
  sshAuthorizedKey: "{{ .Site.SshPublicKey }}"
  proxy: "{{ .Cluster.ProxySettings }}"
  pullSecretRef:
    name: "{{ .Site.PullSecretRef.Name }}"
  ignitionConfigOverride: "{{ .Cluster.IgnitionConfigOverride }}"
  nmStateConfigLabelSelector:
    matchLabels:
      nmstate-label: "{{ .Cluster.ClusterName }}"
  additionalNTPSources: "{{ .Cluster.AdditionalNTPSources }}"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO48-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the append operation to add a kernel argument.
										</div></dd><dt><a href="#CO48-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Specify the kernel argument you want to configure. This example configures the audit kernel argument and the trace kernel argument.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Commit the <code class="literal">InfraEnv-example.yaml</code> CR to the same location in your Git repository that has the <code class="literal">SiteConfig</code> CR and push your changes. The following example shows a sample Git repository structure:
						</p><pre class="programlisting language-text">~/example-ztp/install
          └── site-install
               ├── siteconfig-example.yaml
               ├── InfraEnv-example.yaml
               ...</pre></li><li class="listitem"><p class="simpara">
							Edit the <code class="literal">spec.clusters.crTemplates</code> specification in the <code class="literal">SiteConfig</code> CR to reference the <code class="literal">InfraEnv-example.yaml</code> CR in your Git repository:
						</p><pre class="programlisting language-yaml white-space-pre white-space-pre">clusters:
  crTemplates:
    InfraEnv: "InfraEnv-example.yaml"</pre><p class="simpara">
							When you are ready to deploy your cluster by committing and pushing the <code class="literal">SiteConfig</code> CR, the build pipeline uses the custom <code class="literal">InfraEnv-example</code> CR in your Git repository to configure the infrastructure environment, including the custom kernel arguments.
						</p></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						To verify that the kernel arguments are applied, after the Discovery image verifies that OpenShift Container Platform is ready for installation, you can SSH to the target host before the installation process begins. At that point, you can view the kernel arguments for the Discovery ISO in the <code class="literal">/proc/cmdline</code> file.
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Begin an SSH session with the target host:
						</p><pre class="programlisting language-terminal">$ ssh -i /path/to/privatekey core@&lt;host_name&gt;</pre></li><li class="listitem"><p class="simpara">
							View the system’s kernel arguments by using the following command:
						</p><pre class="programlisting language-terminal">$ cat /proc/cmdline</pre></li></ol></div></section><section class="section" id="ztp-deploying-a-site_ztp-deploying-far-edge-sites"><div class="titlepage"><div><div><h3 class="title">16.3.5. Deploying a managed cluster with SiteConfig and GitOps ZTP</h3></div></div></div><p>
					Use the following procedure to create a <code class="literal">SiteConfig</code> custom resource (CR) and related files and initiate the GitOps Zero Touch Provisioning (ZTP) cluster deployment.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have logged in to the hub cluster as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							You configured the hub cluster for generating the required installation and policy CRs.
						</li><li class="listitem"><p class="simpara">
							You created a Git repository where you manage your custom site configuration data. The repository must be accessible from the hub cluster and you must configure it as a source repository for the ArgoCD application. See "Preparing the GitOps ZTP site configuration repository" for more information.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								When you create the source repository, ensure that you patch the ArgoCD application with the <code class="literal">argocd/deployment/argocd-openshift-gitops-patch.json</code> patch-file that you extract from the <code class="literal">ztp-site-generate</code> container. See "Configuring the hub cluster with ArgoCD".
							</p></div></div></li><li class="listitem"><p class="simpara">
							To be ready for provisioning managed clusters, you require the following for each bare-metal host:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Network connectivity</span></dt><dd>
										Your network requires DNS. Managed cluster hosts should be reachable from the hub cluster. Ensure that Layer 3 connectivity exists between the hub cluster and the managed cluster host.
									</dd><dt><span class="term">Baseboard Management Controller (BMC) details</span></dt><dd>
										GitOps ZTP uses BMC username and password details to connect to the BMC during cluster installation. The GitOps ZTP plugin manages the <code class="literal">ManagedCluster</code> CRs on the hub cluster based on the <code class="literal">SiteConfig</code> CR in your site Git repo. You create individual <code class="literal">BMCSecret</code> CRs for each host manually.
									</dd></dl></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the required managed cluster secrets on the hub cluster. These resources must be in a namespace with a name matching the cluster name. For example, in <code class="literal">out/argocd/example/siteconfig/example-sno.yaml</code>, the cluster name and namespace is <code class="literal">example-sno</code>.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Export the cluster namespace by running the following command:
								</p><pre class="programlisting language-terminal">$ export CLUSTERNS=example-sno</pre></li><li class="listitem"><p class="simpara">
									Create the namespace:
								</p><pre class="programlisting language-terminal">$ oc create namespace $CLUSTERNS</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create pull secret and BMC <code class="literal">Secret</code> CRs for the managed cluster. The pull secret must contain all the credentials necessary for installing OpenShift Container Platform and all required Operators. See "Creating the managed bare-metal host secrets" for more information.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The secrets are referenced from the <code class="literal">SiteConfig</code> custom resource (CR) by name. The namespace must match the <code class="literal">SiteConfig</code> namespace.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">SiteConfig</code> CR for your cluster in your local clone of the Git repository:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Choose the appropriate example for your CR from the <code class="literal">out/argocd/example/siteconfig/</code> folder. The folder includes example files for single node, three-node, and standard clusters:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">example-sno.yaml</code>
										</li><li class="listitem">
											<code class="literal">example-3node.yaml</code>
										</li><li class="listitem">
											<code class="literal">example-standard.yaml</code>
										</li></ul></div></li><li class="listitem"><p class="simpara">
									Change the cluster and host details in the example file to match the type of cluster you want. For example:
								</p><div class="formalpara"><p class="title"><strong>Example single-node OpenShift SiteConfig CR</strong></p><p>
										
<pre class="programlisting language-yaml"># example-node1-bmh-secret &amp; assisted-deployment-pull-secret need to be created under same namespace example-sno
---
apiVersion: ran.openshift.io/v1
kind: SiteConfig
metadata:
  name: "example-sno"
  namespace: "example-sno"
spec:
  baseDomain: "example.com"
  cpuPartitioningMode: AllNodes
  pullSecretRef:
    name: "assisted-deployment-pull-secret"
  clusterImageSetNameRef: "openshift-4.10"
  sshPublicKey: "ssh-rsa AAAA..."
  clusters:
  - clusterName: "example-sno"
    networkType: "OVNKubernetes"
    installConfigOverrides: |
      {
        "capabilities": {
          "baselineCapabilitySet": "None",
          "additionalEnabledCapabilities": [
            "marketplace",
            "NodeTuning"
          ]
        }
      }
    clusterLabels:
      common: true
      group-du-sno: ""
      sites : "example-sno"
    clusterNetwork:
      - cidr: 1001:1::/48
        hostPrefix: 64
    machineNetwork:
      - cidr: 1111:2222:3333:4444::/64
    serviceNetwork:
      - 1001:2::/112
    additionalNTPSources:
      - 1111:2222:3333:4444::2
    # crTemplates:
    #   KlusterletAddonConfig: "KlusterletAddonConfigOverride.yaml"
    nodes:
      - hostName: "example-node1.example.com"
        role: "master"
        bmcAddress: "idrac-virtualmedia+https://[1111:2222:3333:4444::bbbb:1]/redfish/v1/Systems/System.Embedded.1"
        bmcCredentialsName:
          name: "example-node1-bmh-secret"
        bootMACAddress: "AA:BB:CC:DD:EE:11"
        bootMode: "UEFI"
        rootDeviceHints:
          wwn: "0x11111000000asd123"
        # diskPartition:
        #   - device: /dev/disk/by-id/wwn-0x11111000000asd123 # match rootDeviceHints
        #     partitions:
        #       - mount_point: /var/imageregistry
        #         size: 102500
        #         start: 344844
        ignitionConfigOverride: |
          {
            "ignition": {
              "version": "3.2.0"
            },
            "storage": {
              "disks": [
                {
                  "device": "/dev/disk/by-id/wwn-0x11111000000asd123",
                  "wipeTable": false,
                  "partitions": [
                    {
                      "sizeMiB": 16,
                      "label": "httpevent1",
                      "startMiB": 350000
                    },
                    {
                      "sizeMiB": 16,
                      "label": "httpevent2",
                      "startMiB": 350016
                    }
                  ]
                }
              ],
              "filesystem": [
                {
                  "device": "/dev/disk/by-partlabel/httpevent1",
                  "format": "xfs",
                  "wipeFilesystem": true
                },
                {
                  "device": "/dev/disk/by-partlabel/httpevent2",
                  "format": "xfs",
                  "wipeFilesystem": true
                }
              ]
            }
          }
        nodeNetwork:
          interfaces:
            - name: eno1
              macAddress: "AA:BB:CC:DD:EE:11"
          config:
            interfaces:
              - name: eno1
                type: ethernet
                state: up
                ipv4:
                  enabled: false
                ipv6:
                  enabled: true
                  address:
                  - ip: 1111:2222:3333:4444::aaaa:1
                    prefix-length: 64
            dns-resolver:
              config:
                search:
                - example.com
                server:
                - 1111:2222:3333:4444::2
            routes:
              config:
              - destination: ::/0
                next-hop-interface: eno1
                next-hop-address: 1111:2222:3333:4444::1
                table-id: 254</pre>

									</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										For more information about BMC addressing, see the "Additional resources" section.
									</p></div></div></li><li class="listitem">
									You can inspect the default set of extra-manifest <code class="literal">MachineConfig</code> CRs in <code class="literal">out/argocd/extra-manifest</code>. It is automatically applied to the cluster when it is installed.
								</li><li class="listitem"><p class="simpara">
									Optional: To provision additional install-time manifests on the provisioned cluster, create a directory in your Git repository, for example, <code class="literal">sno-extra-manifest/</code>, and add your custom manifest CRs to this directory. If your <code class="literal">SiteConfig.yaml</code> refers to this directory in the <code class="literal">extraManifestPath</code> field, any CRs in this referenced directory are appended to the default set of extra manifests.
								</p><div class="admonition important"><div class="admonition_header">Enabling the crun OCI container runtime</div><div><p>
										For optimal cluster performance, enable crun for master and worker nodes in single-node OpenShift, single-node OpenShift with additional worker nodes, three-node OpenShift, and standard clusters.
									</p><p>
										Enable crun in a <code class="literal">ContainerRuntimeConfig</code> CR as an additional Day 0 install-time manifest to avoid the cluster having to reboot.
									</p><p>
										The <code class="literal">enable-crun-master.yaml</code> and <code class="literal">enable-crun-worker.yaml</code> CR files are in the <code class="literal">out/source-crs/optional-extra-manifest/</code> folder that you can extract from the <code class="literal">ztp-site-generate</code> container. For more information, see "Customizing extra installation manifests in the GitOps ZTP pipeline".
									</p></div></div></li></ol></div></li><li class="listitem">
							Add the <code class="literal">SiteConfig</code> CR to the <code class="literal">kustomization.yaml</code> file in the <code class="literal">generators</code> section, similar to the example shown in <code class="literal">out/argocd/example/siteconfig/kustomization.yaml</code>.
						</li><li class="listitem"><p class="simpara">
							Commit the <code class="literal">SiteConfig</code> CR and associated <code class="literal">kustomization.yaml</code> changes in your Git repository and push the changes.
						</p><p class="simpara">
							The ArgoCD pipeline detects the changes and begins the managed cluster deployment.
						</p></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-sno-siteconfig-config-reference_ztp-deploying-far-edge-sites">Single-node OpenShift SiteConfig CR installation reference</a>
						</li></ul></div><section class="section" id="ztp-sno-siteconfig-config-reference_ztp-deploying-far-edge-sites"><div class="titlepage"><div><div><h4 class="title">16.3.5.1. Single-node OpenShift SiteConfig CR installation reference</h4></div></div></div><div class="table" id="idm139735324281696"><p class="title"><strong>Table 16.5. SiteConfig CR installation options for single-node OpenShift clusters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 75%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735324276816" scope="col">SiteConfig CR field</th><th align="left" valign="top" id="idm139735324275728" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735324276816"> <p>
										<code class="literal">spec.cpuPartitioningMode</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324275728"> <p>
										Configure workload partitioning by setting the value for <code class="literal">cpuPartitioningMode</code> to <code class="literal">AllNodes</code>. To complete the configuration, specify the <code class="literal">isolated</code> and <code class="literal">reserved</code> CPUs in the <code class="literal">PerformanceProfile</code> CR.
									</p>
									 <div class="admonition note"><div class="admonition_header">Note</div><div><p>
											Configuring workload partitioning by using the <code class="literal">cpuPartitioningMode</code> field in the <code class="literal">SiteConfig</code> CR is a Tech Preview feature in OpenShift Container Platform 4.13.
										</p></div></div>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735324276816"> <p>
										<code class="literal">metadata.name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324275728"> <p>
										Set <code class="literal">name</code> to <code class="literal">assisted-deployment-pull-secret</code> and create the <code class="literal">assisted-deployment-pull-secret</code> CR in the same namespace as the <code class="literal">SiteConfig</code> CR.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735324276816"> <p>
										<code class="literal">clusterImageSetNameRef</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324275728"> <p>
										Configure the image set available on the hub cluster. To see the list of supported versions on your hub cluster, run <code class="literal">oc get clusterimagesets</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735324276816"> <p>
										<code class="literal">installConfigOverrides</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324275728"> <p>
										Set the <code class="literal">installConfigOverrides</code> field to enable or disable optional components prior to cluster installation.
									</p>
									 <div class="admonition important"><div class="admonition_header">Important</div><div><p>
											Use the reference configuration as specified in the example <code class="literal">SiteConfig</code> CR. Adding additional components back into the system might require additional reserved CPU capacity.
										</p></div></div>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735324276816"> <p>
										<code class="literal">spec.clusters.clusterLabels</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324275728"> <p>
										Configure cluster labels to correspond to the <code class="literal">bindingRules</code> field in the <code class="literal">PolicyGenTemplate</code> CRs that you define. For example, <code class="literal">policygentemplates/common-ranGen.yaml</code> applies to all clusters with <code class="literal">common: true</code> set, <code class="literal">policygentemplates/group-du-sno-ranGen.yaml</code> applies to all clusters with <code class="literal">group-du-sno: ""</code> set.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735324276816"> <p>
										<code class="literal">spec.clusters.crTemplates.KlusterletAddonConfig</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324275728"> <p>
										Optional. Set <code class="literal">KlusterletAddonConfig</code> to <code class="literal">KlusterletAddonConfigOverride.yaml to override the default `KlusterletAddonConfig</code> that is created for the cluster.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735324276816"> <p>
										<code class="literal">spec.clusters.nodes.hostName</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324275728"> <p>
										For single-node deployments, define a single host. For three-node deployments, define three hosts. For standard deployments, define three hosts with <code class="literal">role: master</code> and two or more hosts defined with <code class="literal">role: worker</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735324276816"> <p>
										<code class="literal">spec.clusters.nodes.bmcAddress</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324275728"> <p>
										BMC address that you use to access the host. Applies to all cluster types. GitOps ZTP supports iPXE and virtual media booting by using Redfish or IPMI protocols. To use iPXE booting, you must use RHACM 2.8 or later. For more information about BMC addressing, see the "Additional resources" section.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735324276816"> <p>
										<code class="literal">spec.clusters.nodes.bmcAddress</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324275728"> <p>
										BMC address that you use to access the host. Applies to all cluster types. GitOps ZTP supports iPXE and virtual media booting by using Redfish or IPMI protocols. To use iPXE booting, you must use RHACM 2.8 or later. For more information about BMC addressing, see the "Additional resources" section.
									</p>
									 <div class="admonition note"><div class="admonition_header">Note</div><div><p>
											In far edge Telco use cases, only virtual media is supported for use with GitOps ZTP.
										</p></div></div>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735324276816"> <p>
										<code class="literal">spec.clusters.nodes.bmcCredentialsName</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324275728"> <p>
										Configure the <code class="literal">bmh-secret</code> CR that you separately create with the host BMC credentials. When creating the <code class="literal">bmh-secret</code> CR, use the same namespace as the <code class="literal">SiteConfig</code> CR that provisions the host.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735324276816"> <p>
										<code class="literal">spec.clusters.nodes.bootMode</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324275728"> <p>
										Set the boot mode for the host to <code class="literal">UEFI</code>. The default value is <code class="literal">UEFI</code>. Use <code class="literal">UEFISecureBoot</code> to enable secure boot on the host.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735324276816"> <p>
										<code class="literal">spec.clusters.nodes.rootDeviceHints</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324275728"> <p>
										Specifies the device for deployment. Identifiers that are stable across reboots are recommended, for example, <code class="literal">wwn: &lt;disk_wwn&gt;</code> or <code class="literal">deviceName: /dev/disk/by-path/&lt;device_path&gt;</code>. For a detailed list of stable identifiers, see the "About root device hints section".
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735324276816"> <p>
										<code class="literal">spec.clusters.nodes.diskPartition</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324275728"> <p>
										Optional. The provided example <code class="literal">diskPartition</code> is used to configure additional disk partitions.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735324276816"> <p>
										<code class="literal">spec.clusters.nodes.ignitionConfigOverride</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324275728"> <p>
										Optional. Use this field to assign partitions for persistent storage. Adjust disk ID and size to the specific hardware.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735324276816"> <p>
										<code class="literal">spec.clusters.nodes.cpuset</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324275728"> <p>
										Configure <code class="literal">cpuset</code> to match value that you set in the cluster <code class="literal">PerformanceProfile</code> CR <code class="literal">spec.cpu.reserved</code> field for workload partitioning.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735324276816"> <p>
										<code class="literal">spec.clusters.nodes.nodeNetwork</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324275728"> <p>
										Configure the network settings for the node.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735324276816"> <p>
										<code class="literal">spec.clusters.nodes.nodeNetwork.config.interfaces.ipv6</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324275728"> <p>
										Configure the IPv6 address for the host. For single-node OpenShift clusters with static IP addresses, the node-specific API and Ingress IPs should be the same.
									</p>
									 </td></tr></tbody></table></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-customizing-the-install-extra-manifests_ztp-advanced-install-ztp">Customizing extra installation manifests in the GitOps ZTP pipeline</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-preparing-the-ztp-git-repository_ztp-preparing-the-hub-cluster">Preparing the GitOps ZTP site configuration repository</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-configuring-hub-cluster-with-argocd_ztp-preparing-the-hub-cluster">Configuring the hub cluster with ArgoCD</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-creating-a-validator-inform-policy_ztp-advanced-policy-config">Signalling ZTP cluster deployment completion with validator inform policies</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-creating-the-site-secrets_ztp-manual-install">Creating the managed bare-metal host secrets</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#bmc-addressing_ipi-install-installation-workflow">BMC addressing</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#root-device-hints_preparing-to-install-with-agent-based-installer">About root device hints</a>
							</li></ul></div></section></section><section class="section" id="ztp-monitoring-deployment-progress_ztp-deploying-far-edge-sites"><div class="titlepage"><div><div><h3 class="title">16.3.6. Monitoring managed cluster installation progress</h3></div></div></div><p>
					The ArgoCD pipeline uses the <code class="literal">SiteConfig</code> CR to generate the cluster configuration CRs and syncs it with the hub cluster. You can monitor the progress of the synchronization in the ArgoCD dashboard.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have logged in to the hub cluster as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						When the synchronization is complete, the installation generally proceeds as follows:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							The Assisted Service Operator installs OpenShift Container Platform on the cluster. You can monitor the progress of cluster installation from the RHACM dashboard or from the command line by running the following commands:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Export the cluster name:
								</p><pre class="programlisting language-terminal">$ export CLUSTER=&lt;clusterName&gt;</pre></li><li class="listitem"><p class="simpara">
									Query the <code class="literal">AgentClusterInstall</code> CR for the managed cluster:
								</p><pre class="programlisting language-terminal">$ oc get agentclusterinstall -n $CLUSTER $CLUSTER -o jsonpath='{.status.conditions[?(@.type=="Completed")]}' | jq</pre></li><li class="listitem"><p class="simpara">
									Get the installation events for the cluster:
								</p><pre class="programlisting language-terminal">$ curl -sk $(oc get agentclusterinstall -n $CLUSTER $CLUSTER -o jsonpath='{.status.debugInfo.eventsURL}')  | jq '.[-2,-1]'</pre></li></ol></div></li></ol></div></section><section class="section" id="ztp-troubleshooting-ztp-gitops-installation-crs_ztp-deploying-far-edge-sites"><div class="titlepage"><div><div><h3 class="title">16.3.7. Troubleshooting GitOps ZTP by validating the installation CRs</h3></div></div></div><p>
					The ArgoCD pipeline uses the <code class="literal">SiteConfig</code> and <code class="literal">PolicyGenTemplate</code> custom resources (CRs) to generate the cluster configuration CRs and Red Hat Advanced Cluster Management (RHACM) policies. Use the following steps to troubleshoot issues that might occur during this process.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have logged in to the hub cluster as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check that the installation CRs were created by using the following command:
						</p><pre class="programlisting language-terminal">$ oc get AgentClusterInstall -n &lt;cluster_name&gt;</pre><p class="simpara">
							If no object is returned, use the following steps to troubleshoot the ArgoCD pipeline flow from <code class="literal">SiteConfig</code> files to the installation CRs.
						</p></li><li class="listitem"><p class="simpara">
							Verify that the <code class="literal">ManagedCluster</code> CR was generated using the <code class="literal">SiteConfig</code> CR on the hub cluster:
						</p><pre class="programlisting language-terminal">$ oc get managedcluster</pre></li><li class="listitem"><p class="simpara">
							If the <code class="literal">ManagedCluster</code> is missing, check if the <code class="literal">clusters</code> application failed to synchronize the files from the Git repository to the hub cluster:
						</p><pre class="programlisting language-terminal">$ oc describe -n openshift-gitops application clusters</pre><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Check for the <code class="literal">Status.Conditions</code> field to view the error logs for the managed cluster. For example, setting an invalid value for <code class="literal">extraManifestPath:</code> in the <code class="literal">SiteConfig</code> CR raises the following error:
								</p><pre class="programlisting language-text">Status:
  Conditions:
    Last Transition Time:  2021-11-26T17:21:39Z
    Message:               rpc error: code = Unknown desc = `kustomize build /tmp/https___git.com/ran-sites/siteconfigs/ --enable-alpha-plugins` failed exit status 1: 2021/11/26 17:21:40 Error could not create extra-manifest ranSite1.extra-manifest3 stat extra-manifest3: no such file or directory 2021/11/26 17:21:40 Error: could not build the entire SiteConfig defined by /tmp/kust-plugin-config-913473579: stat extra-manifest3: no such file or directory Error: failure in plugin configured via /tmp/kust-plugin-config-913473579; exit status 1: exit status 1
    Type:  ComparisonError</pre></li><li class="listitem"><p class="simpara">
									Check the <code class="literal">Status.Sync</code> field. If there are log errors, the <code class="literal">Status.Sync</code> field could indicate an <code class="literal">Unknown</code> error:
								</p><pre class="programlisting language-text">Status:
  Sync:
    Compared To:
      Destination:
        Namespace:  clusters-sub
        Server:     https://kubernetes.default.svc
      Source:
        Path:             sites-config
        Repo URL:         https://git.com/ran-sites/siteconfigs/.git
        Target Revision:  master
    Status:               Unknown</pre></li></ol></div></li></ol></div></section><section class="section" id="ztp-site-cleanup_ztp-deploying-far-edge-sites"><div class="titlepage"><div><div><h3 class="title">16.3.8. Removing a managed cluster site from the GitOps ZTP pipeline</h3></div></div></div><p>
					You can remove a managed site and the associated installation and configuration policy CRs from the GitOps Zero Touch Provisioning (ZTP) pipeline.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have logged in to the hub cluster as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Remove a site and the associated CRs by removing the associated <code class="literal">SiteConfig</code> and <code class="literal">PolicyGenTemplate</code> files from the <code class="literal">kustomization.yaml</code> file.
						</p><p class="simpara">
							When you run the GitOps ZTP pipeline again, the generated CRs are removed.
						</p></li><li class="listitem">
							Optional: If you want to permanently remove a site, you should also remove the <code class="literal">SiteConfig</code> and site-specific <code class="literal">PolicyGenTemplate</code> files from the Git repository.
						</li><li class="listitem">
							Optional: If you want to remove a site temporarily, for example when redeploying a site, you can leave the <code class="literal">SiteConfig</code> and site-specific <code class="literal">PolicyGenTemplate</code> CRs in the Git repository.
						</li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						After removing the <code class="literal">SiteConfig</code> file from the Git repository, if the corresponding clusters get stuck in the detach process, check Red Hat Advanced Cluster Management (RHACM) on the hub cluster for information about cleaning up the detached cluster.
					</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For information about removing a cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.8/html/clusters/cluster_mce_overview#remove-managed-cluster">Removing a cluster from management</a>.
						</li></ul></div></section><section class="section" id="ztp-removing-obsolete-content_ztp-deploying-far-edge-sites"><div class="titlepage"><div><div><h3 class="title">16.3.9. Removing obsolete content from the GitOps ZTP pipeline</h3></div></div></div><p>
					If a change to the <code class="literal">PolicyGenTemplate</code> configuration results in obsolete policies, for example, if you rename policies, use the following procedure to remove the obsolete policies.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have logged in to the hub cluster as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Remove the affected <code class="literal">PolicyGenTemplate</code> files from the Git repository, commit and push to the remote repository.
						</li><li class="listitem">
							Wait for the changes to synchronize through the application and the affected policies to be removed from the hub cluster.
						</li><li class="listitem"><p class="simpara">
							Add the updated <code class="literal">PolicyGenTemplate</code> files back to the Git repository, and then commit and push to the remote repository.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Removing GitOps Zero Touch Provisioning (ZTP) policies from the Git repository, and as a result also removing them from the hub cluster, does not affect the configuration of the managed cluster. The policy and CRs managed by that policy remains in place on the managed cluster.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Optional: As an alternative, after making changes to <code class="literal">PolicyGenTemplate</code> CRs that result in obsolete policies, you can remove these policies from the hub cluster manually. You can delete policies from the RHACM console using the <span class="strong strong"><strong>Governance</strong></span> tab or by running the following command:
						</p><pre class="programlisting language-terminal">$ oc delete policy -n &lt;namespace&gt; &lt;policy_name&gt;</pre></li></ol></div></section><section class="section" id="ztp-tearing-down-the-pipeline_ztp-deploying-far-edge-sites"><div class="titlepage"><div><div><h3 class="title">16.3.10. Tearing down the GitOps ZTP pipeline</h3></div></div></div><p>
					You can remove the ArgoCD pipeline and all generated GitOps Zero Touch Provisioning (ZTP) artifacts.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have logged in to the hub cluster as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Detach all clusters from Red Hat Advanced Cluster Management (RHACM) on the hub cluster.
						</li><li class="listitem"><p class="simpara">
							Delete the <code class="literal">kustomization.yaml</code> file in the <code class="literal">deployment</code> directory using the following command:
						</p><pre class="programlisting language-terminal">$ oc delete -k out/argocd/deployment</pre></li><li class="listitem">
							Commit and push your changes to the site repository.
						</li></ol></div></section></section><section class="section" id="ztp-configuring-managed-clusters-policies"><div class="titlepage"><div><div><h2 class="title">16.4. Configuring managed clusters with policies and PolicyGenTemplate resources</h2></div></div></div><p>
				Applied policy custom resources (CRs) configure the managed clusters that you provision. You can customize how Red Hat Advanced Cluster Management (RHACM) uses <code class="literal">PolicyGenTemplate</code> CRs to generate the applied policy CRs.
			</p><section class="section" id="ztp-the-policygentemplate_ztp-configuring-managed-clusters-policies"><div class="titlepage"><div><div><h3 class="title">16.4.1. About the PolicyGenTemplate CRD</h3></div></div></div><p>
					The <code class="literal">PolicyGenTemplate</code> custom resource definition (CRD) tells the <code class="literal">PolicyGen</code> policy generator what custom resources (CRs) to include in the cluster configuration, how to combine the CRs into the generated policies, and what items in those CRs need to be updated with overlay content.
				</p><p>
					The following example shows a <code class="literal">PolicyGenTemplate</code> CR (<code class="literal">common-du-ranGen.yaml</code>) extracted from the <code class="literal">ztp-site-generate</code> reference container. The <code class="literal">common-du-ranGen.yaml</code> file defines two Red Hat Advanced Cluster Management (RHACM) policies. The polices manage a collection of configuration CRs, one for each unique value of <code class="literal">policyName</code> in the CR. <code class="literal">common-du-ranGen.yaml</code> creates a single placement binding and a placement rule to bind the policies to clusters based on the labels listed in the <code class="literal">bindingRules</code> section.
				</p><div class="formalpara"><p class="title"><strong>Example PolicyGenTemplate CR - common-du-ranGen.yaml</strong></p><p>
						
<pre class="programlisting language-yaml">---
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "common"
  namespace: "ztp-common"
spec:
  bindingRules:
    common: "true" <span id="CO49-1"><!--Empty--></span><span class="callout">1</span>
  sourceFiles: <span id="CO49-2"><!--Empty--></span><span class="callout">2</span>
    - fileName: SriovSubscription.yaml
      policyName: "subscriptions-policy"
    - fileName: SriovSubscriptionNS.yaml
      policyName: "subscriptions-policy"
    - fileName: SriovSubscriptionOperGroup.yaml
      policyName: "subscriptions-policy"
    - fileName: SriovOperatorStatus.yaml
      policyName: "subscriptions-policy"
    - fileName: PtpSubscription.yaml
      policyName: "subscriptions-policy"
    - fileName: PtpSubscriptionNS.yaml
      policyName: "subscriptions-policy"
    - fileName: PtpSubscriptionOperGroup.yaml
      policyName: "subscriptions-policy"
    - fileName: PtpOperatorStatus.yaml
      policyName: "subscriptions-policy"
    - fileName: ClusterLogNS.yaml
      policyName: "subscriptions-policy"
    - fileName: ClusterLogOperGroup.yaml
      policyName: "subscriptions-policy"
    - fileName: ClusterLogSubscription.yaml
      policyName: "subscriptions-policy"
    - fileName: ClusterLogOperatorStatus.yaml
      policyName: "subscriptions-policy"
    - fileName: StorageNS.yaml
      policyName: "subscriptions-policy"
    - fileName: StorageOperGroup.yaml
      policyName: "subscriptions-policy"
    - fileName: StorageSubscription.yaml
      policyName: "subscriptions-policy"
    - fileName: StorageOperatorStatus.yaml
      policyName: "subscriptions-policy"
    - fileName: ReduceMonitoringFootprint.yaml
      policyName: "config-policy"
    - fileName: OperatorHub.yaml <span id="CO49-3"><!--Empty--></span><span class="callout">3</span>
      policyName: "config-policy"
    - fileName: DefaultCatsrc.yaml <span id="CO49-4"><!--Empty--></span><span class="callout">4</span>
      policyName: "config-policy" <span id="CO49-5"><!--Empty--></span><span class="callout">5</span>
      metadata:
        name: redhat-operators
      spec:
        displayName: disconnected-redhat-operators
        image: registry.example.com:5000/disconnected-redhat-operators/disconnected-redhat-operator-index:v4.9
    - fileName: DisconnectedICSP.yaml
      policyName: "config-policy"
      spec:
        repositoryDigestMirrors:
        - mirrors:
          - registry.example.com:5000
          source: registry.redhat.io</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO49-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							<code class="literal">common: "true"</code> applies the policies to all clusters with this label.
						</div></dd><dt><a href="#CO49-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Files listed under <code class="literal">sourceFiles</code> create the Operator policies for installed clusters.
						</div></dd><dt><a href="#CO49-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							<code class="literal">OperatorHub.yaml</code> configures the OperatorHub for the disconnected registry.
						</div></dd><dt><a href="#CO49-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							<code class="literal">DefaultCatsrc.yaml</code> configures the catalog source for the disconnected registry.
						</div></dd><dt><a href="#CO49-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							<code class="literal">policyName: "config-policy"</code> configures Operator subscriptions. The <code class="literal">OperatorHub</code> CR disables the default and this CR replaces <code class="literal">redhat-operators</code> with a <code class="literal">CatalogSource</code> CR that points to the disconnected registry.
						</div></dd></dl></div><p>
					A <code class="literal">PolicyGenTemplate</code> CR can be constructed with any number of included CRs. Apply the following example CR in the hub cluster to generate a policy containing a single CR:
				</p><pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "group-du-sno"
  namespace: "ztp-group"
spec:
  bindingRules:
    group-du-sno: ""
  mcp: "master"
  sourceFiles:
    - fileName: PtpConfigSlave.yaml
      policyName: "config-policy"
      metadata:
        name: "du-ptp-slave"
      spec:
        profile:
        - name: "slave"
          interface: "ens5f0"
          ptp4lOpts: "-2 -s --summary_interval -4"
          phc2sysOpts: "-a -r -n 24"</pre><p>
					Using the source file <code class="literal">PtpConfigSlave.yaml</code> as an example, the file defines a <code class="literal">PtpConfig</code> CR. The generated policy for the <code class="literal">PtpConfigSlave</code> example is named <code class="literal">group-du-sno-config-policy</code>. The <code class="literal">PtpConfig</code> CR defined in the generated <code class="literal">group-du-sno-config-policy</code> is named <code class="literal">du-ptp-slave</code>. The <code class="literal">spec</code> defined in <code class="literal">PtpConfigSlave.yaml</code> is placed under <code class="literal">du-ptp-slave</code> along with the other <code class="literal">spec</code> items defined under the source file.
				</p><p>
					The following example shows the <code class="literal">group-du-sno-config-policy</code> CR:
				</p><pre class="programlisting language-yaml">apiVersion: policy.open-cluster-management.io/v1
kind: Policy
metadata:
  name: group-du-ptp-config-policy
  namespace: groups-sub
  annotations:
    policy.open-cluster-management.io/categories: CM Configuration Management
    policy.open-cluster-management.io/controls: CM-2 Baseline Configuration
    policy.open-cluster-management.io/standards: NIST SP 800-53
spec:
    remediationAction: inform
    disabled: false
    policy-templates:
        - objectDefinition:
            apiVersion: policy.open-cluster-management.io/v1
            kind: ConfigurationPolicy
            metadata:
                name: group-du-ptp-config-policy-config
            spec:
                remediationAction: inform
                severity: low
                namespaceselector:
                    exclude:
                        - kube-*
                    include:
                        - '*'
                object-templates:
                    - complianceType: musthave
                      objectDefinition:
                        apiVersion: ptp.openshift.io/v1
                        kind: PtpConfig
                        metadata:
                            name: du-ptp-slave
                            namespace: openshift-ptp
                        spec:
                            recommend:
                                - match:
                                - nodeLabel: node-role.kubernetes.io/worker-du
                                  priority: 4
                                  profile: slave
                            profile:
                                - interface: ens5f0
                                  name: slave
                                  phc2sysOpts: -a -r -n 24
                                  ptp4lConf: |
                                    [global]
                                    #
                                    # Default Data Set
                                    #
                                    twoStepFlag 1
                                    slaveOnly 0
                                    priority1 128
                                    priority2 128
                                    domainNumber 24
                                    .....</pre></section><section class="section" id="ztp-pgt-config-best-practices_ztp-configuring-managed-clusters-policies"><div class="titlepage"><div><div><h3 class="title">16.4.2. Recommendations when customizing PolicyGenTemplate CRs</h3></div></div></div><p>
					Consider the following best practices when customizing site configuration <code class="literal">PolicyGenTemplate</code> custom resources (CRs):
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Use as few policies as are necessary. Using fewer policies requires less resources. Each additional policy creates overhead for the hub cluster and the deployed managed cluster. CRs are combined into policies based on the <code class="literal">policyName</code> field in the <code class="literal">PolicyGenTemplate</code> CR. CRs in the same <code class="literal">PolicyGenTemplate</code> which have the same value for <code class="literal">policyName</code> are managed under a single policy.
						</li><li class="listitem">
							In disconnected environments, use a single catalog source for all Operators by configuring the registry as a single index containing all Operators. Each additional <code class="literal">CatalogSource</code> CR on the managed clusters increases CPU usage.
						</li><li class="listitem">
							<code class="literal">MachineConfig</code> CRs should be included as <code class="literal">extraManifests</code> in the <code class="literal">SiteConfig</code> CR so that they are applied during installation. This can reduce the overall time taken until the cluster is ready to deploy applications.
						</li><li class="listitem">
							<code class="literal">PolicyGenTemplates</code> should override the channel field to explicitly identify the desired version. This ensures that changes in the source CR during upgrades does not update the generated subscription.
						</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For recommendations about scaling clusters with RHACM, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.6/html/install/installing#performance-and-scalability">Performance and scalability</a>.
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						When managing large numbers of spoke clusters on the hub cluster, minimize the number of policies to reduce resource consumption.
					</p><p>
						Grouping multiple configuration CRs into a single or limited number of policies is one way to reduce the overall number of policies on the hub cluster. When using the common, group, and site hierarchy of policies for managing site configuration, it is especially important to combine site-specific configuration into a single policy.
					</p></div></div></section><section class="section" id="ztp-policygentemplates-for-ran_ztp-configuring-managed-clusters-policies"><div class="titlepage"><div><div><h3 class="title">16.4.3. PolicyGenTemplate CRs for RAN deployments</h3></div></div></div><p>
					Use <code class="literal">PolicyGenTemplate</code> (PGT) custom resources (CRs) to customize the configuration applied to the cluster by using the GitOps Zero Touch Provisioning (ZTP) pipeline. The PGT CR allows you to generate one or more policies to manage the set of configuration CRs on your fleet of clusters. The PGT identifies the set of managed CRs, bundles them into policies, builds the policy wrapping around those CRs, and associates the policies with clusters by using label binding rules.
				</p><p>
					The reference configuration, obtained from the GitOps ZTP container, is designed to provide a set of critical features and node tuning settings that ensure the cluster can support the stringent performance and resource utilization constraints typical of RAN (Radio Access Network) Distributed Unit (DU) applications. Changes or omissions from the baseline configuration can affect feature availability, performance, and resource utilization. Use the reference <code class="literal">PolicyGenTemplate</code> CRs as the basis to create a hierarchy of configuration files tailored to your specific site requirements.
				</p><p>
					The baseline <code class="literal">PolicyGenTemplate</code> CRs that are defined for RAN DU cluster configuration can be extracted from the GitOps ZTP <code class="literal">ztp-site-generate</code> container. See "Preparing the GitOps ZTP site configuration repository" for further details.
				</p><p>
					The <code class="literal">PolicyGenTemplate</code> CRs can be found in the <code class="literal">./out/argocd/example/policygentemplates</code> folder. The reference architecture has common, group, and site-specific configuration CRs. Each <code class="literal">PolicyGenTemplate</code> CR refers to other CRs that can be found in the <code class="literal">./out/source-crs</code> folder.
				</p><p>
					The <code class="literal">PolicyGenTemplate</code> CRs relevant to RAN cluster configuration are described below. Variants are provided for the group <code class="literal">PolicyGenTemplate</code> CRs to account for differences in single-node, three-node compact, and standard cluster configurations. Similarly, site-specific configuration variants are provided for single-node clusters and multi-node (compact or standard) clusters. Use the group and site-specific configuration variants that are relevant for your deployment.
				</p><div class="table" id="idm139735323219952"><p class="title"><strong>Table 16.6. PolicyGenTemplate CRs for RAN deployments</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735323215088" scope="col">PolicyGenTemplate CR</th><th align="left" valign="top" id="idm139735323214000" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735323215088"> <p>
									<code class="literal">example-multinode-site.yaml</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735323214000"> <p>
									Contains a set of CRs that get applied to multi-node clusters. These CRs configure SR-IOV features typical for RAN installations.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735323215088"> <p>
									<code class="literal">example-sno-site.yaml</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735323214000"> <p>
									Contains a set of CRs that get applied to single-node OpenShift clusters. These CRs configure SR-IOV features typical for RAN installations.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735323215088"> <p>
									<code class="literal">common-ranGen.yaml</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735323214000"> <p>
									Contains a set of common RAN CRs that get applied to all clusters. These CRs subscribe to a set of operators providing cluster features typical for RAN as well as baseline cluster tuning.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735323215088"> <p>
									<code class="literal">group-du-3node-ranGen.yaml</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735323214000"> <p>
									Contains the RAN policies for three-node clusters only.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735323215088"> <p>
									<code class="literal">group-du-sno-ranGen.yaml</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735323214000"> <p>
									Contains the RAN policies for single-node clusters only.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735323215088"> <p>
									<code class="literal">group-du-standard-ranGen.yaml</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735323214000"> <p>
									Contains the RAN policies for standard three control-plane clusters.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735323215088"> <p>
									<code class="literal">group-du-3node-validator-ranGen.yaml</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735323214000"> <p>
									<code class="literal">PolicyGenTemplate</code> CR used to generate the various policies required for three-node clusters.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735323215088"> <p>
									<code class="literal">group-du-standard-validator-ranGen.yaml</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735323214000"> <p>
									<code class="literal">PolicyGenTemplate</code> CR used to generate the various policies required for standard clusters.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735323215088"> <p>
									<code class="literal">group-du-sno-validator-ranGen.yaml</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735323214000"> <p>
									<code class="literal">PolicyGenTemplate</code> CR used to generate the various policies required for single-node OpenShift clusters.
								</p>
								 </td></tr></tbody></table></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-preparing-the-ztp-git-repository_ztp-preparing-the-hub-cluster">Preparing the GitOps ZTP site configuration repository</a>
						</li></ul></div></section><section class="section" id="ztp-customizing-a-managed-site-using-pgt_ztp-configuring-managed-clusters-policies"><div class="titlepage"><div><div><h3 class="title">16.4.4. Customizing a managed cluster with PolicyGenTemplate CRs</h3></div></div></div><p>
					Use the following procedure to customize the policies that get applied to the managed cluster that you provision using the GitOps Zero Touch Provisioning (ZTP) pipeline.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have logged in to the hub cluster as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							You configured the hub cluster for generating the required installation and policy CRs.
						</li><li class="listitem">
							You created a Git repository where you manage your custom site configuration data. The repository must be accessible from the hub cluster and be defined as a source repository for the Argo CD application.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">PolicyGenTemplate</code> CR for site-specific configuration CRs.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Choose the appropriate example for your CR from the <code class="literal">out/argocd/example/policygentemplates</code> folder, for example, <code class="literal">example-sno-site.yaml</code> or <code class="literal">example-multinode-site.yaml</code>.
								</li><li class="listitem"><p class="simpara">
									Change the <code class="literal">bindingRules</code> field in the example file to match the site-specific label included in the <code class="literal">SiteConfig</code> CR. In the example <code class="literal">SiteConfig</code> file, the site-specific label is <code class="literal">sites: example-sno</code>.
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Ensure that the labels defined in your <code class="literal">PolicyGenTemplate</code> <code class="literal">bindingRules</code> field correspond to the labels that are defined in the related managed clusters <code class="literal">SiteConfig</code> CR.
									</p></div></div></li><li class="listitem">
									Change the content in the example file to match the desired configuration.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Optional: Create a <code class="literal">PolicyGenTemplate</code> CR for any common configuration CRs that apply to the entire fleet of clusters.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Select the appropriate example for your CR from the <code class="literal">out/argocd/example/policygentemplates</code> folder, for example, <code class="literal">common-ranGen.yaml</code>.
								</li><li class="listitem">
									Change the content in the example file to match the desired configuration.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Optional: Create a <code class="literal">PolicyGenTemplate</code> CR for any group configuration CRs that apply to the certain groups of clusters in the fleet.
						</p><p class="simpara">
							Ensure that the content of the overlaid spec files matches your desired end state. As a reference, the out/source-crs directory contains the full list of source-crs available to be included and overlaid by your PolicyGenTemplate templates.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Depending on the specific requirements of your clusters, you might need more than a single group policy per cluster type, especially considering that the example group policies each have a single PerformancePolicy.yaml file that can only be shared across a set of clusters if those clusters consist of identical hardware configurations.
							</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Select the appropriate example for your CR from the <code class="literal">out/argocd/example/policygentemplates</code> folder, for example, <code class="literal">group-du-sno-ranGen.yaml</code>.
								</li><li class="listitem">
									Change the content in the example file to match the desired configuration.
								</li></ol></div></li><li class="listitem">
							Optional. Create a validator inform policy <code class="literal">PolicyGenTemplate</code> CR to signal when the GitOps ZTP installation and configuration of the deployed cluster is complete. For more information, see "Creating a validator inform policy".
						</li><li class="listitem"><p class="simpara">
							Define all the policy namespaces in a YAML file similar to the example <code class="literal">out/argocd/example/policygentemplates/ns.yaml</code> file.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Do not include the <code class="literal">Namespace</code> CR in the same file with the <code class="literal">PolicyGenTemplate</code> CR.
							</p></div></div></li><li class="listitem">
							Add the <code class="literal">PolicyGenTemplate</code> CRs and <code class="literal">Namespace</code> CR to the <code class="literal">kustomization.yaml</code> file in the generators section, similar to the example shown in <code class="literal">out/argocd/example/policygentemplates/kustomization.yaml</code>.
						</li><li class="listitem"><p class="simpara">
							Commit the <code class="literal">PolicyGenTemplate</code> CRs, <code class="literal">Namespace</code> CR, and associated <code class="literal">kustomization.yaml</code> file in your Git repository and push the changes.
						</p><p class="simpara">
							The ArgoCD pipeline detects the changes and begins the managed cluster deployment. You can push the changes to the <code class="literal">SiteConfig</code> CR and the <code class="literal">PolicyGenTemplate</code> CR simultaneously.
						</p></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-creating-a-validator-inform-policy_ztp-advanced-policy-config">Signalling ZTP cluster deployment completion with validator inform policies</a>
						</li></ul></div></section><section class="section" id="ztp-monitoring-policy-deployment-progress_ztp-configuring-managed-clusters-policies"><div class="titlepage"><div><div><h3 class="title">16.4.5. Monitoring managed cluster policy deployment progress</h3></div></div></div><p>
					The ArgoCD pipeline uses <code class="literal">PolicyGenTemplate</code> CRs in Git to generate the RHACM policies and then sync them to the hub cluster. You can monitor the progress of the managed cluster policy synchronization after the assisted service installs OpenShift Container Platform on the managed cluster.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have logged in to the hub cluster as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							The Topology Aware Lifecycle Manager (TALM) applies the configuration policies that are bound to the cluster.
						</p><p class="simpara">
							After the cluster installation is complete and the cluster becomes <code class="literal">Ready</code>, a <code class="literal">ClusterGroupUpgrade</code> CR corresponding to this cluster, with a list of ordered policies defined by the <code class="literal">ran.openshift.io/ztp-deploy-wave annotations</code>, is automatically created by the TALM. The cluster’s policies are applied in the order listed in <code class="literal">ClusterGroupUpgrade</code> CR.
						</p><p class="simpara">
							You can monitor the high-level progress of configuration policy reconciliation by using the following commands:
						</p><pre class="programlisting language-terminal">$ export CLUSTER=&lt;clusterName&gt;</pre><pre class="programlisting language-terminal">$ oc get clustergroupupgrades -n ztp-install $CLUSTER -o jsonpath='{.status.conditions[-1:]}' | jq</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">{
  "lastTransitionTime": "2022-11-09T07:28:09Z",
  "message": "Remediating non-compliant policies",
  "reason": "InProgress",
  "status": "True",
  "type": "Progressing"
}</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							You can monitor the detailed cluster policy compliance status by using the RHACM dashboard or the command line.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									To check policy compliance by using <code class="literal">oc</code>, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get policies -n $CLUSTER</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                                     REMEDIATION ACTION   COMPLIANCE STATE   AGE
ztp-common.common-config-policy                          inform               Compliant          3h42m
ztp-common.common-subscriptions-policy                   inform               NonCompliant       3h42m
ztp-group.group-du-sno-config-policy                     inform               NonCompliant       3h42m
ztp-group.group-du-sno-validator-du-policy               inform               NonCompliant       3h42m
ztp-install.example1-common-config-policy-pjz9s          enforce              Compliant          167m
ztp-install.example1-common-subscriptions-policy-zzd9k   enforce              NonCompliant       164m
ztp-site.example1-config-policy                          inform               NonCompliant       3h42m
ztp-site.example1-perf-policy                            inform               NonCompliant       3h42m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									To check policy status from the RHACM web console, perform the following actions:
								</p><div class="orderedlist"><ol class="orderedlist" type="i"><li class="listitem">
											Click <span class="strong strong"><strong>Governance</strong></span> → <span class="strong strong"><strong>Find policies</strong></span>.
										</li><li class="listitem">
											Click on a cluster policy to check it’s status.
										</li></ol></div></li></ol></div></li></ol></div><p>
					When all of the cluster policies become compliant, GitOps ZTP installation and configuration for the cluster is complete. The <code class="literal">ztp-done</code> label is added to the cluster.
				</p><p>
					In the reference configuration, the final policy that becomes compliant is the one defined in the <code class="literal">*-du-validator-policy</code> policy. This policy, when compliant on a cluster, ensures that all cluster configuration, Operator installation, and Operator configuration is complete.
				</p></section><section class="section" id="ztp-validating-the-generation-of-configuration-policy-crs_ztp-configuring-managed-clusters-policies"><div class="titlepage"><div><div><h3 class="title">16.4.6. Validating the generation of configuration policy CRs</h3></div></div></div><p>
					Policy custom resources (CRs) are generated in the same namespace as the <code class="literal">PolicyGenTemplate</code> from which they are created. The same troubleshooting flow applies to all policy CRs generated from a <code class="literal">PolicyGenTemplate</code> regardless of whether they are <code class="literal">ztp-common</code>, <code class="literal">ztp-group</code>, or <code class="literal">ztp-site</code> based, as shown using the following commands:
				</p><pre class="programlisting language-terminal">$ export NS=&lt;namespace&gt;</pre><pre class="programlisting language-terminal">$ oc get policy -n $NS</pre><p>
					The expected set of policy-wrapped CRs should be displayed.
				</p><p>
					If the policies failed synchronization, use the following troubleshooting steps.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To display detailed information about the policies, run the following command:
						</p><pre class="programlisting language-terminal">$ oc describe -n openshift-gitops application policies</pre></li><li class="listitem"><p class="simpara">
							Check for <code class="literal">Status: Conditions:</code> to show the error logs. For example, setting an invalid <code class="literal">sourceFile→fileName:</code> generates the error shown below:
						</p><pre class="programlisting language-text">Status:
  Conditions:
    Last Transition Time:  2021-11-26T17:21:39Z
    Message:               rpc error: code = Unknown desc = `kustomize build /tmp/https___git.com/ran-sites/policies/ --enable-alpha-plugins` failed exit status 1: 2021/11/26 17:21:40 Error could not find test.yaml under source-crs/: no such file or directory Error: failure in plugin configured via /tmp/kust-plugin-config-52463179; exit status 1: exit status 1
    Type:  ComparisonError</pre></li><li class="listitem"><p class="simpara">
							Check for <code class="literal">Status: Sync:</code>. If there are log errors at <code class="literal">Status: Conditions:</code>, the <code class="literal">Status: Sync:</code> shows <code class="literal">Unknown</code> or <code class="literal">Error</code>:
						</p><pre class="programlisting language-text">Status:
  Sync:
    Compared To:
      Destination:
        Namespace:  policies-sub
        Server:     https://kubernetes.default.svc
      Source:
        Path:             policies
        Repo URL:         https://git.com/ran-sites/policies/.git
        Target Revision:  master
    Status:               Error</pre></li><li class="listitem"><p class="simpara">
							When Red Hat Advanced Cluster Management (RHACM) recognizes that policies apply to a <code class="literal">ManagedCluster</code> object, the policy CR objects are applied to the cluster namespace. Check to see if the policies were copied to the cluster namespace:
						</p><pre class="programlisting language-terminal">$ oc get policy -n $CLUSTER</pre><div class="formalpara"><p class="title"><strong>Example output:</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                         REMEDIATION ACTION   COMPLIANCE STATE   AGE
ztp-common.common-config-policy              inform               Compliant          13d
ztp-common.common-subscriptions-policy       inform               Compliant          13d
ztp-group.group-du-sno-config-policy         inform               Compliant          13d
Ztp-group.group-du-sno-validator-du-policy   inform               Compliant          13d
ztp-site.example-sno-config-policy           inform               Compliant          13d</pre>

							</p></div><p class="simpara">
							RHACM copies all applicable policies into the cluster namespace. The copied policy names have the format: <code class="literal">&lt;policyGenTemplate.Namespace&gt;.&lt;policyGenTemplate.Name&gt;-&lt;policyName&gt;</code>.
						</p></li><li class="listitem"><p class="simpara">
							Check the placement rule for any policies not copied to the cluster namespace. The <code class="literal">matchSelector</code> in the <code class="literal">PlacementRule</code> for those policies should match labels on the <code class="literal">ManagedCluster</code> object:
						</p><pre class="programlisting language-terminal">$ oc get placementrule -n $NS</pre></li><li class="listitem"><p class="simpara">
							Note the <code class="literal">PlacementRule</code> name appropriate for the missing policy, common, group, or site, using the following command:
						</p><pre class="programlisting language-terminal">$ oc get placementrule -n $NS &lt;placementRuleName&gt; -o yaml</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The status-decisions should include your cluster name.
								</li><li class="listitem">
									The key-value pair of the <code class="literal">matchSelector</code> in the spec must match the labels on your managed cluster.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							Check the labels on the <code class="literal">ManagedCluster</code> object using the following command:
						</p><pre class="programlisting language-terminal">$ oc get ManagedCluster $CLUSTER -o jsonpath='{.metadata.labels}' | jq</pre></li><li class="listitem"><p class="simpara">
							Check to see which policies are compliant using the following command:
						</p><pre class="programlisting language-terminal">$ oc get policy -n $CLUSTER</pre><p class="simpara">
							If the <code class="literal">Namespace</code>, <code class="literal">OperatorGroup</code>, and <code class="literal">Subscription</code> policies are compliant but the Operator configuration policies are not, it is likely that the Operators did not install on the managed cluster. This causes the Operator configuration policies to fail to apply because the CRD is not yet applied to the spoke.
						</p></li></ol></div></section><section class="section" id="ztp-restarting-policies-reconciliation_ztp-configuring-managed-clusters-policies"><div class="titlepage"><div><div><h3 class="title">16.4.7. Restarting policy reconciliation</h3></div></div></div><p>
					You can restart policy reconciliation when unexpected compliance issues occur, for example, when the <code class="literal">ClusterGroupUpgrade</code> custom resource (CR) has timed out.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							A <code class="literal">ClusterGroupUpgrade</code> CR is generated in the namespace <code class="literal">ztp-install</code> by the Topology Aware Lifecycle Manager after the managed cluster becomes <code class="literal">Ready</code>:
						</p><pre class="programlisting language-terminal">$ export CLUSTER=&lt;clusterName&gt;</pre><pre class="programlisting language-terminal">$ oc get clustergroupupgrades -n ztp-install $CLUSTER</pre></li><li class="listitem"><p class="simpara">
							If there are unexpected issues and the policies fail to become complaint within the configured timeout (the default is 4 hours), the status of the <code class="literal">ClusterGroupUpgrade</code> CR shows <code class="literal">UpgradeTimedOut</code>:
						</p><pre class="programlisting language-terminal">$ oc get clustergroupupgrades -n ztp-install $CLUSTER -o jsonpath='{.status.conditions[?(@.type=="Ready")]}'</pre></li><li class="listitem"><p class="simpara">
							A <code class="literal">ClusterGroupUpgrade</code> CR in the <code class="literal">UpgradeTimedOut</code> state automatically restarts its policy reconciliation every hour. If you have changed your policies, you can start a retry immediately by deleting the existing <code class="literal">ClusterGroupUpgrade</code> CR. This triggers the automatic creation of a new <code class="literal">ClusterGroupUpgrade</code> CR that begins reconciling the policies immediately:
						</p><pre class="programlisting language-terminal">$ oc delete clustergroupupgrades -n ztp-install $CLUSTER</pre></li></ol></div><p>
					Note that when the <code class="literal">ClusterGroupUpgrade</code> CR completes with status <code class="literal">UpgradeCompleted</code> and the managed cluster has the label <code class="literal">ztp-done</code> applied, you can make additional configuration changes using <code class="literal">PolicyGenTemplate</code>. Deleting the existing <code class="literal">ClusterGroupUpgrade</code> CR will not make the TALM generate a new CR.
				</p><p>
					At this point, GitOps ZTP has completed its interaction with the cluster and any further interactions should be treated as an update and a new <code class="literal">ClusterGroupUpgrade</code> CR created for remediation of the policies.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For information about using Topology Aware Lifecycle Manager (TALM) to construct your own <code class="literal">ClusterGroupUpgrade</code> CR, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#talo-about-cgu-crs_cnf-topology-aware-lifecycle-manager">About the ClusterGroupUpgrade CR</a>.
						</li></ul></div></section><section class="section" id="ztp-removing-content-from-managed-clusters_ztp-configuring-managed-clusters-policies"><div class="titlepage"><div><div><h3 class="title">16.4.8. Changing applied managed cluster CRs using policies</h3></div></div></div><p>
					You can remove content from a custom resource (CR) that is deployed in a managed cluster through a policy.
				</p><p>
					By default, all <code class="literal">Policy</code> CRs created from a <code class="literal">PolicyGenTemplate</code> CR have the <code class="literal">complianceType</code> field set to <code class="literal">musthave</code>. A <code class="literal">musthave</code> policy without the removed content is still compliant because the CR on the managed cluster has all the specified content. With this configuration, when you remove content from a CR, TALM removes the content from the policy but the content is not removed from the CR on the managed cluster.
				</p><p>
					With the <code class="literal">complianceType</code> field to <code class="literal">mustonlyhave</code>, the policy ensures that the CR on the cluster is an exact match of what is specified in the policy.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have logged in to the hub cluster as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							You have deployed a managed cluster from a hub cluster running RHACM.
						</li><li class="listitem">
							You have installed Topology Aware Lifecycle Manager on the hub cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Remove the content that you no longer need from the affected CRs. In this example, the <code class="literal">disableDrain: false</code> line was removed from the <code class="literal">SriovOperatorConfig</code> CR.
						</p><div class="formalpara"><p class="title"><strong>Example CR</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovOperatorConfig
metadata:
  name: default
  namespace: openshift-sriov-network-operator
spec:
  configDaemonNodeSelector:
    "node-role.kubernetes.io/$mcp": ""
  disableDrain: true
  enableInjector: true
  enableOperatorWebhook: true</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Change the <code class="literal">complianceType</code> of the affected policies to <code class="literal">mustonlyhave</code> in the <code class="literal">group-du-sno-ranGen.yaml</code> file.
						</p><div class="formalpara"><p class="title"><strong>Example YAML</strong></p><p>
								
<pre class="programlisting language-yaml"># ...
- fileName: SriovOperatorConfig.yaml
  policyName: "config-policy"
  complianceType: mustonlyhave
# ...</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">ClusterGroupUpdates</code> CR and specify the clusters that must receive the CR changes::
						</p><div class="formalpara"><p class="title"><strong>Example ClusterGroupUpdates CR</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: cgu-remove
  namespace: default
spec:
  managedPolicies:
    - ztp-group.group-du-sno-config-policy
  enable: false
  clusters:
  - spoke1
  - spoke2
  remediationStrategy:
    maxConcurrency: 2
    timeout: 240
  batchTimeoutAction:</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">ClusterGroupUpgrade</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f cgu-remove.yaml</pre></li><li class="listitem"><p class="simpara">
							When you are ready to apply the changes, for example, during an appropriate maintenance window, change the value of the <code class="literal">spec.enable</code> field to <code class="literal">true</code> by running the following command:
						</p><pre class="programlisting language-terminal">$ oc --namespace=default patch clustergroupupgrade.ran.openshift.io/cgu-remove \
--patch '{"spec":{"enable":true}}' --type=merge</pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check the status of the policies by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get &lt;kind&gt; &lt;changed_cr_name&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAMESPACE   NAME                                                   REMEDIATION ACTION   COMPLIANCE STATE   AGE
default     cgu-ztp-group.group-du-sno-config-policy               enforce                                 17m
default     ztp-group.group-du-sno-config-policy                   inform               NonCompliant       15h</pre>

							</p></div><p class="simpara">
							When the <code class="literal">COMPLIANCE STATE</code> of the policy is <code class="literal">Compliant</code>, it means that the CR is updated and the unwanted content is removed.
						</p></li><li class="listitem"><p class="simpara">
							Check that the policies are removed from the targeted clusters by running the following command on the managed clusters:
						</p><pre class="programlisting language-terminal">$ oc get &lt;kind&gt; &lt;changed_cr_name&gt;</pre><p class="simpara">
							If there are no results, the CR is removed from the managed cluster.
						</p></li></ol></div></section><section class="section" id="ztp-definition-of-done-for-ztp-installations_ztp-configuring-managed-clusters-policies"><div class="titlepage"><div><div><h3 class="title">16.4.9. Indication of done for GitOps ZTP installations</h3></div></div></div><p>
					GitOps Zero Touch Provisioning (ZTP) simplifies the process of checking the GitOps ZTP installation status for a cluster. The GitOps ZTP status moves through three phases: cluster installation, cluster configuration, and GitOps ZTP done.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Cluster installation phase</span></dt><dd>
								The cluster installation phase is shown by the <code class="literal">ManagedClusterJoined</code> and <code class="literal">ManagedClusterAvailable</code> conditions in the <code class="literal">ManagedCluster</code> CR . If the <code class="literal">ManagedCluster</code> CR does not have these conditions, or the condition is set to <code class="literal">False</code>, the cluster is still in the installation phase. Additional details about installation are available from the <code class="literal">AgentClusterInstall</code> and <code class="literal">ClusterDeployment</code> CRs. For more information, see "Troubleshooting GitOps ZTP".
							</dd><dt><span class="term">Cluster configuration phase</span></dt><dd>
								The cluster configuration phase is shown by a <code class="literal">ztp-running</code> label applied the <code class="literal">ManagedCluster</code> CR for the cluster.
							</dd><dt><span class="term">GitOps ZTP done</span></dt><dd><p class="simpara">
								Cluster installation and configuration is complete in the GitOps ZTP done phase. This is shown by the removal of the <code class="literal">ztp-running</code> label and addition of the <code class="literal">ztp-done</code> label to the <code class="literal">ManagedCluster</code> CR. The <code class="literal">ztp-done</code> label shows that the configuration has been applied and the baseline DU configuration has completed cluster tuning.
							</p><p class="simpara">
								The transition to the GitOps ZTP done state is conditional on the compliant state of a Red Hat Advanced Cluster Management (RHACM) validator inform policy. This policy captures the existing criteria for a completed installation and validates that it moves to a compliant state only when GitOps ZTP provisioning of the managed cluster is complete.
							</p><p class="simpara">
								The validator inform policy ensures the configuration of the cluster is fully applied and Operators have completed their initialization. The policy validates the following:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										The target <code class="literal">MachineConfigPool</code> contains the expected entries and has finished updating. All nodes are available and not degraded.
									</li><li class="listitem">
										The SR-IOV Operator has completed initialization as indicated by at least one <code class="literal">SriovNetworkNodeState</code> with <code class="literal">syncStatus: Succeeded</code>.
									</li><li class="listitem">
										The PTP Operator daemon set exists.
									</li></ul></div></dd></dl></div></section></section><section class="section" id="ztp-manual-install"><div class="titlepage"><div><div><h2 class="title">16.5. Manually installing a single-node OpenShift cluster with ZTP</h2></div></div></div><p>
				You can deploy a managed single-node OpenShift cluster by using Red Hat Advanced Cluster Management (RHACM) and the assisted service.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					If you are creating multiple managed clusters, use the <code class="literal">SiteConfig</code> method described in <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-deploying-far-edge-sites">Deploying far edge sites with ZTP</a>.
				</p></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					The target bare-metal host must meet the networking, firmware, and hardware requirements listed in <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#sno-configure-for-vdu">Recommended cluster configuration for vDU application workloads</a>.
				</p></div></div><section class="section" id="ztp-generating-install-and-config-crs-manually_ztp-manual-install"><div class="titlepage"><div><div><h3 class="title">16.5.1. Generating GitOps ZTP installation and configuration CRs manually</h3></div></div></div><p>
					Use the <code class="literal">generator</code> entrypoint for the <code class="literal">ztp-site-generate</code> container to generate the site installation and configuration custom resource (CRs) for a cluster based on <code class="literal">SiteConfig</code> and <code class="literal">PolicyGenTemplate</code> CRs.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have logged in to the hub cluster as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an output folder by running the following command:
						</p><pre class="programlisting language-terminal">$ mkdir -p ./out</pre></li><li class="listitem"><p class="simpara">
							Export the <code class="literal">argocd</code> directory from the <code class="literal">ztp-site-generate</code> container image:
						</p><pre class="programlisting language-terminal">$ podman run --log-driver=none --rm registry.redhat.io/openshift4/ztp-site-generate-rhel8:v4.13 extract /home/ztp --tar | tar x -C ./out</pre><p class="simpara">
							The <code class="literal">./out</code> directory has the reference <code class="literal">PolicyGenTemplate</code> and <code class="literal">SiteConfig</code> CRs in the <code class="literal">out/argocd/example/</code> folder.
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">out
 └── argocd
      └── example
           ├── policygentemplates
           │     ├── common-ranGen.yaml
           │     ├── example-sno-site.yaml
           │     ├── group-du-sno-ranGen.yaml
           │     ├── group-du-sno-validator-ranGen.yaml
           │     ├── kustomization.yaml
           │     └── ns.yaml
           └── siteconfig
                  ├── example-sno.yaml
                  ├── KlusterletAddonConfigOverride.yaml
                  └── kustomization.yaml</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create an output folder for the site installation CRs:
						</p><pre class="programlisting language-terminal">$ mkdir -p ./site-install</pre></li><li class="listitem"><p class="simpara">
							Modify the example <code class="literal">SiteConfig</code> CR for the cluster type that you want to install. Copy <code class="literal">example-sno.yaml</code> to <code class="literal">site-1-sno.yaml</code> and modify the CR to match the details of the site and bare-metal host that you want to install, for example:
						</p><pre class="programlisting language-yaml"># example-node1-bmh-secret &amp; assisted-deployment-pull-secret need to be created under same namespace example-sno
---
apiVersion: ran.openshift.io/v1
kind: SiteConfig
metadata:
  name: "example-sno"
  namespace: "example-sno"
spec:
  baseDomain: "example.com"
  cpuPartitioningMode: AllNodes
  pullSecretRef:
    name: "assisted-deployment-pull-secret"
  clusterImageSetNameRef: "openshift-4.10"
  sshPublicKey: "ssh-rsa AAAA..."
  clusters:
  - clusterName: "example-sno"
    networkType: "OVNKubernetes"
    installConfigOverrides: |
      {
        "capabilities": {
          "baselineCapabilitySet": "None",
          "additionalEnabledCapabilities": [
            "marketplace",
            "NodeTuning"
          ]
        }
      }
    clusterLabels:
      common: true
      group-du-sno: ""
      sites : "example-sno"
    clusterNetwork:
      - cidr: 1001:1::/48
        hostPrefix: 64
    machineNetwork:
      - cidr: 1111:2222:3333:4444::/64
    serviceNetwork:
      - 1001:2::/112
    additionalNTPSources:
      - 1111:2222:3333:4444::2
    # crTemplates:
    #   KlusterletAddonConfig: "KlusterletAddonConfigOverride.yaml"
    nodes:
      - hostName: "example-node1.example.com"
        role: "master"
        bmcAddress: "idrac-virtualmedia+https://[1111:2222:3333:4444::bbbb:1]/redfish/v1/Systems/System.Embedded.1"
        bmcCredentialsName:
          name: "example-node1-bmh-secret"
        bootMACAddress: "AA:BB:CC:DD:EE:11"
        bootMode: "UEFI"
        rootDeviceHints:
          wwn: "0x11111000000asd123"
        # diskPartition:
        #   - device: /dev/disk/by-id/wwn-0x11111000000asd123 # match rootDeviceHints
        #     partitions:
        #       - mount_point: /var/imageregistry
        #         size: 102500
        #         start: 344844
        ignitionConfigOverride: |
          {
            "ignition": {
              "version": "3.2.0"
            },
            "storage": {
              "disks": [
                {
                  "device": "/dev/disk/by-id/wwn-0x11111000000asd123",
                  "wipeTable": false,
                  "partitions": [
                    {
                      "sizeMiB": 16,
                      "label": "httpevent1",
                      "startMiB": 350000
                    },
                    {
                      "sizeMiB": 16,
                      "label": "httpevent2",
                      "startMiB": 350016
                    }
                  ]
                }
              ],
              "filesystem": [
                {
                  "device": "/dev/disk/by-partlabel/httpevent1",
                  "format": "xfs",
                  "wipeFilesystem": true
                },
                {
                  "device": "/dev/disk/by-partlabel/httpevent2",
                  "format": "xfs",
                  "wipeFilesystem": true
                }
              ]
            }
          }
        nodeNetwork:
          interfaces:
            - name: eno1
              macAddress: "AA:BB:CC:DD:EE:11"
          config:
            interfaces:
              - name: eno1
                type: ethernet
                state: up
                ipv4:
                  enabled: false
                ipv6:
                  enabled: true
                  address:
                  - ip: 1111:2222:3333:4444::aaaa:1
                    prefix-length: 64
            dns-resolver:
              config:
                search:
                - example.com
                server:
                - 1111:2222:3333:4444::2
            routes:
              config:
              - destination: ::/0
                next-hop-interface: eno1
                next-hop-address: 1111:2222:3333:4444::1
                table-id: 254</pre></li><li class="listitem"><p class="simpara">
							Generate the Day 0 installation CRs by processing the modified <code class="literal">SiteConfig</code> CR <code class="literal">site-1-sno.yaml</code> by running the following command:
						</p><pre class="programlisting language-terminal">$ podman run -it --rm -v `pwd`/out/argocd/example/siteconfig:/resources:Z -v `pwd`/site-install:/output:Z,U registry.redhat.io/openshift4/ztp-site-generate-rhel8:v4.13 generator install site-1-sno.yaml /output</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">site-install
└── site-1-sno
    ├── site-1_agentclusterinstall_example-sno.yaml
    ├── site-1-sno_baremetalhost_example-node1.example.com.yaml
    ├── site-1-sno_clusterdeployment_example-sno.yaml
    ├── site-1-sno_configmap_example-sno.yaml
    ├── site-1-sno_infraenv_example-sno.yaml
    ├── site-1-sno_klusterletaddonconfig_example-sno.yaml
    ├── site-1-sno_machineconfig_02-master-workload-partitioning.yaml
    ├── site-1-sno_machineconfig_predefined-extra-manifests-master.yaml
    ├── site-1-sno_machineconfig_predefined-extra-manifests-worker.yaml
    ├── site-1-sno_managedcluster_example-sno.yaml
    ├── site-1-sno_namespace_example-sno.yaml
    └── site-1-sno_nmstateconfig_example-node1.example.com.yaml</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Optional: Generate just the Day 0 <code class="literal">MachineConfig</code> installation CRs for a particular cluster type by processing the reference <code class="literal">SiteConfig</code> CR with the <code class="literal">-E</code> option. For example, run the following commands:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create an output folder for the <code class="literal">MachineConfig</code> CRs:
								</p><pre class="programlisting language-terminal">$ mkdir -p ./site-machineconfig</pre></li><li class="listitem"><p class="simpara">
									Generate the <code class="literal">MachineConfig</code> installation CRs:
								</p><pre class="programlisting language-terminal">$ podman run -it --rm -v `pwd`/out/argocd/example/siteconfig:/resources:Z -v `pwd`/site-machineconfig:/output:Z,U registry.redhat.io/openshift4/ztp-site-generate-rhel8:v4.13 generator install -E site-1-sno.yaml /output</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">site-machineconfig
└── site-1-sno
    ├── site-1-sno_machineconfig_02-master-workload-partitioning.yaml
    ├── site-1-sno_machineconfig_predefined-extra-manifests-master.yaml
    └── site-1-sno_machineconfig_predefined-extra-manifests-worker.yaml</pre>

									</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Generate and export the Day 2 configuration CRs using the reference <code class="literal">PolicyGenTemplate</code> CRs from the previous step. Run the following commands:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create an output folder for the Day 2 CRs:
								</p><pre class="programlisting language-terminal">$ mkdir -p ./ref</pre></li><li class="listitem"><p class="simpara">
									Generate and export the Day 2 configuration CRs:
								</p><pre class="programlisting language-terminal">$ podman run -it --rm -v `pwd`/out/argocd/example/policygentemplates:/resources:Z -v `pwd`/ref:/output:Z,U registry.redhat.io/openshift4/ztp-site-generate-rhel8:v4.13 generator config -N . /output</pre><p class="simpara">
									The command generates example group and site-specific <code class="literal">PolicyGenTemplate</code> CRs for single-node OpenShift, three-node clusters, and standard clusters in the <code class="literal">./ref</code> folder.
								</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">ref
 └── customResource
      ├── common
      ├── example-multinode-site
      ├── example-sno
      ├── group-du-3node
      ├── group-du-3node-validator
      │    └── Multiple-validatorCRs
      ├── group-du-sno
      ├── group-du-sno-validator
      ├── group-du-standard
      └── group-du-standard-validator
           └── Multiple-validatorCRs</pre>

									</p></div></li></ol></div></li><li class="listitem">
							Use the generated CRs as the basis for the CRs that you use to install the cluster. You apply the installation CRs to the hub cluster as described in "Installing a single managed cluster". The configuration CRs can be applied to the cluster after cluster installation is complete.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-sno-du-enabling-workload-partitioning_sno-configure-for-vdu">Workload partitioning</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#bmc-addressing_ipi-install-installation-workflow">BMC addressing</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#root-device-hints_preparing-to-install-with-agent-based-installer">About root device hints</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-sno-siteconfig-config-reference_ztp-deploying-far-edge-sites">Single-node OpenShift SiteConfig CR installation reference</a>
						</li></ul></div></section><section class="section" id="ztp-creating-the-site-secrets_ztp-manual-install"><div class="titlepage"><div><div><h3 class="title">16.5.2. Creating the managed bare-metal host secrets</h3></div></div></div><p>
					Add the required <code class="literal">Secret</code> custom resources (CRs) for the managed bare-metal host to the hub cluster. You need a secret for the GitOps Zero Touch Provisioning (ZTP) pipeline to access the Baseboard Management Controller (BMC) and a secret for the assisted installer service to pull cluster installation images from the registry.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The secrets are referenced from the <code class="literal">SiteConfig</code> CR by name. The namespace must match the <code class="literal">SiteConfig</code> namespace.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a YAML secret file containing credentials for the host Baseboard Management Controller (BMC) and a pull secret required for installing OpenShift and all add-on cluster Operators:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Save the following YAML as the file <code class="literal">example-sno-secret.yaml</code>:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: example-sno-bmc-secret
  namespace: example-sno <span id="CO50-1"><!--Empty--></span><span class="callout">1</span>
data: <span id="CO50-2"><!--Empty--></span><span class="callout">2</span>
  password: &lt;base64_password&gt;
  username: &lt;base64_username&gt;
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  name: pull-secret
  namespace: example-sno  <span id="CO50-3"><!--Empty--></span><span class="callout">3</span>
data:
  .dockerconfigjson: &lt;pull_secret&gt; <span id="CO50-4"><!--Empty--></span><span class="callout">4</span>
type: kubernetes.io/dockerconfigjson</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO50-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Must match the namespace configured in the related <code class="literal">SiteConfig</code> CR
										</div></dd><dt><a href="#CO50-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Base64-encoded values for <code class="literal">password</code> and <code class="literal">username</code>
										</div></dd><dt><a href="#CO50-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Must match the namespace configured in the related <code class="literal">SiteConfig</code> CR
										</div></dd><dt><a href="#CO50-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Base64-encoded pull secret
										</div></dd></dl></div></li></ol></div></li><li class="listitem">
							Add the relative path to <code class="literal">example-sno-secret.yaml</code> to the <code class="literal">kustomization.yaml</code> file that you use to install the cluster.
						</li></ol></div></section><section class="section" id="setting-managed-bare-metal-host-kernel-arguments_ztp-manual-install"><div class="titlepage"><div><div><h3 class="title">16.5.3. Configuring Discovery ISO kernel arguments for manual installations using GitOps ZTP</h3></div></div></div><p>
					The GitOps Zero Touch Provisioning (ZTP) workflow uses the Discovery ISO as part of the OpenShift Container Platform installation process on managed bare-metal hosts. You can edit the <code class="literal">InfraEnv</code> resource to specify kernel arguments for the Discovery ISO. This is useful for cluster installations with specific environmental requirements. For example, configure the <code class="literal">rd.net.timeout.carrier</code> kernel argument for the Discovery ISO to facilitate static networking for the cluster or to receive a DHCP address before downloading the root file system during installation.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						In OpenShift Container Platform 4.13, you can only add kernel arguments. You can not replace or delete kernel arguments.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (oc).
						</li><li class="listitem">
							You have logged in to the hub cluster as a user with cluster-admin privileges.
						</li><li class="listitem">
							You have manually generated the installation and configuration custom resources (CRs).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Edit the <code class="literal">spec.kernelArguments</code> specification in the <code class="literal">InfraEnv</code> CR to configure kernel arguments:
						</li></ol></div><pre class="programlisting language-yaml white-space-pre white-space-pre">apiVersion: agent-install.openshift.io/v1beta1
kind: InfraEnv
metadata:
  name: &lt;cluster_name&gt;
  namespace: &lt;cluster_name&gt;
spec:
  kernelArguments:
    - operation: append <span id="CO51-1"><!--Empty--></span><span class="callout">1</span>
      value: audit=0 <span id="CO51-2"><!--Empty--></span><span class="callout">2</span>
    - operation: append
      value: trace=1
  clusterRef:
    name: &lt;cluster_name&gt;
    namespace: &lt;cluster_name&gt;
  pullSecretRef:
    name: pull-secret</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO51-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Specify the append operation to add a kernel argument.
						</div></dd><dt><a href="#CO51-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Specify the kernel argument you want to configure. This example configures the audit kernel argument and the trace kernel argument.
						</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The <code class="literal">SiteConfig</code> CR generates the <code class="literal">InfraEnv</code> resource as part of the day-0 installation CRs.
					</p></div></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						To verify that the kernel arguments are applied, after the Discovery image verifies that OpenShift Container Platform is ready for installation, you can SSH to the target host before the installation process begins. At that point, you can view the kernel arguments for the Discovery ISO in the <code class="literal">/proc/cmdline</code> file.
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Begin an SSH session with the target host:
						</p><pre class="programlisting language-terminal">$ ssh -i /path/to/privatekey core@&lt;host_name&gt;</pre></li><li class="listitem"><p class="simpara">
							View the system’s kernel arguments by using the following command:
						</p><pre class="programlisting language-terminal">$ cat /proc/cmdline</pre></li></ol></div></section><section class="section" id="ztp-manually-install-a-single-managed-cluster_ztp-manual-install"><div class="titlepage"><div><div><h3 class="title">16.5.4. Installing a single managed cluster</h3></div></div></div><p>
					You can manually deploy a single managed cluster using the assisted service and Red Hat Advanced Cluster Management (RHACM).
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have logged in to the hub cluster as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							You have created the baseboard management controller (BMC) <code class="literal">Secret</code> and the image pull-secret <code class="literal">Secret</code> custom resources (CRs). See "Creating the managed bare-metal host secrets" for details.
						</li><li class="listitem">
							Your target bare-metal host meets the networking and hardware requirements for managed clusters.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">ClusterImageSet</code> for each specific cluster version to be deployed, for example <code class="literal">clusterImageSet-4.13.yaml</code>. A <code class="literal">ClusterImageSet</code> has the following format:
						</p><pre class="programlisting language-yaml">apiVersion: hive.openshift.io/v1
kind: ClusterImageSet
metadata:
  name: openshift-4.13.0 <span id="CO52-1"><!--Empty--></span><span class="callout">1</span>
spec:
   releaseImage: quay.io/openshift-release-dev/ocp-release:4.13.0-x86_64 <span id="CO52-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO52-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The descriptive version that you want to deploy.
								</div></dd><dt><a href="#CO52-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specifies the <code class="literal">releaseImage</code> to deploy and determines the operating system image version. The discovery ISO is based on the image version as set by <code class="literal">releaseImage</code>, or the latest version if the exact version is unavailable.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Apply the <code class="literal">clusterImageSet</code> CR:
						</p><pre class="programlisting language-terminal">$ oc apply -f clusterImageSet-4.13.yaml</pre></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">Namespace</code> CR in the <code class="literal">cluster-namespace.yaml</code> file:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:
     name: &lt;cluster_name&gt; <span id="CO53-1"><!--Empty--></span><span class="callout">1</span>
     labels:
        name: &lt;cluster_name&gt; <span id="CO53-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO53-1"><span class="callout">1</span></a> <a href="#CO53-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The name of the managed cluster to provision.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Apply the <code class="literal">Namespace</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f cluster-namespace.yaml</pre></li><li class="listitem"><p class="simpara">
							Apply the generated day-0 CRs that you extracted from the <code class="literal">ztp-site-generate</code> container and customized to meet your requirements:
						</p><pre class="programlisting language-terminal">$ oc apply -R ./site-install/site-sno-1</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-managed-cluster-network-prereqs_sno-configure-for-vdu">Connectivity prerequisites for managed cluster networks</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/storage/#lvms-preface-sno-ran_logical-volume-manager-storage">Deploying LVM Storage on single-node OpenShift clusters</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-provisioning-lvm-storage_ztp-advanced-policy-config">Configuring LVM Storage using PolicyGenTemplate CRs</a>
						</li></ul></div></section><section class="section" id="ztp-checking-the-managed-cluster-status_ztp-manual-install"><div class="titlepage"><div><div><h3 class="title">16.5.5. Monitoring the managed cluster installation status</h3></div></div></div><p>
					Ensure that cluster provisioning was successful by checking the cluster status.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							All of the custom resources have been configured and provisioned, and the <code class="literal">Agent</code> custom resource is created on the hub for the managed cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check the status of the managed cluster:
						</p><pre class="programlisting language-terminal">$ oc get managedcluster</pre><p class="simpara">
							<code class="literal">True</code> indicates the managed cluster is ready.
						</p></li><li class="listitem"><p class="simpara">
							Check the agent status:
						</p><pre class="programlisting language-terminal">$ oc get agent -n &lt;cluster_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Use the <code class="literal">describe</code> command to provide an in-depth description of the agent’s condition. Statuses to be aware of include <code class="literal">BackendError</code>, <code class="literal">InputError</code>, <code class="literal">ValidationsFailing</code>, <code class="literal">InstallationFailed</code>, and <code class="literal">AgentIsConnected</code>. These statuses are relevant to the <code class="literal">Agent</code> and <code class="literal">AgentClusterInstall</code> custom resources.
						</p><pre class="programlisting language-terminal">$ oc describe agent -n &lt;cluster_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Check the cluster provisioning status:
						</p><pre class="programlisting language-terminal">$ oc get agentclusterinstall -n &lt;cluster_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Use the <code class="literal">describe</code> command to provide an in-depth description of the cluster provisioning status:
						</p><pre class="programlisting language-terminal">$ oc describe agentclusterinstall -n &lt;cluster_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Check the status of the managed cluster’s add-on services:
						</p><pre class="programlisting language-terminal">$ oc get managedclusteraddon -n &lt;cluster_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Retrieve the authentication information of the <code class="literal">kubeconfig</code> file for the managed cluster:
						</p><pre class="programlisting language-terminal">$ oc get secret -n &lt;cluster_name&gt; &lt;cluster_name&gt;-admin-kubeconfig -o jsonpath={.data.kubeconfig} | base64 -d &gt; &lt;directory&gt;/&lt;cluster_name&gt;-kubeconfig</pre></li></ol></div></section><section class="section" id="ztp-troubleshooting-the-managed-cluster_ztp-manual-install"><div class="titlepage"><div><div><h3 class="title">16.5.6. Troubleshooting the managed cluster</h3></div></div></div><p>
					Use this procedure to diagnose any installation issues that might occur with the managed cluster.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check the status of the managed cluster:
						</p><pre class="programlisting language-terminal">$ oc get managedcluster</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME            HUB ACCEPTED   MANAGED CLUSTER URLS   JOINED   AVAILABLE   AGE
SNO-cluster     true                                   True     True      2d19h</pre>

							</p></div><p class="simpara">
							If the status in the <code class="literal">AVAILABLE</code> column is <code class="literal">True</code>, the managed cluster is being managed by the hub.
						</p><p class="simpara">
							If the status in the <code class="literal">AVAILABLE</code> column is <code class="literal">Unknown</code>, the managed cluster is not being managed by the hub. Use the following steps to continue checking to get more information.
						</p></li><li class="listitem"><p class="simpara">
							Check the <code class="literal">AgentClusterInstall</code> install status:
						</p><pre class="programlisting language-terminal">$ oc get clusterdeployment -n &lt;cluster_name&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME        PLATFORM            REGION   CLUSTERTYPE   INSTALLED    INFRAID    VERSION  POWERSTATE AGE
Sno0026    agent-baremetal                               false                          Initialized
2d14h</pre>

							</p></div><p class="simpara">
							If the status in the <code class="literal">INSTALLED</code> column is <code class="literal">false</code>, the installation was unsuccessful.
						</p></li><li class="listitem"><p class="simpara">
							If the installation failed, enter the following command to review the status of the <code class="literal">AgentClusterInstall</code> resource:
						</p><pre class="programlisting language-terminal">$ oc describe agentclusterinstall -n &lt;cluster_name&gt; &lt;cluster_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Resolve the errors and reset the cluster:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Remove the cluster’s managed cluster resource:
								</p><pre class="programlisting language-terminal">$ oc delete managedcluster &lt;cluster_name&gt;</pre></li><li class="listitem"><p class="simpara">
									Remove the cluster’s namespace:
								</p><pre class="programlisting language-terminal">$ oc delete namespace &lt;cluster_name&gt;</pre><p class="simpara">
									This deletes all of the namespace-scoped custom resources created for this cluster. You must wait for the <code class="literal">ManagedCluster</code> CR deletion to complete before proceeding.
								</p></li><li class="listitem">
									Recreate the custom resources for the managed cluster.
								</li></ol></div></li></ol></div></section><section class="section" id="ztp-installation-crs_ztp-manual-install"><div class="titlepage"><div><div><h3 class="title">16.5.7. RHACM generated cluster installation CRs reference</h3></div></div></div><p>
					Red Hat Advanced Cluster Management (RHACM) supports deploying OpenShift Container Platform on single-node clusters, three-node clusters, and standard clusters with a specific set of installation custom resources (CRs) that you generate using <code class="literal">SiteConfig</code> CRs for each site.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Every managed cluster has its own namespace, and all of the installation CRs except for <code class="literal">ManagedCluster</code> and <code class="literal">ClusterImageSet</code> are under that namespace. <code class="literal">ManagedCluster</code> and <code class="literal">ClusterImageSet</code> are cluster-scoped, not namespace-scoped. The namespace and the CR names match the cluster name.
					</p></div></div><p>
					The following table lists the installation CRs that are automatically applied by the RHACM assisted service when it installs clusters using the <code class="literal">SiteConfig</code> CRs that you configure.
				</p><div class="table" id="idm139735335063136"><p class="title"><strong>Table 16.7. Cluster installation CRs generated by RHACM</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 14%; " class="col_1"><!--Empty--></col><col style="width: 43%; " class="col_2"><!--Empty--></col><col style="width: 43%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735335057344" scope="col">CR</th><th align="left" valign="top" id="idm139735335056256" scope="col">Description</th><th align="left" valign="top" id="idm139735335055168" scope="col">Usage</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735335057344"> <p>
									<code class="literal">BareMetalHost</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735335056256"> <p>
									Contains the connection information for the Baseboard Management Controller (BMC) of the target bare-metal host.
								</p>
								 </td><td align="left" valign="top" headers="idm139735335055168"> <p>
									Provides access to the BMC to load and start the discovery image on the target server by using the Redfish protocol.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735335057344"> <p>
									<code class="literal">InfraEnv</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735335056256"> <p>
									Contains information for installing OpenShift Container Platform on the target bare-metal host.
								</p>
								 </td><td align="left" valign="top" headers="idm139735335055168"> <p>
									Used with <code class="literal">ClusterDeployment</code> to generate the discovery ISO for the managed cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735335057344"> <p>
									<code class="literal">AgentClusterInstall</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735335056256"> <p>
									Specifies details of the managed cluster configuration such as networking and the number of control plane nodes. Displays the cluster <code class="literal">kubeconfig</code> and credentials when the installation is complete.
								</p>
								 </td><td align="left" valign="top" headers="idm139735335055168"> <p>
									Specifies the managed cluster configuration information and provides status during the installation of the cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735335057344"> <p>
									<code class="literal">ClusterDeployment</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735335056256"> <p>
									References the <code class="literal">AgentClusterInstall</code> CR to use.
								</p>
								 </td><td align="left" valign="top" headers="idm139735335055168"> <p>
									Used with <code class="literal">InfraEnv</code> to generate the discovery ISO for the managed cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735335057344"> <p>
									<code class="literal">NMStateConfig</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735335056256"> <p>
									Provides network configuration information such as <code class="literal">MAC</code> address to <code class="literal">IP</code> mapping, DNS server, default route, and other network settings.
								</p>
								 </td><td align="left" valign="top" headers="idm139735335055168"> <p>
									Sets up a static IP address for the managed cluster’s Kube API server.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735335057344"> <p>
									<code class="literal">Agent</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735335056256"> <p>
									Contains hardware information about the target bare-metal host.
								</p>
								 </td><td align="left" valign="top" headers="idm139735335055168"> <p>
									Created automatically on the hub when the target machine’s discovery image boots.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735335057344"> <p>
									<code class="literal">ManagedCluster</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735335056256"> <p>
									When a cluster is managed by the hub, it must be imported and known. This Kubernetes object provides that interface.
								</p>
								 </td><td align="left" valign="top" headers="idm139735335055168"> <p>
									The hub uses this resource to manage and show the status of managed clusters.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735335057344"> <p>
									<code class="literal">KlusterletAddonConfig</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735335056256"> <p>
									Contains the list of services provided by the hub to be deployed to the <code class="literal">ManagedCluster</code> resource.
								</p>
								 </td><td align="left" valign="top" headers="idm139735335055168"> <p>
									Tells the hub which addon services to deploy to the <code class="literal">ManagedCluster</code> resource.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735335057344"> <p>
									<code class="literal">Namespace</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735335056256"> <p>
									Logical space for <code class="literal">ManagedCluster</code> resources existing on the hub. Unique per site.
								</p>
								 </td><td align="left" valign="top" headers="idm139735335055168"> <p>
									Propagates resources to the <code class="literal">ManagedCluster</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735335057344"> <p>
									<code class="literal">Secret</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735335056256"> <p>
									Two CRs are created: <code class="literal">BMC Secret</code> and <code class="literal">Image Pull Secret</code>.
								</p>
								 </td><td align="left" valign="top" headers="idm139735335055168"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">BMC Secret</code> authenticates into the target bare-metal host using its username and password.
										</li><li class="listitem">
											<code class="literal">Image Pull Secret</code> contains authentication information for the OpenShift Container Platform image installed on the target bare-metal host.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735335057344"> <p>
									<code class="literal">ClusterImageSet</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139735335056256"> <p>
									Contains OpenShift Container Platform image information such as the repository and image name.
								</p>
								 </td><td align="left" valign="top" headers="idm139735335055168"> <p>
									Passed into resources to provide OpenShift Container Platform images.
								</p>
								 </td></tr></tbody></table></div></div></section></section><section class="section" id="sno-configure-for-vdu"><div class="titlepage"><div><div><h2 class="title">16.6. Recommended single-node OpenShift cluster configuration for vDU application workloads</h2></div></div></div><p>
				Use the following reference information to understand the single-node OpenShift configurations required to deploy virtual distributed unit (vDU) applications in the cluster. Configurations include cluster optimizations for high performance workloads, enabling workload partitioning, and minimizing the number of reboots required post-installation.
			</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						To deploy a single cluster by hand, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-manual-install">Manually installing a single-node OpenShift cluster with GitOps ZTP</a>.
					</li><li class="listitem">
						To deploy a fleet of clusters using GitOps Zero Touch Provisioning (ZTP), see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-deploying-far-edge-sites">Deploying far edge sites with GitOps ZTP</a>.
					</li></ul></div><section class="section" id="ztp-low-latency_sno-configure-for-vdu"><div class="titlepage"><div><div><h3 class="title">16.6.1. Running low latency applications on OpenShift Container Platform</h3></div></div></div><p>
					OpenShift Container Platform enables low latency processing for applications running on commercial off-the-shelf (COTS) hardware by using several technologies and specialized hardware devices:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Real-time kernel for RHCOS</span></dt><dd>
								Ensures workloads are handled with a high degree of process determinism.
							</dd><dt><span class="term">CPU isolation</span></dt><dd>
								Avoids CPU scheduling delays and ensures CPU capacity is available consistently.
							</dd><dt><span class="term">NUMA-aware topology management</span></dt><dd>
								Aligns memory and huge pages with CPU and PCI devices to pin guaranteed container memory and huge pages to the non-uniform memory access (NUMA) node. Pod resources for all Quality of Service (QoS) classes stay on the same NUMA node. This decreases latency and improves performance of the node.
							</dd><dt><span class="term">Huge pages memory management</span></dt><dd>
								Using huge page sizes improves system performance by reducing the amount of system resources required to access page tables.
							</dd><dt><span class="term">Precision timing synchronization using PTP</span></dt><dd>
								Allows synchronization between nodes in the network with sub-microsecond accuracy.
							</dd></dl></div></section><section class="section" id="ztp-install-sno-hardware-reqs_sno-configure-for-vdu"><div class="titlepage"><div><div><h3 class="title">16.6.2. Recommended cluster host requirements for vDU application workloads</h3></div></div></div><p>
					Running vDU application workloads requires a bare-metal host with sufficient resources to run OpenShift Container Platform services and production workloads.
				</p><div class="table" id="idm139735324496224"><p class="title"><strong>Table 16.8. Minimum resource requirements</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735327208368" scope="col">Profile</th><th align="left" valign="top" id="idm139735327207280" scope="col">vCPU</th><th align="left" valign="top" id="idm139735327206192" scope="col">Memory</th><th align="left" valign="top" id="idm139735327205104" scope="col">Storage</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735327208368"> <p>
									Minimum
								</p>
								 </td><td align="left" valign="top" headers="idm139735327207280"> <p>
									4 to 8 vCPU cores
								</p>
								 </td><td align="left" valign="top" headers="idm139735327206192"> <p>
									32GB of RAM
								</p>
								 </td><td align="left" valign="top" headers="idm139735327205104"> <p>
									120GB
								</p>
								 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						One vCPU is equivalent to one physical core when simultaneous multithreading (SMT), or Hyper-Threading, is not enabled. When enabled, use the following formula to calculate the corresponding ratio:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								(threads per core × cores) × sockets = vCPUs
							</li></ul></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The server must have a Baseboard Management Controller (BMC) when booting with virtual media.
					</p></div></div></section><section class="section" id="ztp-du-configuring-host-firmware-requirements_sno-configure-for-vdu"><div class="titlepage"><div><div><h3 class="title">16.6.3. Configuring host firmware for low latency and high performance</h3></div></div></div><p>
					Bare-metal hosts require the firmware to be configured before the host can be provisioned. The firmware configuration is dependent on the specific hardware and the particular requirements of your installation.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Set the <span class="strong strong"><strong>UEFI/BIOS Boot Mode</strong></span> to <code class="literal">UEFI</code>.
						</li><li class="listitem">
							In the host boot sequence order, set <span class="strong strong"><strong>Hard drive first</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Apply the specific firmware configuration for your hardware. The following table describes a representative firmware configuration for an Intel Xeon Skylake or Intel Cascade Lake server, based on the Intel FlexRAN 4G and 5G baseband PHY reference design.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								The exact firmware configuration depends on your specific hardware and network requirements. The following sample configuration is for illustrative purposes only.
							</p></div></div><div class="table" id="idm139735327182768"><p class="title"><strong>Table 16.9. Sample firmware configuration for an Intel Xeon Skylake or Cascade Lake server</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735327177232" scope="col">Firmware setting</th><th align="left" valign="top" id="idm139735327176144" scope="col">Configuration</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735327177232"> <p>
											CPU Power and Performance Policy
										</p>
										 </td><td align="left" valign="top" headers="idm139735327176144"> <p>
											Performance
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139735327177232"> <p>
											Uncore Frequency Scaling
										</p>
										 </td><td align="left" valign="top" headers="idm139735327176144"> <p>
											Disabled
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139735327177232"> <p>
											Performance P-limit
										</p>
										 </td><td align="left" valign="top" headers="idm139735327176144"> <p>
											Disabled
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139735327177232"> <p>
											Enhanced Intel SpeedStep ® Tech
										</p>
										 </td><td align="left" valign="top" headers="idm139735327176144"> <p>
											Enabled
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139735327177232"> <p>
											Intel Configurable TDP
										</p>
										 </td><td align="left" valign="top" headers="idm139735327176144"> <p>
											Enabled
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139735327177232"> <p>
											Configurable TDP Level
										</p>
										 </td><td align="left" valign="top" headers="idm139735327176144"> <p>
											Level 2
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139735327177232"> <p>
											Intel® Turbo Boost Technology
										</p>
										 </td><td align="left" valign="top" headers="idm139735327176144"> <p>
											Enabled
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139735327177232"> <p>
											Energy Efficient Turbo
										</p>
										 </td><td align="left" valign="top" headers="idm139735327176144"> <p>
											Disabled
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139735327177232"> <p>
											Hardware P-States
										</p>
										 </td><td align="left" valign="top" headers="idm139735327176144"> <p>
											Disabled
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139735327177232"> <p>
											Package C-State
										</p>
										 </td><td align="left" valign="top" headers="idm139735327176144"> <p>
											C0/C1 state
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139735327177232"> <p>
											C1E
										</p>
										 </td><td align="left" valign="top" headers="idm139735327176144"> <p>
											Disabled
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139735327177232"> <p>
											Processor C6
										</p>
										 </td><td align="left" valign="top" headers="idm139735327176144"> <p>
											Disabled
										</p>
										 </td></tr></tbody></table></div></div></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Enable global SR-IOV and VT-d settings in the firmware for the host. These settings are relevant to bare-metal environments.
					</p></div></div></section><section class="section" id="ztp-managed-cluster-network-prereqs_sno-configure-for-vdu"><div class="titlepage"><div><div><h3 class="title">16.6.4. Connectivity prerequisites for managed cluster networks</h3></div></div></div><p>
					Before you can install and provision a managed cluster with the GitOps Zero Touch Provisioning (ZTP) pipeline, the managed cluster host must meet the following networking prerequisites:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							There must be bi-directional connectivity between the GitOps ZTP container in the hub cluster and the Baseboard Management Controller (BMC) of the target bare-metal host.
						</li><li class="listitem"><p class="simpara">
							The managed cluster must be able to resolve and reach the API hostname of the hub hostname and <code class="literal">*.apps</code> hostname. Here is an example of the API hostname of the hub and <code class="literal">*.apps</code> hostname:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<code class="literal">api.hub-cluster.internal.domain.com</code>
								</li><li class="listitem">
									<code class="literal">console-openshift-console.apps.hub-cluster.internal.domain.com</code>
								</li></ul></div></li><li class="listitem"><p class="simpara">
							The hub cluster must be able to resolve and reach the API and <code class="literal">*.apps</code> hostname of the managed cluster. Here is an example of the API hostname of the managed cluster and <code class="literal">*.apps</code> hostname:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<code class="literal">api.sno-managed-cluster-1.internal.domain.com</code>
								</li><li class="listitem">
									<code class="literal">console-openshift-console.apps.sno-managed-cluster-1.internal.domain.com</code>
								</li></ul></div></li></ul></div></section><section class="section" id="ztp-workload-partitioning-sno_sno-configure-for-vdu"><div class="titlepage"><div><div><h3 class="title">16.6.5. Workload partitioning in single-node OpenShift with GitOps ZTP</h3></div></div></div><p>
					Workload partitioning configures OpenShift Container Platform services, cluster management workloads, and infrastructure pods to run on a reserved number of host CPUs.
				</p><p>
					To configure workload partitioning with GitOps Zero Touch Provisioning (ZTP), you configure a <code class="literal">cpuPartitioningMode</code> field in the <code class="literal">SiteConfig</code> custom resource (CR) that you use to install the cluster and you apply a <code class="literal">PerformanceProfile</code> CR that configures the <code class="literal">isolated</code> and <code class="literal">reserved</code> CPUs on the host.
				</p><p>
					Configuring the <code class="literal">SiteConfig</code> CR enables workload partitioning at cluster installation time and applying the <code class="literal">PerformanceProfile</code> CR configures the specific allocation of CPUs to reserved and isolated sets. Both of these steps happen at different points during cluster provisioning.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Configuring workload partitioning by using the <code class="literal">cpuPartitioningMode</code> field in the <code class="literal">SiteConfig</code> CR is a Tech Preview feature in OpenShift Container Platform 4.13.
					</p><p>
						Alternatively, you can specify cluster management CPU resources with the <code class="literal">cpuset</code> field of the <code class="literal">SiteConfig</code> custom resource (CR) and the <code class="literal">reserved</code> field of the group <code class="literal">PolicyGenTemplate</code> CR. The GitOps ZTP pipeline uses these values to populate the required fields in the workload partitioning <code class="literal">MachineConfig</code> CR (<code class="literal">cpuset</code>) and the <code class="literal">PerformanceProfile</code> CR (<code class="literal">reserved</code>) that configure the single-node OpenShift cluster. This method is a General Availability feature in OpenShift Container Platform 4.14.
					</p></div></div><p>
					The workload partitioning configuration pins the OpenShift Container Platform infrastructure pods to the <code class="literal">reserved</code> CPU set. Platform services such as systemd, CRI-O, and kubelet run on the <code class="literal">reserved</code> CPU set. The <code class="literal">isolated</code> CPU sets are exclusively allocated to your container workloads. Isolating CPUs ensures that the workload has guaranteed access to the specified CPUs without contention from other applications running on the same node. All CPUs that are not isolated should be reserved.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Ensure that <code class="literal">reserved</code> and <code class="literal">isolated</code> CPU sets do not overlap with each other.
					</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For the recommended single-node OpenShift workload partitioning configuration, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-sno-du-enabling-workload-partitioning_sno-configure-for-vdu">Workload partitioning</a>.
						</li></ul></div></section><section class="section" id="ztp-sno-install-time-cluster-config"><div class="titlepage"><div><div><h3 class="title">16.6.6. Recommended cluster install manifests</h3></div></div></div><p>
					The ZTP pipeline applies the following custom resources (CRs) during cluster installation. These configuration CRs ensure that the cluster meets the feature and performance requirements necessary for running a vDU application.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						When using the GitOps ZTP plugin and <code class="literal">SiteConfig</code> CRs for cluster deployment, the following <code class="literal">MachineConfig</code> CRs are included by default.
					</p></div></div><p>
					Use the <code class="literal">SiteConfig</code> <code class="literal">extraManifests</code> filter to alter the CRs that are included by default. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-advanced-install-ztp">Advanced managed cluster configuration with SiteConfig CRs</a>.
				</p><section class="section" id="ztp-sno-du-enabling-workload-partitioning_sno-configure-for-vdu"><div class="titlepage"><div><div><h4 class="title">16.6.6.1. Workload partitioning</h4></div></div></div><p>
						Single-node OpenShift clusters that run DU workloads require workload partitioning. This limits the cores allowed to run platform services, maximizing the CPU core for application payloads.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Workload partitioning can be enabled during cluster installation only. You cannot disable workload partitioning post-installation. You can however change the set of CPUs assigned to the isolated and reserved sets through the <code class="literal">PerformanceProfile</code> CR. Changes to CPU settings cause the node to reboot.
						</p></div></div><div class="admonition note"><div class="admonition_header">Upgrading from OpenShift Container Platform 4.12 to 4.13+</div><div><p>
							When transitioning to using <code class="literal">cpuPartitioningMode</code> for enabling workload partitioning, remove the workload partitioning <code class="literal">MachineConfig</code> CRs from the <code class="literal">/extra-manifest</code> folder that you use to provision the cluster.
						</p></div></div><div class="formalpara"><p class="title"><strong>Recommended SiteConfig CR configuration for workload partitioning</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1
kind: SiteConfig
metadata:
  name: "&lt;site_name&gt;"
  namespace: "&lt;site_name&gt;"
spec:
  baseDomain: "example.com"
  cpuPartitioningMode: AllNodes <span id="CO54-1"><!--Empty--></span><span class="callout">1</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO54-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Set the <code class="literal">cpuPartitioningMode</code> field to <code class="literal">AllNodes</code> to configure workload partitioning for all nodes in the cluster.
							</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
							Check that the applications and cluster system CPU pinning is correct. Run the following commands:
						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Open a remote shell prompt to the managed cluster:
							</p><pre class="programlisting language-terminal">$ oc debug node/example-sno-1</pre></li><li class="listitem"><p class="simpara">
								Check that the user applications CPU pinning is correct:
							</p><pre class="programlisting language-terminal">sh-4.4# pgrep ovn | while read i; do taskset -cp $i; done</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">pid 8481's current affinity list: 0-3
pid 8726's current affinity list: 0-3
pid 9088's current affinity list: 0-3
pid 9945's current affinity list: 0-3
pid 10387's current affinity list: 0-3
pid 12123's current affinity list: 0-3
pid 13313's current affinity list: 0-3</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Check that the system applications CPU pinning is correct:
							</p><pre class="programlisting language-terminal">sh-4.4# pgrep systemd | while read i; do taskset -cp $i; done</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">pid 1's current affinity list: 0-3
pid 938's current affinity list: 0-3
pid 962's current affinity list: 0-3
pid 1197's current affinity list: 0-3</pre>

								</p></div></li></ol></div></section><section class="section" id="ztp-sno-du-configuring-the-container-mountspace_sno-configure-for-vdu"><div class="titlepage"><div><div><h4 class="title">16.6.6.2. Reduced platform management footprint</h4></div></div></div><p>
						To reduce the overall management footprint of the platform, a <code class="literal">MachineConfig</code> custom resource (CR) is required that places all Kubernetes-specific mount points in a new namespace separate from the host operating system. The following base64-encoded example <code class="literal">MachineConfig</code> CR illustrates this configuration.
					</p><div class="formalpara"><p class="title"><strong>Recommended container mount namespace configuration (01-container-mount-ns-and-kubelet-conf-master.yaml)</strong></p><p>
							
<pre class="programlisting language-yaml"># Automatically generated by extra-manifests-builder
# Do not make changes directly.
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: container-mount-namespace-and-kubelet-conf-master
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
      - contents:
          source: data:text/plain;charset=utf-8;base64,IyEvYmluL2Jhc2gKCmRlYnVnKCkgewogIGVjaG8gJEAgPiYyCn0KCnVzYWdlKCkgewogIGVjaG8gVXNhZ2U6ICQoYmFzZW5hbWUgJDApIFVOSVQgW2VudmZpbGUgW3Zhcm5hbWVdXQogIGVjaG8KICBlY2hvIEV4dHJhY3QgdGhlIGNvbnRlbnRzIG9mIHRoZSBmaXJzdCBFeGVjU3RhcnQgc3RhbnphIGZyb20gdGhlIGdpdmVuIHN5c3RlbWQgdW5pdCBhbmQgcmV0dXJuIGl0IHRvIHN0ZG91dAogIGVjaG8KICBlY2hvICJJZiAnZW52ZmlsZScgaXMgcHJvdmlkZWQsIHB1dCBpdCBpbiB0aGVyZSBpbnN0ZWFkLCBhcyBhbiBlbnZpcm9ubWVudCB2YXJpYWJsZSBuYW1lZCAndmFybmFtZSciCiAgZWNobyAiRGVmYXVsdCAndmFybmFtZScgaXMgRVhFQ1NUQVJUIGlmIG5vdCBzcGVjaWZpZWQiCiAgZXhpdCAxCn0KClVOSVQ9JDEKRU5WRklMRT0kMgpWQVJOQU1FPSQzCmlmIFtbIC16ICRVTklUIHx8ICRVTklUID09ICItLWhlbHAiIHx8ICRVTklUID09ICItaCIgXV07IHRoZW4KICB1c2FnZQpmaQpkZWJ1ZyAiRXh0cmFjdGluZyBFeGVjU3RhcnQgZnJvbSAkVU5JVCIKRklMRT0kKHN5c3RlbWN0bCBjYXQgJFVOSVQgfCBoZWFkIC1uIDEpCkZJTEU9JHtGSUxFI1wjIH0KaWYgW1sgISAtZiAkRklMRSBdXTsgdGhlbgogIGRlYnVnICJGYWlsZWQgdG8gZmluZCByb290IGZpbGUgZm9yIHVuaXQgJFVOSVQgKCRGSUxFKSIKICBleGl0CmZpCmRlYnVnICJTZXJ2aWNlIGRlZmluaXRpb24gaXMgaW4gJEZJTEUiCkVYRUNTVEFSVD0kKHNlZCAtbiAtZSAnL15FeGVjU3RhcnQ9LipcXCQvLC9bXlxcXSQvIHsgcy9eRXhlY1N0YXJ0PS8vOyBwIH0nIC1lICcvXkV4ZWNTdGFydD0uKlteXFxdJC8geyBzL15FeGVjU3RhcnQ9Ly87IHAgfScgJEZJTEUpCgppZiBbWyAkRU5WRklMRSBdXTsgdGhlbgogIFZBUk5BTUU9JHtWQVJOQU1FOi1FWEVDU1RBUlR9CiAgZWNobyAiJHtWQVJOQU1FfT0ke0VYRUNTVEFSVH0iID4gJEVOVkZJTEUKZWxzZQogIGVjaG8gJEVYRUNTVEFSVApmaQo=
        mode: 493
        path: /usr/local/bin/extractExecStart
      - contents:
          source: data:text/plain;charset=utf-8;base64,IyEvYmluL2Jhc2gKbnNlbnRlciAtLW1vdW50PS9ydW4vY29udGFpbmVyLW1vdW50LW5hbWVzcGFjZS9tbnQgIiRAIgo=
        mode: 493
        path: /usr/local/bin/nsenterCmns
    systemd:
      units:
      - contents: |
          [Unit]
          Description=Manages a mount namespace that both kubelet and crio can use to share their container-specific mounts

          [Service]
          Type=oneshot
          RemainAfterExit=yes
          RuntimeDirectory=container-mount-namespace
          Environment=RUNTIME_DIRECTORY=%t/container-mount-namespace
          Environment=BIND_POINT=%t/container-mount-namespace/mnt
          ExecStartPre=bash -c "findmnt ${RUNTIME_DIRECTORY} || mount --make-unbindable --bind ${RUNTIME_DIRECTORY} ${RUNTIME_DIRECTORY}"
          ExecStartPre=touch ${BIND_POINT}
          ExecStart=unshare --mount=${BIND_POINT} --propagation slave mount --make-rshared /
          ExecStop=umount -R ${RUNTIME_DIRECTORY}
        name: container-mount-namespace.service
      - dropins:
        - contents: |
            [Unit]
            Wants=container-mount-namespace.service
            After=container-mount-namespace.service

            [Service]
            ExecStartPre=/usr/local/bin/extractExecStart %n /%t/%N-execstart.env ORIG_EXECSTART
            EnvironmentFile=-/%t/%N-execstart.env
            ExecStart=
            ExecStart=bash -c "nsenter --mount=%t/container-mount-namespace/mnt \
                ${ORIG_EXECSTART}"
          name: 90-container-mount-namespace.conf
        name: crio.service
      - dropins:
        - contents: |
            [Unit]
            Wants=container-mount-namespace.service
            After=container-mount-namespace.service

            [Service]
            ExecStartPre=/usr/local/bin/extractExecStart %n /%t/%N-execstart.env ORIG_EXECSTART
            EnvironmentFile=-/%t/%N-execstart.env
            ExecStart=
            ExecStart=bash -c "nsenter --mount=%t/container-mount-namespace/mnt \
                ${ORIG_EXECSTART} --housekeeping-interval=30s"
          name: 90-container-mount-namespace.conf
        - contents: |
            [Service]
            Environment="OPENSHIFT_MAX_HOUSEKEEPING_INTERVAL_DURATION=60s"
            Environment="OPENSHIFT_EVICTION_MONITORING_PERIOD_DURATION=30s"
          name: 30-kubelet-interval-tuning.conf
        name: kubelet.service</pre>

						</p></div></section><section class="section" id="ztp-sno-du-enabling-sctp_sno-configure-for-vdu"><div class="titlepage"><div><div><h4 class="title">16.6.6.3. SCTP</h4></div></div></div><p>
						Stream Control Transmission Protocol (SCTP) is a key protocol used in RAN applications. This <code class="literal">MachineConfig</code> object adds the SCTP kernel module to the node to enable this protocol.
					</p><div class="formalpara"><p class="title"><strong>Recommended SCTP configuration (03-sctp-machine-config-master.yaml)</strong></p><p>
							
<pre class="programlisting language-yaml"># Automatically generated by extra-manifests-builder
# Do not make changes directly.
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: load-sctp-module-master
spec:
  config:
    ignition:
      version: 2.2.0
    storage:
      files:
        - contents:
            source: data:,
            verification: {}
          filesystem: root
          mode: 420
          path: /etc/modprobe.d/sctp-blacklist.conf
        - contents:
            source: data:text/plain;charset=utf-8,sctp
          filesystem: root
          mode: 420
          path: /etc/modules-load.d/sctp-load.conf</pre>

						</p></div></section><section class="section" id="ztp-sno-du-accelerating-container-startup_sno-configure-for-vdu"><div class="titlepage"><div><div><h4 class="title">16.6.6.4. Accelerated container startup</h4></div></div></div><p>
						The following <code class="literal">MachineConfig</code> CR configures core OpenShift processes and containers to use all available CPU cores during system startup and shutdown. This accelerates the system recovery during initial boot and reboots.
					</p><div class="formalpara"><p class="title"><strong>Recommended accelerated container startup configuration (04-accelerated-container-startup-master.yaml)</strong></p><p>
							
<pre class="programlisting language-yaml"># Automatically generated by extra-manifests-builder
# Do not make changes directly.
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 04-accelerated-container-startup-master
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
      - contents:
          source: data:text/plain;charset=utf-8;base64,#!/bin/bash
#
# Temporarily reset the core system processes's CPU affinity to be unrestricted to accelerate startup and shutdown
#
# The defaults below can be overridden via environment variables
#

# The default set of critical processes whose affinity should be temporarily unbound:
CRITICAL_PROCESSES=${CRITICAL_PROCESSES:-"crio kubelet NetworkManager conmon dbus"}

# Default wait time is 600s = 10m:
MAXIMUM_WAIT_TIME=${MAXIMUM_WAIT_TIME:-600}

# Default steady-state threshold = 2%
# Allowed values:
#  4  - absolute pod count (+/-)
#  4% - percent change (+/-)
#  -1 - disable the steady-state check
STEADY_STATE_THRESHOLD=${STEADY_STATE_THRESHOLD:-2%}

# Default steady-state window = 60s
# If the running pod count stays within the given threshold for this time
# period, return CPU utilization to normal before the maximum wait time has
# expires
STEADY_STATE_WINDOW=${STEADY_STATE_WINDOW:-60}

# Default steady-state allows any pod count to be "steady state"
# Increasing this will skip any steady-state checks until the count rises above
# this number to avoid false positives if there are some periods where the
# count doesn't increase but we know we can't be at steady-state yet.
STEADY_STATE_MINIMUM=${STEADY_STATE_MINIMUM:-0}

#######################################################

KUBELET_CPU_STATE=/var/lib/kubelet/cpu_manager_state
FULL_CPU_STATE=/sys/fs/cgroup/cpuset/cpuset.cpus
KUBELET_CONF=/etc/kubernetes/kubelet.conf
unrestrictedCpuset() {
  local cpus
  if [[ -e $KUBELET_CPU_STATE ]]; then
    cpus=$(jq -r '.defaultCpuSet' <$KUBELET_CPU_STATE)
    if [[ -n "${cpus}" && -e ${KUBELET_CONF} ]]; then
      reserved_cpus=$(jq -r '.reservedSystemCPUs' </etc/kubernetes/kubelet.conf)
      if [[ -n "${reserved_cpus}" ]]; then
        # Use taskset to merge the two cpusets
        cpus=$(taskset -c "${reserved_cpus},${cpus}" grep -i Cpus_allowed_list /proc/self/status | awk '{print $2}')
      fi
    fi
  fi
  if [[ -z $cpus ]]; then
    # fall back to using all cpus if the kubelet state is not configured yet
    [[ -e $FULL_CPU_STATE ]] || return 1
    cpus=$(<$FULL_CPU_STATE)
  fi
  echo $cpus
}

restrictedCpuset() {
  for arg in $(</proc/cmdline); do
    if [[ $arg =~ ^systemd.cpu_affinity= ]]; then
      echo ${arg#*=}
      return 0
    fi
  done
  return 1
}

resetAffinity() {
  local cpuset="$1"
  local failcount=0
  local successcount=0
  logger "Recovery: Setting CPU affinity for critical processes \"$CRITICAL_PROCESSES\" to $cpuset"
  for proc in $CRITICAL_PROCESSES; do
    local pids="$(pgrep $proc)"
    for pid in $pids; do
      local tasksetOutput
      tasksetOutput="$(taskset -apc "$cpuset" $pid 2>&1)"
      if [[ $? -ne 0 ]]; then
        echo "ERROR: $tasksetOutput"
        ((failcount++))
      else
        ((successcount++))
      fi
    done
  done

  logger "Recovery: Re-affined $successcount pids successfully"
  if [[ $failcount -gt 0 ]]; then
    logger "Recovery: Failed to re-affine $failcount processes"
    return 1
  fi
}

setUnrestricted() {
  logger "Recovery: Setting critical system processes to have unrestricted CPU access"
  resetAffinity "$(unrestrictedCpuset)"
}

setRestricted() {
  logger "Recovery: Resetting critical system processes back to normally restricted access"
  resetAffinity "$(restrictedCpuset)"
}

currentAffinity() {
  local pid="$1"
  taskset -pc $pid | awk -F': ' '{print $2}'
}

within() {
  local last=$1 current=$2 threshold=$3
  local delta=0 pchange
  delta=$(( current - last ))
  if [[ $current -eq $last ]]; then
    pchange=0
  elif [[ $last -eq 0 ]]; then
    pchange=1000000
  else
    pchange=$(( ( $delta * 100) / last ))
  fi
  echo -n "last:$last current:$current delta:$delta pchange:${pchange}%: "
  local absolute limit
  case $threshold in
    *%)
      absolute=${pchange##-} # absolute value
      limit=${threshold%%%}
      ;;
    *)
      absolute=${delta##-} # absolute value
      limit=$threshold
      ;;
  esac
  if [[ $absolute -le $limit ]]; then
    echo "within (+/-)$threshold"
    return 0
  else
    echo "outside (+/-)$threshold"
    return 1
  fi
}

steadystate() {
  local last=$1 current=$2
  if [[ $last -lt $STEADY_STATE_MINIMUM ]]; then
    echo "last:$last current:$current Waiting to reach $STEADY_STATE_MINIMUM before checking for steady-state"
    return 1
  fi
  within $last $current $STEADY_STATE_THRESHOLD
}

waitForReady() {
  logger "Recovery: Waiting ${MAXIMUM_WAIT_TIME}s for the initialization to complete"
  local lastSystemdCpuset="$(currentAffinity 1)"
  local lastDesiredCpuset="$(unrestrictedCpuset)"
  local t=0 s=10
  local lastCcount=0 ccount=0 steadyStateTime=0
  while [[ $t -lt $MAXIMUM_WAIT_TIME ]]; do
    sleep $s
    ((t += s))
    # Re-check the current affinity of systemd, in case some other process has changed it
    local systemdCpuset="$(currentAffinity 1)"
    # Re-check the unrestricted Cpuset, as the allowed set of unreserved cores may change as pods are assigned to cores
    local desiredCpuset="$(unrestrictedCpuset)"
    if [[ $systemdCpuset != $lastSystemdCpuset || $lastDesiredCpuset != $desiredCpuset ]]; then
      resetAffinity "$desiredCpuset"
      lastSystemdCpuset="$(currentAffinity 1)"
      lastDesiredCpuset="$desiredCpuset"
    fi

    # Detect steady-state pod count
    ccount=$(crictl ps | wc -l)
    if steadystate $lastCcount $ccount; then
      ((steadyStateTime += s))
      echo "Steady-state for ${steadyStateTime}s/${STEADY_STATE_WINDOW}s"
      if [[ $steadyStateTime -ge $STEADY_STATE_WINDOW ]]; then
        logger "Recovery: Steady-state (+/- $STEADY_STATE_THRESHOLD) for ${STEADY_STATE_WINDOW}s: Done"
        return 0
      fi
    else
      if [[ $steadyStateTime -gt 0 ]]; then
        echo "Resetting steady-state timer"
        steadyStateTime=0
      fi
    fi
    lastCcount=$ccount
  done
  logger "Recovery: Recovery Complete Timeout"
}

main() {
  if ! unrestrictedCpuset >&/dev/null; then
    logger "Recovery: No unrestricted Cpuset could be detected"
    return 1
  fi

  if ! restrictedCpuset >&/dev/null; then
    logger "Recovery: No restricted Cpuset has been configured.  We are already running unrestricted."
    return 0
  fi

  # Ensure we reset the CPU affinity when we exit this script for any reason
  # This way either after the timer expires or after the process is interrupted
  # via ^C or SIGTERM, we return things back to the way they should be.
  trap setRestricted EXIT

  logger "Recovery: Recovery Mode Starting"
  setUnrestricted
  waitForReady
}

if [[ "${BASH_SOURCE[0]}" = "${0}" ]]; then
  main "${@}"
  exit $?
fi

        mode: 493
        path: /usr/local/bin/accelerated-container-startup.sh
    systemd:
      units:
      - contents: |
          [Unit]
          Description=Unlocks more CPUs for critical system processes during container startup

          [Service]
          Type=simple
          ExecStart=/usr/local/bin/accelerated-container-startup.sh

          # Maximum wait time is 600s = 10m:
          Environment=MAXIMUM_WAIT_TIME=600

          # Steady-state threshold = 2%
          # Allowed values:
          #  4  - absolute pod count (+/-)
          #  4% - percent change (+/-)
          #  -1 - disable the steady-state check
          # Note: '%' must be escaped as '%%' in systemd unit files
          Environment=STEADY_STATE_THRESHOLD=2%%

          # Steady-state window = 120s
          # If the running pod count stays within the given threshold for this time
          # period, return CPU utilization to normal before the maximum wait time has
          # expires
          Environment=STEADY_STATE_WINDOW=120

          # Steady-state minimum = 40
          # Increasing this will skip any steady-state checks until the count rises above
          # this number to avoid false positives if there are some periods where the
          # count doesn't increase but we know we can't be at steady-state yet.
          Environment=STEADY_STATE_MINIMUM=40

          [Install]
          WantedBy=multi-user.target
        enabled: true
        name: accelerated-container-startup.service
      - contents: |
          [Unit]
          Description=Unlocks more CPUs for critical system processes during container shutdown
          DefaultDependencies=no

          [Service]
          Type=simple
          ExecStart=/usr/local/bin/accelerated-container-startup.sh

          # Maximum wait time is 600s = 10m:
          Environment=MAXIMUM_WAIT_TIME=600

          # Steady-state threshold
          # Allowed values:
          #  4  - absolute pod count (+/-)
          #  4% - percent change (+/-)
          #  -1 - disable the steady-state check
          # Note: '%' must be escaped as '%%' in systemd unit files
          Environment=STEADY_STATE_THRESHOLD=-1

          # Steady-state window = 60s
          # If the running pod count stays within the given threshold for this time
          # period, return CPU utilization to normal before the maximum wait time has
          # expires
          Environment=STEADY_STATE_WINDOW=60

          [Install]
          WantedBy=shutdown.target reboot.target halt.target
        enabled: true
        name: accelerated-container-shutdown.service</pre>

						</p></div></section><section class="section" id="ztp-sno-du-enabling-kdump_sno-configure-for-vdu"><div class="titlepage"><div><div><h4 class="title">16.6.6.5. Automatic kernel crash dumps with kdump</h4></div></div></div><p>
						<code class="literal">kdump</code> is a Linux kernel feature that creates a kernel crash dump when the kernel crashes. <code class="literal">kdump</code> is enabled with the following <code class="literal">MachineConfig</code> CRs.
					</p><div class="formalpara"><p class="title"><strong>Recommended MachineConfig to remove ice driver (05-kdump-config-master.yaml)</strong></p><p>
							
<pre class="programlisting language-yaml"># Automatically generated by extra-manifests-builder
# Do not make changes directly.
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 05-kdump-config-master
spec:
  config:
    ignition:
      version: 3.2.0
    systemd:
      units:
      - enabled: true
        name: kdump-remove-ice-module.service
        contents: |
          [Unit]
          Description=Remove ice module when doing kdump
          Before=kdump.service
          [Service]
          Type=oneshot
          RemainAfterExit=true
          ExecStart=/usr/local/bin/kdump-remove-ice-module.sh
          [Install]
          WantedBy=multi-user.target
    storage:
      files:
        - contents:
            source: data:text/plain;charset=utf-8;base64,IyEvdXNyL2Jpbi9lbnYgYmFzaAoKIyBUaGlzIHNjcmlwdCByZW1vdmVzIHRoZSBpY2UgbW9kdWxlIGZyb20ga2R1bXAgdG8gcHJldmVudCBrZHVtcCBmYWlsdXJlcyBvbiBjZXJ0YWluIHNlcnZlcnMuCiMgVGhpcyBpcyBhIHRlbXBvcmFyeSB3b3JrYXJvdW5kIGZvciBSSEVMUExBTi0xMzgyMzYgYW5kIGNhbiBiZSByZW1vdmVkIHdoZW4gdGhhdCBpc3N1ZSBpcwojIGZpeGVkLgoKc2V0IC14CgpTRUQ9Ii91c3IvYmluL3NlZCIKR1JFUD0iL3Vzci9iaW4vZ3JlcCIKCiMgb3ZlcnJpZGUgZm9yIHRlc3RpbmcgcHVycG9zZXMKS0RVTVBfQ09ORj0iJHsxOi0vZXRjL3N5c2NvbmZpZy9rZHVtcH0iClJFTU9WRV9JQ0VfU1RSPSJtb2R1bGVfYmxhY2tsaXN0PWljZSIKCiMgZXhpdCBpZiBmaWxlIGRvZXNuJ3QgZXhpc3QKWyAhIC1mICR7S0RVTVBfQ09ORn0gXSAmJiBleGl0IDAKCiMgZXhpdCBpZiBmaWxlIGFscmVhZHkgdXBkYXRlZAoke0dSRVB9IC1GcSAke1JFTU9WRV9JQ0VfU1RSfSAke0tEVU1QX0NPTkZ9ICYmIGV4aXQgMAoKIyBUYXJnZXQgbGluZSBsb29rcyBzb21ldGhpbmcgbGlrZSB0aGlzOgojIEtEVU1QX0NPTU1BTkRMSU5FX0FQUEVORD0iaXJxcG9sbCBucl9jcHVzPTEgLi4uIGhlc3RfZGlzYWJsZSIKIyBVc2Ugc2VkIHRvIG1hdGNoIGV2ZXJ5dGhpbmcgYmV0d2VlbiB0aGUgcXVvdGVzIGFuZCBhcHBlbmQgdGhlIFJFTU9WRV9JQ0VfU1RSIHRvIGl0CiR7U0VEfSAtaSAncy9eS0RVTVBfQ09NTUFORExJTkVfQVBQRU5EPSJbXiJdKi8mICcke1JFTU9WRV9JQ0VfU1RSfScvJyAke0tEVU1QX0NPTkZ9IHx8IGV4aXQgMAo=
          mode: 448
          path: /usr/local/bin/kdump-remove-ice-module.sh</pre>

						</p></div><div class="formalpara"><p class="title"><strong>Recommended kdump configuration (06-kdump-master.yaml)</strong></p><p>
							
<pre class="programlisting language-yaml"># Automatically generated by extra-manifests-builder
# Do not make changes directly.
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 06-kdump-enable-master
spec:
  config:
    ignition:
      version: 3.2.0
    systemd:
      units:
      - enabled: true
        name: kdump.service
  kernelArguments:
    - crashkernel=512M</pre>

						</p></div></section><section class="section" id="ztp-sno-du-disabling-crio-wipe_sno-configure-for-vdu"><div class="titlepage"><div><div><h4 class="title">16.6.6.6. Disable automatic CRI-O cache wipe</h4></div></div></div><p>
						After an uncontrolled host shutdown or cluster reboot, CRI-O automatically deletes the entire CRI-O cache, causing all images to be pulled from the registry when the node reboots. This can result in unacceptably slow recovery times or recovery failures. To prevent this from happening in single-node OpenShift clusters that you install with GitOps ZTP, disable the CRI-O delete cache feature during cluster installation.
					</p><div class="formalpara"><p class="title"><strong>Recommended MachineConfig CR to disable CRI-O cache wipe on control plane nodes (99-crio-disable-wipe-master.yaml)</strong></p><p>
							
<pre class="programlisting language-yaml"># Automatically generated by extra-manifests-builder
# Do not make changes directly.
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 99-crio-disable-wipe-master
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
        - contents:
            source: data:text/plain;charset=utf-8;base64,W2NyaW9dCmNsZWFuX3NodXRkb3duX2ZpbGUgPSAiIgo=
          mode: 420
          path: /etc/crio/crio.conf.d/99-crio-disable-wipe.toml</pre>

						</p></div><div class="formalpara"><p class="title"><strong>Recommended MachineConfig CR to disable CRI-O cache wipe on worker nodes (99-crio-disable-wipe-worker.yaml)</strong></p><p>
							
<pre class="programlisting language-yaml"># Automatically generated by extra-manifests-builder
# Do not make changes directly.
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 99-crio-disable-wipe-worker
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
        - contents:
            source: data:text/plain;charset=utf-8;base64,W2NyaW9dCmNsZWFuX3NodXRkb3duX2ZpbGUgPSAiIgo=
          mode: 420
          path: /etc/crio/crio.conf.d/99-crio-disable-wipe.toml</pre>

						</p></div></section><section class="section" id="ztp-sno-du-configuring-crun-container-runtime_sno-configure-for-vdu"><div class="titlepage"><div><div><h4 class="title">16.6.6.7. Configuring crun as the default container runtime</h4></div></div></div><p>
						The following <code class="literal">ContainerRuntimeConfig</code> custom resources (CRs) configure crun as the default OCI container runtime for control plane and worker nodes. The crun container runtime is fast and lightweight and has a low memory footprint.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							For optimal performance, enable crun for control plane and worker nodes in single-node OpenShift, three-node OpenShift, and standard clusters. To avoid the cluster rebooting when the CR is applied, apply the change as a GitOps ZTP additional Day 0 install-time manifest.
						</p></div></div><div class="formalpara"><p class="title"><strong>Recommended ContainerRuntimeConfig CR for control plane nodes (enable-crun-master.yaml)</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: ContainerRuntimeConfig
metadata:
 name: enable-crun-master
spec:
 machineConfigPoolSelector:
   matchLabels:
     pools.operator.machineconfiguration.openshift.io/master: ""
 containerRuntimeConfig:
   defaultRuntime: crun</pre>

						</p></div><div class="formalpara"><p class="title"><strong>Recommended ContainerRuntimeConfig CR for worker nodes (enable-crun-worker.yaml)</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: ContainerRuntimeConfig
metadata:
 name: enable-crun-worker
spec:
 machineConfigPoolSelector:
   matchLabels:
     pools.operator.machineconfiguration.openshift.io/worker: ""
 containerRuntimeConfig:
   defaultRuntime: crun</pre>

						</p></div></section></section><section class="section" id="ztp-sno-post-install-time-cluster-config"><div class="titlepage"><div><div><h3 class="title">16.6.7. Recommended post-installation cluster configurations</h3></div></div></div><p>
					When the cluster installation is complete, the ZTP pipeline applies the following custom resources (CRs) that are required to run DU workloads.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						In GitOps ZTP v4.10 and earlier, you configure UEFI secure boot with a <code class="literal">MachineConfig</code> CR. This is no longer required in GitOps ZTP v4.11 and later. In v4.11, you configure UEFI secure boot for single-node OpenShift clusters by updating the <code class="literal">spec.clusters.nodes.bootMode</code> field in the <code class="literal">SiteConfig</code> CR that you use to install the cluster. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-deploying-a-site_ztp-deploying-far-edge-sites">Deploying a managed cluster with SiteConfig and GitOps ZTP</a>.
					</p></div></div><section class="section" id="ztp-sno-du-configuring-the-operators_sno-configure-for-vdu"><div class="titlepage"><div><div><h4 class="title">16.6.7.1. Operator namespaces and Operator groups</h4></div></div></div><p>
						Single-node OpenShift clusters that run DU workloads require the following <code class="literal">OperatorGroup</code> and <code class="literal">Namespace</code> custom resources (CRs):
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Local Storage Operator
							</li><li class="listitem">
								Logging Operator
							</li><li class="listitem">
								PTP Operator
							</li><li class="listitem">
								SR-IOV Network Operator
							</li></ul></div><p>
						The following CRs are required:
					</p><div class="formalpara"><p class="title"><strong>Recommended Storage Operator Namespace and OperatorGroup configuration</strong></p><p>
							
<pre class="programlisting language-yaml">---
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-local-storage
  annotations:
    workload.openshift.io/allowed: management
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: openshift-local-storage
  namespace: openshift-local-storage
spec:
  targetNamespaces:
  - openshift-local-storage</pre>

						</p></div><div class="formalpara"><p class="title"><strong>Recommended Cluster Logging Operator Namespace and OperatorGroup configuration</strong></p><p>
							
<pre class="programlisting language-yaml">---
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-logging
  annotations:
    workload.openshift.io/allowed: management
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: cluster-logging
  namespace: openshift-logging
spec:
  targetNamespaces:
  - openshift-logging</pre>

						</p></div><div class="formalpara"><p class="title"><strong>Recommended PTP Operator Namespace and OperatorGroup configuration</strong></p><p>
							
<pre class="programlisting language-yaml">---
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-ptp
  annotations:
    workload.openshift.io/allowed: management
  labels:
    openshift.io/cluster-monitoring: "true"
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: ptp-operators
  namespace: openshift-ptp
spec:
  targetNamespaces:
  - openshift-ptp</pre>

						</p></div><div class="formalpara"><p class="title"><strong>Recommended SR-IOV Operator Namespace and OperatorGroup configuration</strong></p><p>
							
<pre class="programlisting language-yaml">---
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-sriov-network-operator
  annotations:
    workload.openshift.io/allowed: management
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: sriov-network-operators
  namespace: openshift-sriov-network-operator
spec:
  targetNamespaces:
  - openshift-sriov-network-operator</pre>

						</p></div></section><section class="section" id="ztp-sno-du-subscribing-to-the-operators-needed-for-platform-configuration_sno-configure-for-vdu"><div class="titlepage"><div><div><h4 class="title">16.6.7.2. Operator subscriptions</h4></div></div></div><p>
						Single-node OpenShift clusters that run DU workloads require the following <code class="literal">Subscription</code> CRs. The subscription provides the location to download the following Operators:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Local Storage Operator
							</li><li class="listitem">
								Logging Operator
							</li><li class="listitem">
								PTP Operator
							</li><li class="listitem">
								SR-IOV Network Operator
							</li></ul></div><p>
						For each Operator subscription, specify the channel to get the Operator from. The recommended channel is <code class="literal">stable</code>.
					</p><p>
						You can specify <code class="literal">Manual</code> or <code class="literal">Automatic</code> updates. In <code class="literal">Automatic</code> mode, the Operator automatically updates to the latest versions in the channel as they become available in the registry. In <code class="literal">Manual</code> mode, new Operator versions are installed only when they are explicitly approved.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Use Manual mode for subscriptions. This allows you to control the timing of Operator updates to fit within planned/scheduled maintenance windows.
						</p></div></div><div class="formalpara"><p class="title"><strong>Recommended Local Storage Operator subscription</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: local-storage-operator
  namespace: openshift-local-storage
spec:
  channel: "stable"
  name: local-storage-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  installPlanApproval: Manual
status:
  state: AtLatestKnown</pre>

						</p></div><div class="formalpara"><p class="title"><strong>Recommended SR-IOV Operator subscription</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: sriov-network-operator-subscription
  namespace: openshift-sriov-network-operator
spec:
  channel: "stable"
  name: sriov-network-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  installPlanApproval: Manual
status:
  state: AtLatestKnown</pre>

						</p></div><div class="formalpara"><p class="title"><strong>Recommended PTP Operator subscription</strong></p><p>
							
<pre class="programlisting language-yaml">---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: ptp-operator-subscription
  namespace: openshift-ptp
spec:
  channel: "stable"
  name: ptp-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  installPlanApproval: Manual
status:
  state: AtLatestKnown</pre>

						</p></div><div class="formalpara"><p class="title"><strong>Recommended Cluster Logging Operator subscription</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: cluster-logging
  namespace: openshift-logging
spec:
  channel: "stable"
  name: cluster-logging
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  installPlanApproval: Manual
status:
  state: AtLatestKnown</pre>

						</p></div></section><section class="section" id="ztp-sno-du-configuring-logging-locally-and-forwarding_sno-configure-for-vdu"><div class="titlepage"><div><div><h4 class="title">16.6.7.3. Cluster logging and log forwarding</h4></div></div></div><p>
						Single-node OpenShift clusters that run DU workloads require logging and log forwarding for debugging. The following <code class="literal">ClusterLogging</code> and <code class="literal">ClusterLogForwarder</code> custom resources (CRs) are required.
					</p><div class="formalpara"><p class="title"><strong>Recommended cluster logging and log forwarding configuration</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: logging.openshift.io/v1
kind: ClusterLogging
metadata:
  name: instance
  namespace: openshift-logging
spec:
 managementState: "Managed"
 curation:
   type: "curator"
   curator:
     schedule: "30 3 * * *"
 collection:
   logs:
     type: "fluentd"
     fluentd: {}</pre>

						</p></div><div class="formalpara"><p class="title"><strong>Recommended log forwarding configuration</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: "logging.openshift.io/v1"
kind: ClusterLogForwarder
metadata:
  name: instance
  namespace: openshift-logging
spec:
  outputs:
    - type: "kafka"
      name: kafka-open
      url: tcp://10.46.55.190:9092/test
  inputs:
    - name: infra-logs
      infrastructure: {}
  pipelines:
    - name: audit-logs
      inputRefs:
        - audit
      outputRefs:
        - kafka-open
    - name: infrastructure-logs
      inputRefs:
        - infrastructure
      outputRefs:
        - kafka-open</pre>

						</p></div><p>
						Set the <code class="literal">spec.outputs.url</code> field to the URL of the Kafka server where the logs are forwarded to.
					</p></section><section class="section" id="ztp-sno-du-configuring-performance-addons_sno-configure-for-vdu"><div class="titlepage"><div><div><h4 class="title">16.6.7.4. Performance profile</h4></div></div></div><p>
						Single-node OpenShift clusters that run DU workloads require a Node Tuning Operator performance profile to use real-time host capabilities and services.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							In earlier versions of OpenShift Container Platform, the Performance Addon Operator was used to implement automatic tuning to achieve low latency performance for OpenShift applications. In OpenShift Container Platform 4.11 and later, this functionality is part of the Node Tuning Operator.
						</p></div></div><p>
						The following example <code class="literal">PerformanceProfile</code> CR illustrates the required single-node OpenShift cluster configuration.
					</p><div class="formalpara"><p class="title"><strong>Recommended performance profile configuration</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: openshift-node-performance-profile
spec:
  additionalKernelArgs:
  - "rcupdate.rcu_normal_after_boot=0"
  - "efi=runtime"
  - "module_blacklist=irdma"
  cpu:
    isolated: 2-51,54-103
    reserved: 0-1,52-53
  hugepages:
    defaultHugepagesSize: 1G
    pages:
      - count: 32
        size: 1G
        node: 0
  machineConfigPoolSelector:
    pools.operator.machineconfiguration.openshift.io/master: ""
  nodeSelector:
    node-role.kubernetes.io/master: ''
  numa:
    topologyPolicy: "restricted"
  realTimeKernel:
    enabled: true
  workloadHints:
    realTime: true
    highPowerConsumption: false
    perPodPowerManagement: false</pre>

						</p></div><div class="table" id="idm139735327118144"><p class="title"><strong>Table 16.10. PerformanceProfile CR options for single-node OpenShift clusters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735327112608" scope="col">PerformanceProfile CR field</th><th align="left" valign="top" id="idm139735327111504" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735327112608"> <p>
										<code class="literal">metadata.name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735327111504"> <p>
										Ensure that <code class="literal">name</code> matches the following fields set in related GitOps ZTP custom resources (CRs):
									</p>
									 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												<code class="literal">include=openshift-node-performance-${PerformanceProfile.metadata.name}</code> in <code class="literal">TunedPerformancePatch.yaml</code>
											</li><li class="listitem">
												<code class="literal">name: 50-performance-${PerformanceProfile.metadata.name}</code> in <code class="literal">validatorCRs/informDuValidator.yaml</code>
											</li></ul></div>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735327112608"> <p>
										<code class="literal">spec.additionalKernelArgs</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735327111504"> <p>
										<code class="literal">"efi=runtime"</code> Configures UEFI secure boot for the cluster host.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735327112608"> <p>
										<code class="literal">spec.cpu.isolated</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735327111504"> <p>
										Set the isolated CPUs. Ensure all of the Hyper-Threading pairs match.
									</p>
									 <div class="admonition important"><div class="admonition_header">Important</div><div><p>
											The reserved and isolated CPU pools must not overlap and together must span all available cores. CPU cores that are not accounted for cause an undefined behaviour in the system.
										</p></div></div>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735327112608"> <p>
										<code class="literal">spec.cpu.reserved</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735327111504"> <p>
										Set the reserved CPUs. When workload partitioning is enabled, system processes, kernel threads, and system container threads are restricted to these CPUs. All CPUs that are not isolated should be reserved.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735327112608"> <p>
										<code class="literal">spec.hugepages.pages</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735327111504"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												Set the number of huge pages (<code class="literal">count</code>)
											</li><li class="listitem">
												Set the huge pages size (<code class="literal">size</code>).
											</li><li class="listitem">
												Set <code class="literal">node</code> to the NUMA node where the <code class="literal">hugepages</code> are allocated (<code class="literal">node</code>)
											</li></ul></div>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735327112608"> <p>
										<code class="literal">spec.realTimeKernel</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735327111504"> <p>
										Set <code class="literal">enabled</code> to <code class="literal">true</code> to use the realtime kernel.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735327112608"> <p>
										<code class="literal">spec.workloadHints</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735327111504"> <p>
										Use <code class="literal">workloadHints</code> to define the set of top level flags for different type of workloads. The example configuration configures the cluster for low latency and high performance.
									</p>
									 </td></tr></tbody></table></div></div></section><section class="section" id="ztp-sno-du-configuring-ptp_sno-configure-for-vdu"><div class="titlepage"><div><div><h4 class="title">16.6.7.5. PTP</h4></div></div></div><p>
						Single-node OpenShift clusters use Precision Time Protocol (PTP) for network time synchronization. The following example <code class="literal">PtpConfig</code> CR illustrates the required PTP slave configuration.
					</p><div class="formalpara"><p class="title"><strong>Recommended PTP configuration</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: ptp.openshift.io/v1
kind: PtpConfig
metadata:
  name: slave
  namespace: openshift-ptp
spec:
  profile:
  - name: "slave"
    # The interface name is hardware-specific
    interface: ens5f0
    ptp4lOpts: "-2 -s"
    phc2sysOpts: "-a -r -n 24"
    ptpSchedulingPolicy: SCHED_FIFO
    ptpSchedulingPriority: 10
    ptpSettings:
      logReduce: "true"
    ptp4lConf: |
      [global]
      #
      # Default Data Set
      #
      twoStepFlag 1
      slaveOnly 0
      priority1 128
      priority2 128
      domainNumber 24
      #utc_offset 37
      clockClass 255
      clockAccuracy 0xFE
      offsetScaledLogVariance 0xFFFF
      free_running 0
      freq_est_interval 1
      dscp_event 0
      dscp_general 0
      dataset_comparison G.8275.x
      G.8275.defaultDS.localPriority 128
      #
      # Port Data Set
      #
      logAnnounceInterval -3
      logSyncInterval -4
      logMinDelayReqInterval -4
      logMinPdelayReqInterval -4
      announceReceiptTimeout 3
      syncReceiptTimeout 0
      delayAsymmetry 0
      fault_reset_interval 4
      neighborPropDelayThresh 20000000
      masterOnly 0
      G.8275.portDS.localPriority 128
      #
      # Run time options
      #
      assume_two_step 0
      logging_level 6
      path_trace_enabled 0
      follow_up_info 0
      hybrid_e2e 0
      inhibit_multicast_service 0
      net_sync_monitor 0
      tc_spanning_tree 0
      tx_timestamp_timeout 50
      unicast_listen 0
      unicast_master_table 0
      unicast_req_duration 3600
      use_syslog 1
      verbose 0
      summary_interval 0
      kernel_leap 1
      check_fup_sync 0
      #
      # Servo Options
      #
      pi_proportional_const 0.0
      pi_integral_const 0.0
      pi_proportional_scale 0.0
      pi_proportional_exponent -0.3
      pi_proportional_norm_max 0.7
      pi_integral_scale 0.0
      pi_integral_exponent 0.4
      pi_integral_norm_max 0.3
      step_threshold 2.0
      first_step_threshold 0.00002
      max_frequency 900000000
      clock_servo pi
      sanity_freq_limit 200000000
      ntpshm_segment 0
      #
      # Transport options
      #
      transportSpecific 0x0
      ptp_dst_mac 01:1B:19:00:00:00
      p2p_dst_mac 01:80:C2:00:00:0E
      udp_ttl 1
      udp6_scope 0x0E
      uds_address /var/run/ptp4l
      #
      # Default interface options
      #
      clock_type OC
      network_transport L2
      delay_mechanism E2E
      time_stamping hardware
      tsproc_mode filter
      delay_filter moving_median
      delay_filter_length 10
      egressLatency 0
      ingressLatency 0
      boundary_clock_jbod 0
      #
      # Clock description
      #
      productDescription ;;
      revisionData ;;
      manufacturerIdentity 00:00:00
      userDescription ;
      timeSource 0xA0
  recommend:
  - profile: "slave"
    priority: 4
    match:
    - nodeLabel: "node-role.kubernetes.io/master"</pre>

						</p></div></section><section class="section" id="ztp-sno-du-tuning-the-performance-patch_sno-configure-for-vdu"><div class="titlepage"><div><div><h4 class="title">16.6.7.6. Extended Tuned profile</h4></div></div></div><p>
						Single-node OpenShift clusters that run DU workloads require additional performance tuning configurations necessary for high-performance workloads. The following example <code class="literal">Tuned</code> CR extends the <code class="literal">Tuned</code> profile:
					</p><div class="formalpara"><p class="title"><strong>Recommended extended Tuned profile configuration</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: performance-patch
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
    - name: performance-patch
      data: |
        [main]
        summary=Configuration changes profile inherited from performance created tuned
        include=openshift-node-performance-openshift-node-performance-profile
        [sysctl]
        kernel.timer_migration=1
        [scheduler]
        group.ice-ptp=0:f:10:*:ice-ptp.*
        group.ice-gnss=0:f:10:*:ice-gnss.*
        [service]
        service.stalld=start,enable
        service.chronyd=stop,disable
  recommend:
    - machineConfigLabels:
        machineconfiguration.openshift.io/role: "master"
      priority: 19
      profile: performance-patch</pre>

						</p></div><div class="table" id="idm139735323167744"><p class="title"><strong>Table 16.11. Tuned CR options for single-node OpenShift clusters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735323162144" scope="col">Tuned CR field</th><th align="left" valign="top" id="idm139735323161056" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735323162144"> <p>
										<code class="literal">spec.profile.data</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735323161056"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												The <code class="literal">include</code> line that you set in <code class="literal">spec.profile.data</code> must match the associated <code class="literal">PerformanceProfile</code> CR name. For example, <code class="literal">include=openshift-node-performance-${PerformanceProfile.metadata.name}</code>.
											</li><li class="listitem">
												When using the non-realtime kernel, remove the <code class="literal">timer_migration override</code> line from the <code class="literal">[sysctl]</code> section.
											</li></ul></div>
									 </td></tr></tbody></table></div></div></section><section class="section" id="ztp-sno-du-configuring-sriov_sno-configure-for-vdu"><div class="titlepage"><div><div><h4 class="title">16.6.7.7. SR-IOV</h4></div></div></div><p>
						Single root I/O virtualization (SR-IOV) is commonly used to enable fronthaul and midhaul networks. The following YAML example configures SR-IOV for a single-node OpenShift cluster.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The configuration of the <code class="literal">SriovNetwork</code> CR will vary depending on your specific network and infrastructure requirements.
						</p></div></div><div class="formalpara"><p class="title"><strong>Recommended SriovOperatorConfig configuration</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovOperatorConfig
metadata:
  name: default
  namespace: openshift-sriov-network-operator
spec:
  configDaemonNodeSelector:
    "node-role.kubernetes.io/master": ""
  enableInjector: true
  enableOperatorWebhook: true</pre>

						</p></div><div class="table" id="idm139735323143344"><p class="title"><strong>Table 16.12. SriovOperatorConfig CR options for single-node OpenShift clusters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735324597344" scope="col">SriovOperatorConfig CR field</th><th align="left" valign="top" id="idm139735324596240" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735324597344"> <p>
										<code class="literal">spec.enableInjector</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324596240"> <p>
										Disable <code class="literal">Injector</code> pods to reduce the number of management pods. Start with the <code class="literal">Injector</code> pods enabled, and only disable them after verifying the user manifests. If the injector is disabled, containers that use SR-IOV resources must explicitly assign them in the <code class="literal">requests</code> and <code class="literal">limits</code> section of the container spec.
									</p>
									 <p>
										For example:
									</p>
									 
<pre class="programlisting language-yaml">containers:
- name: my-sriov-workload-container
  resources:
    limits:
      openshift.io/&lt;resource_name&gt;:  "1"
    requests:
      openshift.io/&lt;resource_name&gt;:  "1"</pre>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735324597344"> <p>
										<code class="literal">spec.enableOperatorWebhook</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324596240"> <p>
										Disable <code class="literal">OperatorWebhook</code> pods to reduce the number of management pods. Start with the <code class="literal">OperatorWebhook</code> pods enabled, and only disable them after verifying the user manifests.
									</p>
									 </td></tr></tbody></table></div></div><div class="formalpara"><p class="title"><strong>Recommended SriovNetwork configuration</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: ""
  namespace: openshift-sriov-network-operator
spec:
  resourceName: "du_mh"
  networkNamespace:  openshift-sriov-network-operator
  vlan: "150"
  spoofChk: ""
  ipam: ""
  linkState: ""
  maxTxRate: ""
  minTxRate: ""
  vlanQoS: ""
  trust: ""
  capabilities: ""</pre>

						</p></div><div class="table" id="idm139735324577968"><p class="title"><strong>Table 16.13. SriovNetwork CR options for single-node OpenShift clusters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735324572352" scope="col">SriovNetwork CR field</th><th align="left" valign="top" id="idm139735324571264" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735324572352"> <p>
										<code class="literal">spec.vlan</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324571264"> <p>
										Configure <code class="literal">vlan</code> with the VLAN for the midhaul network.
									</p>
									 </td></tr></tbody></table></div></div><div class="formalpara"><p class="title"><strong>Recommended SriovNetworkNodePolicy configuration</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: $name
  namespace: openshift-sriov-network-operator
spec:
  # Attributes for Mellanox/Intel based NICs
  deviceType: netdevice/vfio-pci
  isRdma: true/false
  nicSelector:
    # The exact physical function name must match the hardware used
    pfNames: [ens7f0]
  nodeSelector:
    node-role.kubernetes.io/master: ""
  numVfs: 8
  priority: 10
  resourceName: du_mh</pre>

						</p></div><div class="table" id="idm139735324561456"><p class="title"><strong>Table 16.14. SriovNetworkPolicy CR options for single-node OpenShift clusters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735324555840" scope="col">SriovNetworkNodePolicy CR field</th><th align="left" valign="top" id="idm139735324554736" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735324555840"> <p>
										<code class="literal">spec.deviceType</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324554736"> <p>
										Configure <code class="literal">deviceType</code> as <code class="literal">vfio-pci</code> or <code class="literal">netdevice</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735324555840"> <p>
										<code class="literal">spec.nicSelector.pfNames</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324554736"> <p>
										Specifies the interface connected to the fronthaul network.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735324555840"> <p>
										<code class="literal">spec.numVfs</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735324554736"> <p>
										Specifies the number of VFs for the fronthaul network.
									</p>
									 </td></tr></tbody></table></div></div></section><section class="section" id="ztp-sno-du-removing-the-console-operator_sno-configure-for-vdu"><div class="titlepage"><div><div><h4 class="title">16.6.7.8. Console Operator</h4></div></div></div><p>
						Use the cluster capabilities feature to prevent the Console Operator from being installed. When the node is centrally managed it is not needed. Removing the Operator provides additional space and capacity for application workloads.
					</p><p>
						To disable the Console Operator during the installation of the managed cluster, set the following in the <code class="literal">spec.clusters.0.installConfigOverrides</code> field of the <code class="literal">SiteConfig</code> custom resource (CR):
					</p><pre class="programlisting language-yaml">installConfigOverrides:  "{\"capabilities\":{\"baselineCapabilitySet\": \"None\" }}"</pre></section><section class="section" id="ztp-sno-du-reducing-resource-usage-with-cluster-monitoring_sno-configure-for-vdu"><div class="titlepage"><div><div><h4 class="title">16.6.7.9. Grafana and Alertmanager</h4></div></div></div><p>
						Single-node OpenShift clusters that run DU workloads require reduced CPU resources consumed by the OpenShift Container Platform monitoring components. The following <code class="literal">ConfigMap</code> custom resource (CR) disables Grafana and Alertmanager.
					</p><div class="formalpara"><p class="title"><strong>Recommended cluster monitoring configuration (ReduceMonitoringFootprint.yaml)</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
  annotations:
    ran.openshift.io/ztp-deploy-wave: "1"
data:
  config.yaml: |
    grafana:
      enabled: false
    alertmanagerMain:
      enabled: false
    prometheusK8s:
       retention: 24h</pre>

						</p></div></section><section class="section" id="lvms-configuring-lvms-on-sno_sno-configure-for-vdu"><div class="titlepage"><div><div><h4 class="title">16.6.7.10. LVM Storage</h4></div></div></div><p>
						You can dynamically provision local storage on single-node OpenShift clusters with Logical volume manager storage (LVM Storage).
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The recommended storage solution for single-node OpenShift is the Local Storage Operator. Alternatively, you can use LVM Storage but it requires additional CPU resources to be allocated.
						</p></div></div><p>
						The following YAML example configures the storage of the node to be available to OpenShift Container Platform applications.
					</p><div class="formalpara"><p class="title"><strong>Recommended LVMCluster configuration (StorageLVMCluster.yaml)</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: lvm.topolvm.io/v1alpha1
kind: LVMCluster
metadata:
  name: odf-lvmcluster
  namespace: openshift-storage
spec:
  storage:
    deviceClasses:
    - name: vg1
      deviceSelector:
        paths:
        - /usr/disk/by-path/pci-0000:11:00.0-nvme-1
      thinPoolConfig:
        name: thin-pool-1
        overprovisionRatio: 10
        sizePercent: 90</pre>

						</p></div><div class="table" id="idm139735326268464"><p class="title"><strong>Table 16.15. LVMCluster CR options for single-node OpenShift clusters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735326262848" scope="col">LVMCluster CR field</th><th align="left" valign="top" id="idm139735326261760" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735326262848"> <p>
										<code class="literal">deviceSelector.paths</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735326261760"> <p>
										Configure the disks used for LVM storage. If no disks are specified, the LVM Storage uses all the unused disks in the specified thin pool.
									</p>
									 </td></tr></tbody></table></div></div></section><section class="section" id="ztp-sno-du-disabling-network-diagnostics_sno-configure-for-vdu"><div class="titlepage"><div><div><h4 class="title">16.6.7.11. Network diagnostics</h4></div></div></div><p>
						Single-node OpenShift clusters that run DU workloads require less inter-pod network connectivity checks to reduce the additional load created by these pods. The following custom resource (CR) disables these checks.
					</p><div class="formalpara"><p class="title"><strong>Recommended network diagnostics configuration (DisableSnoNetworkDiag.yaml)</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  disableNetworkDiagnostics: true</pre>

						</p></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-deploying-far-edge-sites">Deploying far edge sites using ZTP</a>
							</li></ul></div></section></section></section><section class="section" id="ztp-vdu-configuration-reference"><div class="titlepage"><div><div><h2 class="title">16.7. Validating single-node OpenShift cluster tuning for vDU application workloads</h2></div></div></div><p>
				Before you can deploy virtual distributed unit (vDU) applications, you need to tune and configure the cluster host firmware and various other cluster configuration settings. Use the following information to validate the cluster configuration to support vDU workloads.
			</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						For more information about single-node OpenShift clusters tuned for vDU application deployments, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#sno-configure-for-vdu">Reference configuration for deploying vDUs on single-node OpenShift</a>.
					</li></ul></div><section class="section" id="ztp-du-firmware-config-reference_vdu-config-ref"><div class="titlepage"><div><div><h3 class="title">16.7.1. Recommended firmware configuration for vDU cluster hosts</h3></div></div></div><p>
					Use the following table as the basis to configure the cluster host firmware for vDU applications running on OpenShift Container Platform 4.13.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The following table is a general recommendation for vDU cluster host firmware configuration. Exact firmware settings will depend on your requirements and specific hardware platform. Automatic setting of firmware is not handled by the zero touch provisioning pipeline.
					</p></div></div><div class="table" id="idm139735322946544"><p class="title"><strong>Table 16.16. Recommended cluster host firmware settings</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735322940816" scope="col">Firmware setting</th><th align="left" valign="top" id="idm139735322939728" scope="col">Configuration</th><th align="left" valign="top" id="idm139735322938640" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735322940816"> <p>
									HyperTransport (HT)
								</p>
								 </td><td align="left" valign="top" headers="idm139735322939728"> <p>
									Enabled
								</p>
								 </td><td align="left" valign="top" headers="idm139735322938640"> <p>
									HyperTransport (HT) bus is a bus technology developed by AMD. HT provides a high-speed link between the components in the host memory and other system peripherals.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735322940816"> <p>
									UEFI
								</p>
								 </td><td align="left" valign="top" headers="idm139735322939728"> <p>
									Enabled
								</p>
								 </td><td align="left" valign="top" headers="idm139735322938640"> <p>
									Enable booting from UEFI for the vDU host.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735322940816"> <p>
									CPU Power and Performance Policy
								</p>
								 </td><td align="left" valign="top" headers="idm139735322939728"> <p>
									Performance
								</p>
								 </td><td align="left" valign="top" headers="idm139735322938640"> <p>
									Set CPU Power and Performance Policy to optimize the system for performance over energy efficiency.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735322940816"> <p>
									Uncore Frequency Scaling
								</p>
								 </td><td align="left" valign="top" headers="idm139735322939728"> <p>
									Disabled
								</p>
								 </td><td align="left" valign="top" headers="idm139735322938640"> <p>
									Disable Uncore Frequency Scaling to prevent the voltage and frequency of non-core parts of the CPU from being set independently.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735322940816"> <p>
									Uncore Frequency
								</p>
								 </td><td align="left" valign="top" headers="idm139735322939728"> <p>
									Maximum
								</p>
								 </td><td align="left" valign="top" headers="idm139735322938640"> <p>
									Sets the non-core parts of the CPU such as cache and memory controller to their maximum possible frequency of operation.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735322940816"> <p>
									Performance P-limit
								</p>
								 </td><td align="left" valign="top" headers="idm139735322939728"> <p>
									Disabled
								</p>
								 </td><td align="left" valign="top" headers="idm139735322938640"> <p>
									Disable Performance P-limit to prevent the Uncore frequency coordination of processors.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735322940816"> <p>
									Enhanced Intel® SpeedStep Tech
								</p>
								 </td><td align="left" valign="top" headers="idm139735322939728"> <p>
									Enabled
								</p>
								 </td><td align="left" valign="top" headers="idm139735322938640"> <p>
									Enable Enhanced Intel SpeedStep to allow the system to dynamically adjust processor voltage and core frequency that decreases power consumption and heat production in the host.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735322940816"> <p>
									Intel® Turbo Boost Technology
								</p>
								 </td><td align="left" valign="top" headers="idm139735322939728"> <p>
									Enabled
								</p>
								 </td><td align="left" valign="top" headers="idm139735322938640"> <p>
									Enable Turbo Boost Technology for Intel-based CPUs to automatically allow processor cores to run faster than the rated operating frequency if they are operating below power, current, and temperature specification limits.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735322940816"> <p>
									Intel Configurable TDP
								</p>
								 </td><td align="left" valign="top" headers="idm139735322939728"> <p>
									Enabled
								</p>
								 </td><td align="left" valign="top" headers="idm139735322938640"> <p>
									Enables Thermal Design Power (TDP) for the CPU.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735322940816"> <p>
									Configurable TDP Level
								</p>
								 </td><td align="left" valign="top" headers="idm139735322939728"> <p>
									Level 2
								</p>
								 </td><td align="left" valign="top" headers="idm139735322938640"> <p>
									TDP level sets the CPU power consumption required for a particular performance rating. TDP level 2 sets the CPU to the most stable performance level at the cost of power consumption.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735322940816"> <p>
									Energy Efficient Turbo
								</p>
								 </td><td align="left" valign="top" headers="idm139735322939728"> <p>
									Disabled
								</p>
								 </td><td align="left" valign="top" headers="idm139735322938640"> <p>
									Disable Energy Efficient Turbo to prevent the processor from using an energy-efficiency based policy.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735322940816"> <p>
									Hardware P-States
								</p>
								 </td><td align="left" valign="top" headers="idm139735322939728"> <p>
									Enabled or Disabled
								</p>
								 </td><td align="left" valign="top" headers="idm139735322938640"> <p>
									Enable OS-controlled P-States to allow power saving configurations. Disable <code class="literal">P-states</code> (performance states) to optimize the operating system and CPU for performance over power consumption.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735322940816"> <p>
									Package C-State
								</p>
								 </td><td align="left" valign="top" headers="idm139735322939728"> <p>
									C0/C1 state
								</p>
								 </td><td align="left" valign="top" headers="idm139735322938640"> <p>
									Use C0 or C1 states to set the processor to a fully active state (C0) or to stop CPU internal clocks running in software (C1).
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735322940816"> <p>
									C1E
								</p>
								 </td><td align="left" valign="top" headers="idm139735322939728"> <p>
									Disabled
								</p>
								 </td><td align="left" valign="top" headers="idm139735322938640"> <p>
									CPU Enhanced Halt (C1E) is a power saving feature in Intel chips. Disabling C1E prevents the operating system from sending a halt command to the CPU when inactive.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735322940816"> <p>
									Processor C6
								</p>
								 </td><td align="left" valign="top" headers="idm139735322939728"> <p>
									Disabled
								</p>
								 </td><td align="left" valign="top" headers="idm139735322938640"> <p>
									C6 power-saving is a CPU feature that automatically disables idle CPU cores and cache. Disabling C6 improves system performance.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139735322940816"> <p>
									Sub-NUMA Clustering
								</p>
								 </td><td align="left" valign="top" headers="idm139735322939728"> <p>
									Disabled
								</p>
								 </td><td align="left" valign="top" headers="idm139735322938640"> <p>
									Sub-NUMA clustering divides the processor cores, cache, and memory into multiple NUMA domains. Disabling this option can increase performance for latency-sensitive workloads.
								</p>
								 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Enable global SR-IOV and VT-d settings in the firmware for the host. These settings are relevant to bare-metal environments.
					</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Enable both <code class="literal">C-states</code> and OS-controlled <code class="literal">P-States</code> to allow per pod power management.
					</p></div></div></section><section class="section" id="ztp-du-cluster-config-reference_vdu-config-ref"><div class="titlepage"><div><div><h3 class="title">16.7.2. Recommended cluster configurations to run vDU applications</h3></div></div></div><p>
					Clusters running virtualized distributed unit (vDU) applications require a highly tuned and optimized configuration. The following information describes the various elements that you require to support vDU workloads in OpenShift Container Platform 4.13 clusters.
				</p><section class="section" id="ztp-recommended-cluster-mc-crs_vdu-config-ref"><div class="titlepage"><div><div><h4 class="title">16.7.2.1. Recommended cluster MachineConfig CRs for single-node OpenShift clusters</h4></div></div></div><p>
						Check that the <code class="literal">MachineConfig</code> custom resources (CRs) that you extract from the <code class="literal">ztp-site-generate</code> container are applied in the cluster. The CRs can be found in the extracted <code class="literal">out/source-crs/extra-manifest/</code> folder.
					</p><p>
						The following <code class="literal">MachineConfig</code> CRs from the <code class="literal">ztp-site-generate</code> container configure the cluster host:
					</p><div class="table" id="idm139735325452880"><p class="title"><strong>Table 16.17. Recommended GitOps ZTP MachineConfig CRs</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139735325448016" scope="col">MachineConfig CR</th><th align="left" valign="top" id="idm139735325446928" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139735325448016"> <p>
										<code class="literal">01-container-mount-ns-and-kubelet-conf-master.yaml</code>
									</p>
									 <p>
										<code class="literal">01-container-mount-ns-and-kubelet-conf-worker.yaml</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735325446928"> <p>
										Configures the container mount namespace and kubelet configuration.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735325448016"> <p>
										<code class="literal">02-workload-partitioning.yaml</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735325446928"> <p>
										Configures workload partitioning for the cluster. Apply this <code class="literal">MachineConfig</code> CR when you install the cluster.
									</p>
									 <div class="admonition note"><div class="admonition_header">Note</div><div><p>
											If you use the <code class="literal">cpuPartitioningMode</code> field in the <code class="literal">SiteConfig</code> CR to configure workload partitioning, you do not need to use the <code class="literal">02-workload-partitioning.yaml</code> CR. Using the <code class="literal">cpuPartitioningMode</code> field is a Technology Preview feature in OpenShift Container Platform 4.13. For more information, see "Workload partitioning in single-node OpenShift with GitOps ZTP".
										</p></div></div>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735325448016"> <p>
										<code class="literal">03-sctp-machine-config-master.yaml</code>
									</p>
									 <p>
										<code class="literal">03-sctp-machine-config-worker.yaml</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735325446928"> <p>
										Loads the SCTP kernel module. These <code class="literal">MachineConfig</code> CRs are optional and can be omitted if you do not require this kernel module.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735325448016"> <p>
										<code class="literal">04-accelerated-container-startup-master.yaml</code>
									</p>
									 <p>
										<code class="literal">04-accelerated-container-startup-worker.yaml</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735325446928"> <p>
										Configures accelerated startup for the cluster.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735325448016"> <p>
										<code class="literal">05-kdump-config-master.yaml</code>
									</p>
									 <p>
										<code class="literal">05-kdump-config-worker.yaml</code>
									</p>
									 <p>
										<code class="literal">06-kdump-master.yaml</code>
									</p>
									 <p>
										<code class="literal">06-kdump-worker.yaml</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735325446928"> <p>
										Configures kdump crash reporting for the cluster.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139735325448016"> <p>
										<code class="literal">99-crio-disable-wipe-master.yaml</code>
									</p>
									 <p>
										<code class="literal">99-crio-disable-wipe-worker.yaml</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139735325446928"> <p>
										Disables the automatic CRI-O cache wipe following cluster reboot.
									</p>
									 </td></tr></tbody></table></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-preparing-the-ztp-git-repository_ztp-preparing-the-hub-cluster">Extracting source CRs from the ztp-site-generate container</a>
							</li></ul></div></section><section class="section" id="ztp-recommended-cluster-operators_vdu-config-ref"><div class="titlepage"><div><div><h4 class="title">16.7.2.2. Recommended cluster Operators</h4></div></div></div><p>
						The following Operators are required for clusters running virtualized distributed unit (vDU) applications and are a part of the baseline reference configuration:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Node Tuning Operator (NTO). NTO packages functionality that was previously delivered with the Performance Addon Operator, which is now a part of NTO.
							</li><li class="listitem">
								PTP Operator
							</li><li class="listitem">
								SR-IOV Network Operator
							</li><li class="listitem">
								Red Hat OpenShift Logging Operator
							</li><li class="listitem">
								Local Storage Operator
							</li></ul></div></section><section class="section" id="ztp-recommended-cluster-kernel-config_vdu-config-ref"><div class="titlepage"><div><div><h4 class="title">16.7.2.3. Recommended cluster kernel configuration</h4></div></div></div><p>
						Always use the latest supported real-time kernel version in your cluster. Ensure that you apply the following configurations in the cluster:
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Ensure that the following <code class="literal">additionalKernelArgs</code> are set in the cluster performance profile:
							</p><pre class="programlisting language-yaml">spec:
  additionalKernelArgs:
  - "rcupdate.rcu_normal_after_boot=0"
  - "efi=runtime"
  - "module_blacklist=irdma"</pre></li><li class="listitem"><p class="simpara">
								Ensure that the <code class="literal">performance-patch</code> profile in the <code class="literal">Tuned</code> CR configures the correct CPU isolation set that matches the <code class="literal">isolated</code> CPU set in the related <code class="literal">PerformanceProfile</code> CR, for example:
							</p><pre class="programlisting language-yaml">spec:
  profile:
    - name: performance-patch
      # The 'include' line must match the associated PerformanceProfile name, for example:
      # include=openshift-node-performance-${PerformanceProfile.metadata.name}
      # When using the standard (non-realtime) kernel, remove the kernel.timer_migration override from the [sysctl] section
      data: |
        [main]
        summary=Configuration changes profile inherited from performance created tuned
        include=openshift-node-performance-openshift-node-performance-profile
        [sysctl]
        kernel.timer_migration=1
        [scheduler]
        group.ice-ptp=0:f:10:*:ice-ptp.*
        group.ice-gnss=0:f:10:*:ice-gnss.*
        [service]
        service.stalld=start,enable
        service.chronyd=stop,disable</pre></li></ol></div></section><section class="section" id="ztp-checking-kernel-rt-in-cluster_vdu-config-ref"><div class="titlepage"><div><div><h4 class="title">16.7.2.4. Checking the realtime kernel version</h4></div></div></div><p>
						Always use the latest version of the realtime kernel in your OpenShift Container Platform clusters. If you are unsure about the kernel version that is in use in the cluster, you can compare the current realtime kernel version to the release version with the following procedure.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								You are logged in as a user with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have installed <code class="literal">podman</code>.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Run the following command to get the cluster version:
							</p><pre class="programlisting language-terminal">$ OCP_VERSION=$(oc get clusterversion version -o jsonpath='{.status.desired.version}{"\n"}')</pre></li><li class="listitem"><p class="simpara">
								Get the release image SHA number:
							</p><pre class="programlisting language-terminal">$ DTK_IMAGE=$(oc adm release info --image-for=driver-toolkit quay.io/openshift-release-dev/ocp-release:$OCP_VERSION-x86_64)</pre></li><li class="listitem"><p class="simpara">
								Run the release image container and extract the kernel version that is packaged with cluster’s current release:
							</p><pre class="programlisting language-terminal">$ podman run --rm $DTK_IMAGE rpm -qa | grep 'kernel-rt-core-' | sed 's#kernel-rt-core-##'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">4.18.0-305.49.1.rt7.121.el8_4.x86_64</pre>

								</p></div><p class="simpara">
								This is the default realtime kernel version that ships with the release.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									The realtime kernel is denoted by the string <code class="literal">.rt</code> in the kernel version.
								</p></div></div></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
							Check that the kernel version listed for the cluster’s current release matches actual realtime kernel that is running in the cluster. Run the following commands to check the running realtime kernel version:
						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Open a remote shell connection to the cluster node:
							</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node_name&gt;</pre></li><li class="listitem"><p class="simpara">
								Check the realtime kernel version:
							</p><pre class="programlisting language-terminal">sh-4.4# uname -r</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">4.18.0-305.49.1.rt7.121.el8_4.x86_64</pre>

								</p></div></li></ol></div></section></section><section class="section" id="ztp-checking-du-cluster-config_vdu-config-ref"><div class="titlepage"><div><div><h3 class="title">16.7.3. Checking that the recommended cluster configurations are applied</h3></div></div></div><p>
					You can check that clusters are running the correct configuration. The following procedure describes how to check the various configurations that you require to deploy a DU application in OpenShift Container Platform 4.13 clusters.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have deployed a cluster and tuned it for vDU workloads.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have logged in as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check that the default OperatorHub sources are disabled. Run the following command:
						</p><pre class="programlisting language-terminal">$ oc get operatorhub cluster -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">spec:
    disableAllDefaultSources: true</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check that all required <code class="literal">CatalogSource</code> resources are annotated for workload partitioning (<code class="literal">PreferredDuringScheduling</code>) by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get catalogsource -A -o jsonpath='{range .items[*]}{.metadata.name}{" -- "}{.metadata.annotations.target\.workload\.openshift\.io/management}{"\n"}{end}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">certified-operators -- {"effect": "PreferredDuringScheduling"}
community-operators -- {"effect": "PreferredDuringScheduling"}
ran-operators <span id="CO55-1"><!--Empty--></span><span class="callout">1</span>
redhat-marketplace -- {"effect": "PreferredDuringScheduling"}
redhat-operators -- {"effect": "PreferredDuringScheduling"}</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO55-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									<code class="literal">CatalogSource</code> resources that are not annotated are also returned. In this example, the <code class="literal">ran-operators</code> <code class="literal">CatalogSource</code> resource is not annotated and does not have the <code class="literal">PreferredDuringScheduling</code> annotation.
								</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								In a properly configured vDU cluster, only a single annotated catalog source is listed.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Check that all applicable OpenShift Container Platform Operator namespaces are annotated for workload partitioning. This includes all Operators installed with core OpenShift Container Platform and the set of additional Operators included in the reference DU tuning configuration. Run the following command:
						</p><pre class="programlisting language-terminal">$ oc get namespaces -A -o jsonpath='{range .items[*]}{.metadata.name}{" -- "}{.metadata.annotations.workload\.openshift\.io/allowed}{"\n"}{end}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">default --
openshift-apiserver -- management
openshift-apiserver-operator -- management
openshift-authentication -- management
openshift-authentication-operator -- management</pre>

							</p></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Additional Operators must not be annotated for workload partitioning. In the output from the previous command, additional Operators should be listed without any value on the right side of the <code class="literal">--</code> separator.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Check that the <code class="literal">ClusterLogging</code> configuration is correct. Run the following commands:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Validate that the appropriate input and output logs are configured:
								</p><pre class="programlisting language-terminal">$ oc get -n openshift-logging ClusterLogForwarder instance -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: logging.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  creationTimestamp: "2022-07-19T21:51:41Z"
  generation: 1
  name: instance
  namespace: openshift-logging
  resourceVersion: "1030342"
  uid: 8c1a842d-80c5-447a-9150-40350bdf40f0
spec:
  inputs:
  - infrastructure: {}
    name: infra-logs
  outputs:
  - name: kafka-open
    type: kafka
    url: tcp://10.46.55.190:9092/test
  pipelines:
  - inputRefs:
    - audit
    name: audit-logs
    outputRefs:
    - kafka-open
  - inputRefs:
    - infrastructure
    name: infrastructure-logs
    outputRefs:
    - kafka-open
...</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Check that the curation schedule is appropriate for your application:
								</p><pre class="programlisting language-terminal">$ oc get -n openshift-logging clusterloggings.logging.openshift.io instance -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: logging.openshift.io/v1
kind: ClusterLogging
metadata:
  creationTimestamp: "2022-07-07T18:22:56Z"
  generation: 1
  name: instance
  namespace: openshift-logging
  resourceVersion: "235796"
  uid: ef67b9b8-0e65-4a10-88ff-ec06922ea796
spec:
  collection:
    logs:
      fluentd: {}
      type: fluentd
  curation:
    curator:
      schedule: 30 3 * * *
    type: curator
  managementState: Managed
...</pre>

									</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Check that the web console is disabled (<code class="literal">managementState: Removed</code>) by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get consoles.operator.openshift.io cluster -o jsonpath="{ .spec.managementState }"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Removed</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check that <code class="literal">chronyd</code> is disabled on the cluster node by running the following commands:
						</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node_name&gt;</pre><p class="simpara">
							Check the status of <code class="literal">chronyd</code> on the node:
						</p><pre class="programlisting language-terminal">sh-4.4# chroot /host</pre><pre class="programlisting language-terminal">sh-4.4# systemctl status chronyd</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">● chronyd.service - NTP client/server
    Loaded: loaded (/usr/lib/systemd/system/chronyd.service; disabled; vendor preset: enabled)
    Active: inactive (dead)
      Docs: man:chronyd(8)
            man:chrony.conf(5)</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check that the PTP interface is successfully synchronized to the primary clock using a remote shell connection to the <code class="literal">linuxptp-daemon</code> container and the PTP Management Client (<code class="literal">pmc</code>) tool:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Set the <code class="literal">$PTP_POD_NAME</code> variable with the name of the <code class="literal">linuxptp-daemon</code> pod by running the following command:
								</p><pre class="programlisting language-terminal">$ PTP_POD_NAME=$(oc get pods -n openshift-ptp -l app=linuxptp-daemon -o name)</pre></li><li class="listitem"><p class="simpara">
									Run the following command to check the sync status of the PTP device:
								</p><pre class="programlisting language-terminal">$ oc -n openshift-ptp rsh -c linuxptp-daemon-container ${PTP_POD_NAME} pmc -u -f /var/run/ptp4l.0.config -b 0 'GET PORT_DATA_SET'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">sending: GET PORT_DATA_SET
  3cecef.fffe.7a7020-1 seq 0 RESPONSE MANAGEMENT PORT_DATA_SET
    portIdentity            3cecef.fffe.7a7020-1
    portState               SLAVE
    logMinDelayReqInterval  -4
    peerMeanPathDelay       0
    logAnnounceInterval     1
    announceReceiptTimeout  3
    logSyncInterval         0
    delayMechanism          1
    logMinPdelayReqInterval 0
    versionNumber           2
  3cecef.fffe.7a7020-2 seq 0 RESPONSE MANAGEMENT PORT_DATA_SET
    portIdentity            3cecef.fffe.7a7020-2
    portState               LISTENING
    logMinDelayReqInterval  0
    peerMeanPathDelay       0
    logAnnounceInterval     1
    announceReceiptTimeout  3
    logSyncInterval         0
    delayMechanism          1
    logMinPdelayReqInterval 0
    versionNumber           2</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Run the following <code class="literal">pmc</code> command to check the PTP clock status:
								</p><pre class="programlisting language-terminal">$ oc -n openshift-ptp rsh -c linuxptp-daemon-container ${PTP_POD_NAME} pmc -u -f /var/run/ptp4l.0.config -b 0 'GET TIME_STATUS_NP'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">sending: GET TIME_STATUS_NP
  3cecef.fffe.7a7020-0 seq 0 RESPONSE MANAGEMENT TIME_STATUS_NP
    master_offset              10 <span id="CO56-1"><!--Empty--></span><span class="callout">1</span>
    ingress_time               1657275432697400530
    cumulativeScaledRateOffset +0.000000000
    scaledLastGmPhaseChange    0
    gmTimeBaseIndicator        0
    lastGmPhaseChange          0x0000'0000000000000000.0000
    gmPresent                  true <span id="CO56-2"><!--Empty--></span><span class="callout">2</span>
    gmIdentity                 3c2c30.ffff.670e00</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO56-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											<code class="literal">master_offset</code> should be between -100 and 100 ns.
										</div></dd><dt><a href="#CO56-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Indicates that the PTP clock is synchronized to a master, and the local clock is not the grandmaster clock.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Check that the expected <code class="literal">master offset</code> value corresponding to the value in <code class="literal">/var/run/ptp4l.0.config</code> is found in the <code class="literal">linuxptp-daemon-container</code> log:
								</p><pre class="programlisting language-terminal">$ oc logs $PTP_POD_NAME -n openshift-ptp -c linuxptp-daemon-container</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">phc2sys[56020.341]: [ptp4l.1.config] CLOCK_REALTIME phc offset  -1731092 s2 freq -1546242 delay    497
ptp4l[56020.390]: [ptp4l.1.config] master offset         -2 s2 freq   -5863 path delay       541
ptp4l[56020.390]: [ptp4l.0.config] master offset         -8 s2 freq  -10699 path delay       533</pre>

									</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Check that the SR-IOV configuration is correct by running the following commands:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Check that the <code class="literal">disableDrain</code> value in the <code class="literal">SriovOperatorConfig</code> resource is set to <code class="literal">true</code>:
								</p><pre class="programlisting language-terminal">$ oc get sriovoperatorconfig -n openshift-sriov-network-operator default -o jsonpath="{.spec.disableDrain}{'\n'}"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">true</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Check that the <code class="literal">SriovNetworkNodeState</code> sync status is <code class="literal">Succeeded</code> by running the following command:
								</p><pre class="programlisting language-terminal">$ oc get SriovNetworkNodeStates -n openshift-sriov-network-operator -o jsonpath="{.items[*].status.syncStatus}{'\n'}"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">Succeeded</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Verify that the expected number and configuration of virtual functions (<code class="literal">Vfs</code>) under each interface configured for SR-IOV is present and correct in the <code class="literal">.status.interfaces</code> field. For example:
								</p><pre class="programlisting language-terminal">$ oc get SriovNetworkNodeStates -n openshift-sriov-network-operator -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: v1
items:
- apiVersion: sriovnetwork.openshift.io/v1
  kind: SriovNetworkNodeState
...
  status:
    interfaces:
    ...
    - Vfs:
      - deviceID: 154c
        driver: vfio-pci
        pciAddress: 0000:3b:0a.0
        vendor: "8086"
        vfID: 0
      - deviceID: 154c
        driver: vfio-pci
        pciAddress: 0000:3b:0a.1
        vendor: "8086"
        vfID: 1
      - deviceID: 154c
        driver: vfio-pci
        pciAddress: 0000:3b:0a.2
        vendor: "8086"
        vfID: 2
      - deviceID: 154c
        driver: vfio-pci
        pciAddress: 0000:3b:0a.3
        vendor: "8086"
        vfID: 3
      - deviceID: 154c
        driver: vfio-pci
        pciAddress: 0000:3b:0a.4
        vendor: "8086"
        vfID: 4
      - deviceID: 154c
        driver: vfio-pci
        pciAddress: 0000:3b:0a.5
        vendor: "8086"
        vfID: 5
      - deviceID: 154c
        driver: vfio-pci
        pciAddress: 0000:3b:0a.6
        vendor: "8086"
        vfID: 6
      - deviceID: 154c
        driver: vfio-pci
        pciAddress: 0000:3b:0a.7
        vendor: "8086"
        vfID: 7</pre>

									</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Check that the cluster performance profile is correct. The <code class="literal">cpu</code> and <code class="literal">hugepages</code> sections will vary depending on your hardware configuration. Run the following command:
						</p><pre class="programlisting language-terminal">$ oc get PerformanceProfile openshift-node-performance-profile -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  creationTimestamp: "2022-07-19T21:51:31Z"
  finalizers:
  - foreground-deletion
  generation: 1
  name: openshift-node-performance-profile
  resourceVersion: "33558"
  uid: 217958c0-9122-4c62-9d4d-fdc27c31118c
spec:
  additionalKernelArgs:
  - idle=poll
  - rcupdate.rcu_normal_after_boot=0
  - efi=runtime
  cpu:
    isolated: 2-51,54-103
    reserved: 0-1,52-53
  hugepages:
    defaultHugepagesSize: 1G
    pages:
    - count: 32
      size: 1G
  machineConfigPoolSelector:
    pools.operator.machineconfiguration.openshift.io/master: ""
  net:
    userLevelNetworking: true
  nodeSelector:
    node-role.kubernetes.io/master: ""
  numa:
    topologyPolicy: restricted
  realTimeKernel:
    enabled: true
status:
  conditions:
  - lastHeartbeatTime: "2022-07-19T21:51:31Z"
    lastTransitionTime: "2022-07-19T21:51:31Z"
    status: "True"
    type: Available
  - lastHeartbeatTime: "2022-07-19T21:51:31Z"
    lastTransitionTime: "2022-07-19T21:51:31Z"
    status: "True"
    type: Upgradeable
  - lastHeartbeatTime: "2022-07-19T21:51:31Z"
    lastTransitionTime: "2022-07-19T21:51:31Z"
    status: "False"
    type: Progressing
  - lastHeartbeatTime: "2022-07-19T21:51:31Z"
    lastTransitionTime: "2022-07-19T21:51:31Z"
    status: "False"
    type: Degraded
  runtimeClass: performance-openshift-node-performance-profile
  tuned: openshift-cluster-node-tuning-operator/openshift-node-performance-openshift-node-performance-profile</pre>

							</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								CPU settings are dependent on the number of cores available on the server and should align with workload partitioning settings. <code class="literal">hugepages</code> configuration is server and application dependent.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Check that the <code class="literal">PerformanceProfile</code> was successfully applied to the cluster by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get performanceprofile openshift-node-performance-profile -o jsonpath="{range .status.conditions[*]}{ @.type }{' -- '}{@.status}{'\n'}{end}"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Available -- True
Upgradeable -- True
Progressing -- False
Degraded -- False</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check the <code class="literal">Tuned</code> performance patch settings by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get tuneds.tuned.openshift.io -n openshift-cluster-node-tuning-operator performance-patch -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  creationTimestamp: "2022-07-18T10:33:52Z"
  generation: 1
  name: performance-patch
  namespace: openshift-cluster-node-tuning-operator
  resourceVersion: "34024"
  uid: f9799811-f744-4179-bf00-32d4436c08fd
spec:
  profile:
  - data: |
      [main]
      summary=Configuration changes profile inherited from performance created tuned
      include=openshift-node-performance-openshift-node-performance-profile
      [bootloader]
      cmdline_crash=nohz_full=2-23,26-47 <span id="CO57-1"><!--Empty--></span><span class="callout">1</span>
      [sysctl]
      kernel.timer_migration=1
      [scheduler]
      group.ice-ptp=0:f:10:*:ice-ptp.*
      [service]
      service.stalld=start,enable
      service.chronyd=stop,disable
    name: performance-patch
  recommend:
  - machineConfigLabels:
      machineconfiguration.openshift.io/role: master
    priority: 19
    profile: performance-patch</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO57-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The cpu list in <code class="literal">cmdline=nohz_full=</code> will vary based on your hardware configuration.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Check that cluster networking diagnostics are disabled by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get networks.operator.openshift.io cluster -o jsonpath='{.spec.disableNetworkDiagnostics}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">true</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check that the <code class="literal">Kubelet</code> housekeeping interval is tuned to slower rate. This is set in the <code class="literal">containerMountNS</code> machine config. Run the following command:
						</p><pre class="programlisting language-terminal">$ oc describe machineconfig container-mount-namespace-and-kubelet-conf-master | grep OPENSHIFT_MAX_HOUSEKEEPING_INTERVAL_DURATION</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Environment="OPENSHIFT_MAX_HOUSEKEEPING_INTERVAL_DURATION=60s"</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check that Grafana and <code class="literal">alertManagerMain</code> are disabled and that the Prometheus retention period is set to 24h by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get configmap cluster-monitoring-config -n openshift-monitoring -o jsonpath="{ .data.config\.yaml }"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">grafana:
  enabled: false
alertmanagerMain:
  enabled: false
prometheusK8s:
   retention: 24h</pre>

							</p></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Use the following commands to verify that Grafana and <code class="literal">alertManagerMain</code> routes are not found in the cluster:
								</p><pre class="programlisting language-terminal">$ oc get route -n openshift-monitoring alertmanager-main</pre><pre class="programlisting language-terminal">$ oc get route -n openshift-monitoring grafana</pre><p class="simpara">
									Both queries should return <code class="literal">Error from server (NotFound)</code> messages.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Check that there is a minimum of 4 CPUs allocated as <code class="literal">reserved</code> for each of the <code class="literal">PerformanceProfile</code>, <code class="literal">Tuned</code> performance-patch, workload partitioning, and kernel command line arguments by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get performanceprofile -o jsonpath="{ .items[0].spec.cpu.reserved }"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">0-3</pre>

							</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Depending on your workload requirements, you might require additional reserved CPUs to be allocated.
							</p></div></div></li></ol></div></section></section><section class="section" id="ztp-advanced-install-ztp"><div class="titlepage"><div><div><h2 class="title">16.8. Advanced managed cluster configuration with SiteConfig resources</h2></div></div></div><p>
				You can use <code class="literal">SiteConfig</code> custom resources (CRs) to deploy custom functionality and configurations in your managed clusters at installation time.
			</p><section class="section" id="ztp-customizing-the-install-extra-manifests_ztp-advanced-install-ztp"><div class="titlepage"><div><div><h3 class="title">16.8.1. Customizing extra installation manifests in the GitOps ZTP pipeline</h3></div></div></div><p>
					You can define a set of extra manifests for inclusion in the installation phase of the GitOps Zero Touch Provisioning (ZTP) pipeline. These manifests are linked to the <code class="literal">SiteConfig</code> custom resources (CRs) and are applied to the cluster during installation. Including <code class="literal">MachineConfig</code> CRs at install time makes the installation process more efficient.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Create a Git repository where you manage your custom site configuration data. The repository must be accessible from the hub cluster and be defined as a source repository for the Argo CD application.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Create a set of extra manifest CRs that the GitOps ZTP pipeline uses to customize the cluster installs.
						</li><li class="listitem"><p class="simpara">
							In your custom <code class="literal">/siteconfig</code> directory, create an <code class="literal">/extra-manifest</code> folder for your extra manifests. The following example illustrates a sample <code class="literal">/siteconfig</code> with <code class="literal">/extra-manifest</code> folder:
						</p><pre class="programlisting language-text">siteconfig
├── site1-sno-du.yaml
├── site2-standard-du.yaml
└── extra-manifest
    └── 01-example-machine-config.yaml</pre></li><li class="listitem">
							Add your custom extra manifest CRs to the <code class="literal">siteconfig/extra-manifest</code> directory.
						</li><li class="listitem"><p class="simpara">
							In your <code class="literal">SiteConfig</code> CR, enter the directory name in the <code class="literal">extraManifestPath</code> field, for example:
						</p><pre class="programlisting language-yaml">clusters:
- clusterName: "example-sno"
  networkType: "OVNKubernetes"
  extraManifestPath: extra-manifest</pre></li><li class="listitem">
							Save the <code class="literal">SiteConfig</code> CRs and <code class="literal">/extra-manifest</code> CRs and push them to the site configuration repo.
						</li></ol></div><p>
					The GitOps ZTP pipeline appends the CRs in the <code class="literal">/extra-manifest</code> directory to the default set of extra manifests during cluster provisioning.
				</p></section><section class="section" id="ztp-filtering-ai-crs-using-siteconfig_ztp-advanced-install-ztp"><div class="titlepage"><div><div><h3 class="title">16.8.2. Filtering custom resources using SiteConfig filters</h3></div></div></div><p>
					By using filters, you can easily customize <code class="literal">SiteConfig</code> custom resources (CRs) to include or exclude other CRs for use in the installation phase of the GitOps Zero Touch Provisioning (ZTP) pipeline.
				</p><p>
					You can specify an <code class="literal">inclusionDefault</code> value of <code class="literal">include</code> or <code class="literal">exclude</code> for the <code class="literal">SiteConfig</code> CR, along with a list of the specific <code class="literal">extraManifest</code> RAN CRs that you want to include or exclude. Setting <code class="literal">inclusionDefault</code> to <code class="literal">include</code> makes the GitOps ZTP pipeline apply all the files in <code class="literal">/source-crs/extra-manifest</code> during installation. Setting <code class="literal">inclusionDefault</code> to <code class="literal">exclude</code> does the opposite.
				</p><p>
					You can exclude individual CRs from the <code class="literal">/source-crs/extra-manifest</code> folder that are otherwise included by default. The following example configures a custom single-node OpenShift <code class="literal">SiteConfig</code> CR to exclude the <code class="literal">/source-crs/extra-manifest/03-sctp-machine-config-worker.yaml</code> CR at installation time.
				</p><p>
					Some additional optional filtering scenarios are also described.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You configured the hub cluster for generating the required installation and policy CRs.
						</li><li class="listitem">
							You created a Git repository where you manage your custom site configuration data. The repository must be accessible from the hub cluster and be defined as a source repository for the Argo CD application.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To prevent the GitOps ZTP pipeline from applying the <code class="literal">03-sctp-machine-config-worker.yaml</code> CR file, apply the following YAML in the <code class="literal">SiteConfig</code> CR:
						</p><pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1
kind: SiteConfig
metadata:
  name: "site1-sno-du"
  namespace: "site1-sno-du"
spec:
  baseDomain: "example.com"
  pullSecretRef:
    name: "assisted-deployment-pull-secret"
  clusterImageSetNameRef: "openshift-4.13"
  sshPublicKey: "&lt;ssh_public_key&gt;"
  clusters:
- clusterName: "site1-sno-du"
  extraManifests:
    filter:
      exclude:
        - 03-sctp-machine-config-worker.yaml</pre><p class="simpara">
							The GitOps ZTP pipeline skips the <code class="literal">03-sctp-machine-config-worker.yaml</code> CR during installation. All other CRs in <code class="literal">/source-crs/extra-manifest</code> are applied.
						</p></li><li class="listitem"><p class="simpara">
							Save the <code class="literal">SiteConfig</code> CR and push the changes to the site configuration repository.
						</p><p class="simpara">
							The GitOps ZTP pipeline monitors and adjusts what CRs it applies based on the <code class="literal">SiteConfig</code> filter instructions.
						</p></li><li class="listitem"><p class="simpara">
							Optional: To prevent the GitOps ZTP pipeline from applying all the <code class="literal">/source-crs/extra-manifest</code> CRs during cluster installation, apply the following YAML in the <code class="literal">SiteConfig</code> CR:
						</p><pre class="programlisting language-yaml">- clusterName: "site1-sno-du"
  extraManifests:
    filter:
      inclusionDefault: exclude</pre></li><li class="listitem"><p class="simpara">
							Optional: To exclude all the <code class="literal">/source-crs/extra-manifest</code> RAN CRs and instead include a custom CR file during installation, edit the custom <code class="literal">SiteConfig</code> CR to set the custom manifests folder and the <code class="literal">include</code> file, for example:
						</p><pre class="programlisting language-yaml">clusters:
- clusterName: "site1-sno-du"
  extraManifestPath: "&lt;custom_manifest_folder&gt;" <span id="CO58-1"><!--Empty--></span><span class="callout">1</span>
  extraManifests:
    filter:
      inclusionDefault: exclude  <span id="CO58-2"><!--Empty--></span><span class="callout">2</span>
      include:
        - custom-sctp-machine-config-worker.yaml</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO58-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal">&lt;custom_manifest_folder&gt;</code> with the name of the folder that contains the custom installation CRs, for example, <code class="literal">user-custom-manifest/</code>.
								</div></dd><dt><a href="#CO58-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Set <code class="literal">inclusionDefault</code> to <code class="literal">exclude</code> to prevent the GitOps ZTP pipeline from applying the files in <code class="literal">/source-crs/extra-manifest</code> during installation.
								</div></dd></dl></div><p class="simpara">
							The following example illustrates the custom folder structure:
						</p><pre class="programlisting language-text">siteconfig
  ├── site1-sno-du.yaml
  └── user-custom-manifest
        └── custom-sctp-machine-config-worker.yaml</pre></li></ol></div></section></section><section class="section" id="ztp-advanced-policy-config"><div class="titlepage"><div><div><h2 class="title">16.9. Advanced managed cluster configuration with PolicyGenTemplate resources</h2></div></div></div><p>
				You can use <code class="literal">PolicyGenTemplate</code> CRs to deploy custom functionality in your managed clusters.
			</p><section class="section" id="ztp-deploying-additional-changes-to-clusters_ztp-advanced-policy-config"><div class="titlepage"><div><div><h3 class="title">16.9.1. Deploying additional changes to clusters</h3></div></div></div><p>
					If you require cluster configuration changes outside of the base GitOps Zero Touch Provisioning (ZTP) pipeline configuration, there are three options:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Apply the additional configuration after the GitOps ZTP pipeline is complete</span></dt><dd>
								When the GitOps ZTP pipeline deployment is complete, the deployed cluster is ready for application workloads. At this point, you can install additional Operators and apply configurations specific to your requirements. Ensure that additional configurations do not negatively affect the performance of the platform or allocated CPU budget.
							</dd><dt><span class="term">Add content to the GitOps ZTP library</span></dt><dd>
								The base source custom resources (CRs) that you deploy with the GitOps ZTP pipeline can be augmented with custom content as required.
							</dd><dt><span class="term">Create extra manifests for the cluster installation</span></dt><dd>
								Extra manifests are applied during installation and make the installation process more efficient.
							</dd></dl></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Providing additional source CRs or modifying existing source CRs can significantly impact the performance or CPU profile of OpenShift Container Platform.
					</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-customizing-the-install-extra-manifests_ztp-advanced-install-ztp">Customizing extra installation manifests in the GitOps ZTP pipeline</a>
						</li></ul></div></section><section class="section" id="ztp-using-pgt-to-update-source-crs_ztp-advanced-policy-config"><div class="titlepage"><div><div><h3 class="title">16.9.2. Using PolicyGenTemplate CRs to override source CRs content</h3></div></div></div><p>
					<code class="literal">PolicyGenTemplate</code> custom resources (CRs) allow you to overlay additional configuration details on top of the base source CRs provided with the GitOps plugin in the <code class="literal">ztp-site-generate</code> container. You can think of <code class="literal">PolicyGenTemplate</code> CRs as a logical merge or patch to the base CR. Use <code class="literal">PolicyGenTemplate</code> CRs to update a single field of the base CR, or overlay the entire contents of the base CR. You can update values and insert fields that are not in the base CR.
				</p><p>
					The following example procedure describes how to update fields in the generated <code class="literal">PerformanceProfile</code> CR for the reference configuration based on the <code class="literal">PolicyGenTemplate</code> CR in the <code class="literal">group-du-sno-ranGen.yaml</code> file. Use the procedure as a basis for modifying other parts of the <code class="literal">PolicyGenTemplate</code> based on your requirements.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Create a Git repository where you manage your custom site configuration data. The repository must be accessible from the hub cluster and be defined as a source repository for Argo CD.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Review the baseline source CR for existing content. You can review the source CRs listed in the reference <code class="literal">PolicyGenTemplate</code> CRs by extracting them from the GitOps Zero Touch Provisioning (ZTP) container.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create an <code class="literal">/out</code> folder:
								</p><pre class="programlisting language-terminal">$ mkdir -p ./out</pre></li><li class="listitem"><p class="simpara">
									Extract the source CRs:
								</p><pre class="programlisting language-terminal">$ podman run --log-driver=none --rm registry.redhat.io/openshift4/ztp-site-generate-rhel8:v4.13.1 extract /home/ztp --tar | tar x -C ./out</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Review the baseline <code class="literal">PerformanceProfile</code> CR in <code class="literal">./out/source-crs/PerformanceProfile.yaml</code>:
						</p><pre class="programlisting language-yaml">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: $name
  annotations:
    ran.openshift.io/ztp-deploy-wave: "10"
spec:
  additionalKernelArgs:
  - "idle=poll"
  - "rcupdate.rcu_normal_after_boot=0"
  cpu:
    isolated: $isolated
    reserved: $reserved
  hugepages:
    defaultHugepagesSize: $defaultHugepagesSize
    pages:
      - size: $size
        count: $count
        node: $node
  machineConfigPoolSelector:
    pools.operator.machineconfiguration.openshift.io/$mcp: ""
  net:
    userLevelNetworking: true
  nodeSelector:
    node-role.kubernetes.io/$mcp: ''
  numa:
    topologyPolicy: "restricted"
  realTimeKernel:
    enabled: true</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Any fields in the source CR which contain <code class="literal">$…​</code> are removed from the generated CR if they are not provided in the <code class="literal">PolicyGenTemplate</code> CR.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Update the <code class="literal">PolicyGenTemplate</code> entry for <code class="literal">PerformanceProfile</code> in the <code class="literal">group-du-sno-ranGen.yaml</code> reference file. The following example <code class="literal">PolicyGenTemplate</code> CR stanza supplies appropriate CPU specifications, sets the <code class="literal">hugepages</code> configuration, and adds a new field that sets <code class="literal">globallyDisableIrqLoadBalancing</code> to false.
						</p><pre class="programlisting language-yaml">- fileName: PerformanceProfile.yaml
  policyName: "config-policy"
  metadata:
    name: openshift-node-performance-profile
  spec:
    cpu:
      # These must be tailored for the specific hardware platform
      isolated: "2-19,22-39"
      reserved: "0-1,20-21"
    hugepages:
      defaultHugepagesSize: 1G
      pages:
        - size: 1G
          count: 10
    globallyDisableIrqLoadBalancing: false</pre></li><li class="listitem">
							Commit the <code class="literal">PolicyGenTemplate</code> change in Git, and then push to the Git repository being monitored by the GitOps ZTP argo CD application.
						</li></ol></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						The GitOps ZTP application generates an RHACM policy that contains the generated <code class="literal">PerformanceProfile</code> CR. The contents of that CR are derived by merging the <code class="literal">metadata</code> and <code class="literal">spec</code> contents from the <code class="literal">PerformanceProfile</code> entry in the <code class="literal">PolicyGenTemplate</code> onto the source CR. The resulting CR has the following content:
					</p></div><pre class="programlisting language-yaml">---
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
    name: openshift-node-performance-profile
spec:
    additionalKernelArgs:
        - idle=poll
        - rcupdate.rcu_normal_after_boot=0
    cpu:
        isolated: 2-19,22-39
        reserved: 0-1,20-21
    globallyDisableIrqLoadBalancing: false
    hugepages:
        defaultHugepagesSize: 1G
        pages:
            - count: 10
              size: 1G
    machineConfigPoolSelector:
        pools.operator.machineconfiguration.openshift.io/master: ""
    net:
        userLevelNetworking: true
    nodeSelector:
        node-role.kubernetes.io/master: ""
    numa:
        topologyPolicy: restricted
    realTimeKernel:
        enabled: true</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						In the <code class="literal">/source-crs</code> folder that you extract from the <code class="literal">ztp-site-generate</code> container, the <code class="literal">$</code> syntax is not used for template substitution as implied by the syntax. Rather, if the <code class="literal">policyGen</code> tool sees the <code class="literal">$</code> prefix for a string and you do not specify a value for that field in the related <code class="literal">PolicyGenTemplate</code> CR, the field is omitted from the output CR entirely.
					</p><p>
						An exception to this is the <code class="literal">$mcp</code> variable in <code class="literal">/source-crs</code> YAML files that is substituted with the specified value for <code class="literal">mcp</code> from the <code class="literal">PolicyGenTemplate</code> CR. For example, in <code class="literal">example/policygentemplates/group-du-standard-ranGen.yaml</code>, the value for <code class="literal">mcp</code> is <code class="literal">worker</code>:
					</p><pre class="programlisting language-yaml">spec:
  bindingRules:
    group-du-standard: ""
  mcp: "worker"</pre><p>
						The <code class="literal">policyGen</code> tool replace instances of <code class="literal">$mcp</code> with <code class="literal">worker</code> in the output CRs.
					</p></div></div></section><section class="section" id="ztp-adding-new-content-to-gitops-ztp_ztp-advanced-policy-config"><div class="titlepage"><div><div><h3 class="title">16.9.3. Adding new content to the GitOps ZTP pipeline</h3></div></div></div><p>
					The source CRs in the GitOps Zero Touch Provisioning (ZTP) site generator container provide a set of critical features and node tuning settings for RAN Distributed Unit (DU) applications. These are applied to the clusters that you deploy with GitOps ZTP. To add or modify existing source CRs in the <code class="literal">ztp-site-generate</code> container, rebuild the <code class="literal">ztp-site-generate</code> container and make it available to the hub cluster, typically from the disconnected registry associated with the hub cluster. Any valid OpenShift Container Platform CR can be added.
				</p><p>
					Perform the following procedure to add new content to the GitOps ZTP pipeline.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a directory containing a Containerfile and the source CR YAML files that you want to include in the updated <code class="literal">ztp-site-generate</code> container, for example:
						</p><pre class="programlisting language-text">ztp-update/
├── example-cr1.yaml
├── example-cr2.yaml
└── ztp-update.in</pre></li><li class="listitem"><p class="simpara">
							Add the following content to the <code class="literal">ztp-update.in</code> Containerfile:
						</p><pre class="programlisting language-text">FROM registry.redhat.io/openshift4/ztp-site-generate-rhel8:v4.13

ADD example-cr2.yaml /kustomize/plugin/ran.openshift.io/v1/policygentemplate/source-crs/
ADD example-cr1.yaml /kustomize/plugin/ran.openshift.io/v1/policygentemplate/source-crs/</pre></li><li class="listitem"><p class="simpara">
							Open a terminal at the <code class="literal">ztp-update/</code> folder and rebuild the container:
						</p><pre class="programlisting language-terminal">$ podman build -t ztp-site-generate-rhel8-custom:v4.13-custom-1</pre></li><li class="listitem"><p class="simpara">
							Push the built container image to your disconnected registry, for example:
						</p><pre class="programlisting language-terminal">$ podman push localhost/ztp-site-generate-rhel8-custom:v4.13-custom-1 registry.example.com:5000/ztp-site-generate-rhel8-custom:v4.13-custom-1</pre></li><li class="listitem"><p class="simpara">
							Patch the Argo CD instance on the hub cluster to point to the newly built container image:
						</p><pre class="programlisting language-terminal">$ oc patch -n openshift-gitops argocd openshift-gitops --type=json -p '[{"op": "replace", "path":"/spec/repo/initContainers/0/image", "value": "registry.example.com:5000/ztp-site-generate-rhel8-custom:v4.13-custom-1"} ]'</pre><p class="simpara">
							When the Argo CD instance is patched, the <code class="literal">openshift-gitops-repo-server</code> pod automatically restarts.
						</p></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Verify that the new <code class="literal">openshift-gitops-repo-server</code> pod has completed initialization and that the previous repo pod is terminated:
						</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-gitops | grep openshift-gitops-repo-server</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">openshift-gitops-server-7df86f9774-db682          1/1     Running   	     1          28s</pre>

							</p></div><p class="simpara">
							You must wait until the new <code class="literal">openshift-gitops-repo-server</code> pod has completed initialization and the previous pod is terminated before the newly added container image content is available.
						</p></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							Alternatively, you can patch the ArgoCD instance as described in <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-configuring-hub-cluster-with-argocd_ztp-preparing-the-hub-cluster">Configuring the hub cluster with ArgoCD</a> by modifying <code class="literal">argocd-openshift-gitops-patch.json</code> with an updated <code class="literal">initContainer</code> image before applying the patch file.
						</li></ul></div></section><section class="section" id="ztp-configuring-pgt-compliance-eval-timeouts_ztp-advanced-policy-config"><div class="titlepage"><div><div><h3 class="title">16.9.4. Configuring policy compliance evaluation timeouts for PolicyGenTemplate CRs</h3></div></div></div><p>
					Use Red Hat Advanced Cluster Management (RHACM) installed on a hub cluster to monitor and report on whether your managed clusters are compliant with applied policies. RHACM uses policy templates to apply predefined policy controllers and policies. Policy controllers are Kubernetes custom resource definition (CRD) instances.
				</p><p>
					You can override the default policy evaluation intervals with <code class="literal">PolicyGenTemplate</code> custom resources (CRs). You configure duration settings that define how long a <code class="literal">ConfigurationPolicy</code> CR can be in a state of policy compliance or non-compliance before RHACM re-evaluates the applied cluster policies.
				</p><p>
					The GitOps Zero Touch Provisioning (ZTP) policy generator generates <code class="literal">ConfigurationPolicy</code> CR policies with pre-defined policy evaluation intervals. The default value for the <code class="literal">noncompliant</code> state is 10 seconds. The default value for the <code class="literal">compliant</code> state is 10 minutes. To disable the evaluation interval, set the value to <code class="literal">never</code>.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have logged in to the hub cluster as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							You have created a Git repository where you manage your custom site configuration data.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To configure the evaluation interval for all policies in a <code class="literal">PolicyGenTemplate</code> CR, add <code class="literal">evaluationInterval</code> to the <code class="literal">spec</code> field, and then set the appropriate <code class="literal">compliant</code> and <code class="literal">noncompliant</code> values. For example:
						</p><pre class="programlisting language-yaml">spec:
  evaluationInterval:
    compliant: 30m
    noncompliant: 20s</pre></li><li class="listitem"><p class="simpara">
							To configure the evaluation interval for the <code class="literal">spec.sourceFiles</code> object in a <code class="literal">PolicyGenTemplate</code> CR, add <code class="literal">evaluationInterval</code> to the <code class="literal">sourceFiles</code> field, for example:
						</p><pre class="programlisting language-yaml">spec:
  sourceFiles:
   - fileName: SriovSubscription.yaml
     policyName: "sriov-sub-policy"
     evaluationInterval:
       compliant: never
       noncompliant: 10s</pre></li><li class="listitem">
							Commit the <code class="literal">PolicyGenTemplate</code> CRs files in the Git repository and push your changes.
						</li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						Check that the managed spoke cluster policies are monitored at the expected intervals.
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges on the managed cluster.
						</li><li class="listitem"><p class="simpara">
							Get the pods that are running in the <code class="literal">open-cluster-management-agent-addon</code> namespace. Run the following command:
						</p><pre class="programlisting language-terminal">$ oc get pods -n open-cluster-management-agent-addon</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                         READY   STATUS    RESTARTS        AGE
config-policy-controller-858b894c68-v4xdb    1/1     Running   22 (5d8h ago)   10d</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check the applied policies are being evaluated at the expected interval in the logs for the <code class="literal">config-policy-controller</code> pod:
						</p><pre class="programlisting language-terminal">$ oc logs -n open-cluster-management-agent-addon config-policy-controller-858b894c68-v4xdb</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">2022-05-10T15:10:25.280Z       info   configuration-policy-controller controllers/configurationpolicy_controller.go:166      Skipping the policy evaluation due to the policy not reaching the evaluation interval  {"policy": "compute-1-config-policy-config"}
2022-05-10T15:10:25.280Z       info   configuration-policy-controller controllers/configurationpolicy_controller.go:166      Skipping the policy evaluation due to the policy not reaching the evaluation interval  {"policy": "compute-1-common-compute-1-catalog-policy-config"}</pre>

							</p></div></li></ol></div></section><section class="section" id="ztp-creating-a-validator-inform-policy_ztp-advanced-policy-config"><div class="titlepage"><div><div><h3 class="title">16.9.5. Signalling GitOps ZTP cluster deployment completion with validator inform policies</h3></div></div></div><p>
					Create a validator inform policy that signals when the GitOps Zero Touch Provisioning (ZTP) installation and configuration of the deployed cluster is complete. This policy can be used for deployments of single-node OpenShift clusters, three-node clusters, and standard clusters.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a standalone <code class="literal">PolicyGenTemplate</code> custom resource (CR) that contains the source file <code class="literal">validatorCRs/informDuValidator.yaml</code>. You only need one standalone <code class="literal">PolicyGenTemplate</code> CR for each cluster type. For example, this CR applies a validator inform policy for single-node OpenShift clusters:
						</p><div class="formalpara"><p class="title"><strong>Example single-node cluster validator inform policy CR (group-du-sno-validator-ranGen.yaml)</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "group-du-sno-validator" <span id="CO59-1"><!--Empty--></span><span class="callout">1</span>
  namespace: "ztp-group" <span id="CO59-2"><!--Empty--></span><span class="callout">2</span>
spec:
  bindingRules:
    group-du-sno: "" <span id="CO59-3"><!--Empty--></span><span class="callout">3</span>
  bindingExcludedRules:
    ztp-done: "" <span id="CO59-4"><!--Empty--></span><span class="callout">4</span>
  mcp: "master" <span id="CO59-5"><!--Empty--></span><span class="callout">5</span>
  sourceFiles:
    - fileName: validatorCRs/informDuValidator.yaml
      remediationAction: inform <span id="CO59-6"><!--Empty--></span><span class="callout">6</span>
      policyName: "du-policy" <span id="CO59-7"><!--Empty--></span><span class="callout">7</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO59-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of <code class="literal">PolicyGenTemplates</code> object. This name is also used as part of the names for the <code class="literal">placementBinding</code>, <code class="literal">placementRule</code>, and <code class="literal">policy</code> that are created in the requested <code class="literal">namespace</code>.
								</div></dd><dt><a href="#CO59-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									This value should match the <code class="literal">namespace</code> used in the group <code class="literal">PolicyGenTemplates</code>.
								</div></dd><dt><a href="#CO59-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The <code class="literal">group-du-*</code> label defined in <code class="literal">bindingRules</code> must exist in the <code class="literal">SiteConfig</code> files.
								</div></dd><dt><a href="#CO59-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The label defined in <code class="literal">bindingExcludedRules</code> must be`ztp-done:`. The <code class="literal">ztp-done</code> label is used in coordination with the Topology Aware Lifecycle Manager.
								</div></dd><dt><a href="#CO59-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									<code class="literal">mcp</code> defines the <code class="literal">MachineConfigPool</code> object that is used in the source file <code class="literal">validatorCRs/informDuValidator.yaml</code>. It should be <code class="literal">master</code> for single node and three-node cluster deployments and <code class="literal">worker</code> for standard cluster deployments.
								</div></dd><dt><a href="#CO59-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Optional. The default value is <code class="literal">inform</code>.
								</div></dd><dt><a href="#CO59-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									This value is used as part of the name for the generated RHACM policy. The generated validator policy for the single node example is <code class="literal">group-du-sno-validator-du-policy</code>.
								</div></dd></dl></div></li><li class="listitem">
							Commit the <code class="literal">PolicyGenTemplate</code> CR file in your Git repository and push the changes.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-updating-gitops">Upgrading GitOps ZTP</a>
						</li></ul></div></section><section class="section" id="ztp-using-pgt-to-configure-power-saving-states_ztp-advanced-policy-config"><div class="titlepage"><div><div><h3 class="title">16.9.6. Configuring power states using PolicyGenTemplates CRs</h3></div></div></div><p>
					For low latency and high-performance edge deployments, it is necessary to disable or limit C-states and P-states. With this configuration, the CPU runs at a constant frequency, which is typically the maximum turbo frequency. This ensures that the CPU is always running at its maximum speed, which results in high performance and low latency. This leads to the best latency for workloads. However, this also leads to the highest power consumption, which might not be necessary for all workloads.
				</p><p>
					Workloads can be classified as critical or non-critical, with critical workloads requiring disabled C-state and P-state settings for high performance and low latency, while non-critical workloads use C-state and P-state settings for power savings at the expense of some latency and performance. You can configure the following three power states using GitOps Zero Touch Provisioning (ZTP):
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							High-performance mode provides ultra low latency at the highest power consumption.
						</li><li class="listitem">
							Performance mode provides low latency at a relatively high power consumption.
						</li><li class="listitem">
							Power saving balances reduced power consumption with increased latency.
						</li></ul></div><p>
					The default configuration is for a low latency, performance mode.
				</p><p>
					<code class="literal">PolicyGenTemplate</code> custom resources (CRs) allow you to overlay additional configuration details onto the base source CRs provided with the GitOps plugin in the <code class="literal">ztp-site-generate</code> container.
				</p><p>
					Configure the power states by updating the <code class="literal">workloadHints</code> fields in the generated <code class="literal">PerformanceProfile</code> CR for the reference configuration, based on the <code class="literal">PolicyGenTemplate</code> CR in the <code class="literal">group-du-sno-ranGen.yaml</code>.
				</p><p>
					The following common prerequisites apply to configuring all three power states.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have created a Git repository where you manage your custom site configuration data. The repository must be accessible from the hub cluster and be defined as a source repository for Argo CD.
						</li><li class="listitem">
							You have followed the procedure described in "Preparing the GitOps ZTP site configuration repository".
						</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#cnf-understanding-workload-hints_cnf-master">Understanding workload hints</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#configuring-workload-hints_cnf-master">Configuring workload hints manually</a>
						</li></ul></div><section class="section" id="ztp-using-pgt-to-configure-performance-mode_ztp-advanced-policy-config"><div class="titlepage"><div><div><h4 class="title">16.9.6.1. Configuring performance mode using PolicyGenTemplate CRs</h4></div></div></div><p>
						Follow this example to set performance mode by updating the <code class="literal">workloadHints</code> fields in the generated <code class="literal">PerformanceProfile</code> CR for the reference configuration, based on the <code class="literal">PolicyGenTemplate</code> CR in the <code class="literal">group-du-sno-ranGen.yaml</code>.
					</p><p>
						Performance mode provides low latency at a relatively high power consumption.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have configured the BIOS with performance related settings by following the guidance in "Configuring host firmware for low latency and high performance".
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Update the <code class="literal">PolicyGenTemplate</code> entry for <code class="literal">PerformanceProfile</code> in the <code class="literal">group-du-sno-ranGen.yaml</code> reference file in <code class="literal">out/argocd/example/policygentemplates</code> as follows to set performance mode.
							</p><pre class="programlisting language-yaml">- fileName: PerformanceProfile.yaml
  policyName: "config-policy"
  metadata:
    [...]
  spec:
    [...]
    workloadHints:
         realTime: true
         highPowerConsumption: false
         perPodPowerManagement: false</pre></li><li class="listitem">
								Commit the <code class="literal">PolicyGenTemplate</code> change in Git, and then push to the Git repository being monitored by the GitOps ZTP Argo CD application.
							</li></ol></div></section><section class="section" id="ztp-using-pgt-to-configure-high-performance-mode_ztp-advanced-policy-config"><div class="titlepage"><div><div><h4 class="title">16.9.6.2. Configuring high-performance mode using PolicyGenTemplate CRs</h4></div></div></div><p>
						Follow this example to set high performance mode by updating the <code class="literal">workloadHints</code> fields in the generated <code class="literal">PerformanceProfile</code> CR for the reference configuration, based on the <code class="literal">PolicyGenTemplate</code> CR in the <code class="literal">group-du-sno-ranGen.yaml</code>.
					</p><p>
						High performance mode provides ultra low latency at the highest power consumption.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have configured the BIOS with performance related settings by following the guidance in "Configuring host firmware for low latency and high performance".
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Update the <code class="literal">PolicyGenTemplate</code> entry for <code class="literal">PerformanceProfile</code> in the <code class="literal">group-du-sno-ranGen.yaml</code> reference file in <code class="literal">out/argocd/example/policygentemplates</code> as follows to set high-performance mode.
							</p><pre class="programlisting language-yaml">- fileName: PerformanceProfile.yaml
  policyName: "config-policy"
  metadata:
    [...]
  spec:
    [...]
    workloadHints:
         realTime: true
         highPowerConsumption: true
         perPodPowerManagement: false</pre></li><li class="listitem">
								Commit the <code class="literal">PolicyGenTemplate</code> change in Git, and then push to the Git repository being monitored by the GitOps ZTP Argo CD application.
							</li></ol></div></section><section class="section" id="ztp-using-pgt-to-configure-power-saving-mode_ztp-advanced-policy-config"><div class="titlepage"><div><div><h4 class="title">16.9.6.3. Configuring power saving mode using PolicyGenTemplate CRs</h4></div></div></div><p>
						Follow this example to set power saving mode by updating the <code class="literal">workloadHints</code> fields in the generated <code class="literal">PerformanceProfile</code> CR for the reference configuration, based on the <code class="literal">PolicyGenTemplate</code> CR in the <code class="literal">group-du-sno-ranGen.yaml</code>.
					</p><p>
						The power saving mode balances reduced power consumption with increased latency.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You enabled C-states and OS-controlled P-states in the BIOS.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Update the <code class="literal">PolicyGenTemplate</code> entry for <code class="literal">PerformanceProfile</code> in the <code class="literal">group-du-sno-ranGen.yaml</code> reference file in <code class="literal">out/argocd/example/policygentemplates</code> as follows to configure power saving mode. It is recommended to configure the CPU governor for the power saving mode through the additional kernel arguments object.
							</p><pre class="programlisting language-yaml">- fileName: PerformanceProfile.yaml
  policyName: "config-policy"
  metadata:
    [...]
  spec:
    [...]
    workloadHints:
         realTime: true
         highPowerConsumption: false
         perPodPowerManagement: true
    [...]
    additionalKernelArgs:
       - [...]
       - "cpufreq.default_governor=schedutil" <span id="CO60-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO60-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The <code class="literal">schedutil</code> governor is recommended, however, other governors that can be used include <code class="literal">ondemand</code> and <code class="literal">powersave</code>.
									</div></dd></dl></div></li><li class="listitem">
								Commit the <code class="literal">PolicyGenTemplate</code> change in Git, and then push to the Git repository being monitored by the GitOps ZTP Argo CD application.
							</li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Select a worker node in your deployed cluster from the list of nodes identified by using the following command:
							</p><pre class="programlisting language-terminal">$ oc get nodes</pre></li><li class="listitem"><p class="simpara">
								Log in to the node by using the following command:
							</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node-name&gt;</pre><p class="simpara">
								Replace <code class="literal">&lt;node-name&gt;</code> with the name of the node you want to verify the power state on.
							</p></li><li class="listitem"><p class="simpara">
								Set <code class="literal">/host</code> as the root directory within the debug shell. The debug pod mounts the host’s root file system in <code class="literal">/host</code> within the pod. By changing the root directory to <code class="literal">/host</code>, you can run binaries contained in the host’s executable paths as shown in the following example:
							</p><pre class="programlisting language-terminal"># chroot /host</pre></li><li class="listitem"><p class="simpara">
								Run the following command to verify the applied power state:
							</p><pre class="programlisting language-terminal"># cat /proc/cmdline</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Expected output</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								For power saving mode the <code class="literal">intel_pstate=passive</code>.
							</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#node-tuning-operator-pod-power-saving-config_cnf-master">Enabling critical workloads for power saving configurations</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-du-configuring-host-firmware-requirements_sno-configure-for-vdu">Configuring host firmware for low latency and high performance</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-preparing-the-ztp-git-repository_ztp-preparing-the-hub-cluster">Preparing the GitOps ZTP site configuration repository</a>
							</li></ul></div></section><section class="section" id="ztp-using-pgt-to-maximize-power-savings-mode_ztp-advanced-policy-config"><div class="titlepage"><div><div><h4 class="title">16.9.6.4. Maximizing power savings</h4></div></div></div><p>
						Limiting the maximum CPU frequency is recommended to achieve maximum power savings. Enabling C-states on the non-critical workload CPUs without restricting the maximum CPU frequency negates much of the power savings by boosting the frequency of the critical CPUs.
					</p><p>
						Maximize power savings by updating the <code class="literal">sysfs</code> plugin fields, setting an appropriate value for <code class="literal">max_perf_pct</code> in the <code class="literal">TunedPerformancePatch</code> CR for the reference configuration. This example based on the <code class="literal">group-du-sno-ranGen.yaml</code> describes the procedure to follow to restrict the maximum CPU frequency.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have configured power savings mode as described in "Using PolicyGenTemplate CRs to configure power savings mode".
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Update the <code class="literal">PolicyGenTemplate</code> entry for <code class="literal">TunedPerformancePatch</code> in the <code class="literal">group-du-sno-ranGen.yaml</code> reference file in <code class="literal">out/argocd/example/policygentemplates</code>. To maximize power savings, add <code class="literal">max_perf_pct</code> as shown in the following example:
							</p><pre class="programlisting language-yaml">- fileName: TunedPerformancePatch.yaml
      policyName: "config-policy"
      spec:
        profile:
          - name: performance-patch
            data: |
              [...]
              [sysfs]
              /sys/devices/system/cpu/intel_pstate/max_perf_pct=&lt;x&gt; <span id="CO61-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO61-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The <code class="literal">max_perf_pct</code> controls the maximum frequency the <code class="literal">cpufreq</code> driver is allowed to set as a percentage of the maximum supported CPU frequency. This value applies to all CPUs. You can check the maximum supported frequency in <code class="literal">/sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq</code>. As a starting point, you can use a percentage that caps all CPUs at the <code class="literal">All Cores Turbo</code> frequency. The <code class="literal">All Cores Turbo</code> frequency is the frequency that all cores will run at when the cores are all fully occupied.
									</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									To maximize power savings, set a lower value. Setting a lower value for <code class="literal">max_perf_pct</code> limits the maximum CPU frequency, thereby reducing power consumption, but also potentially impacting performance. Experiment with different values and monitor the system’s performance and power consumption to find the optimal setting for your use-case.
								</p></div></div></li><li class="listitem">
								Commit the <code class="literal">PolicyGenTemplate</code> change in Git, and then push to the Git repository being monitored by the GitOps ZTP Argo CD application.
							</li></ol></div></section></section><section class="section" id="ztp-provisioning-lvm-storage_ztp-advanced-policy-config"><div class="titlepage"><div><div><h3 class="title">16.9.7. Configuring LVM Storage using PolicyGenTemplate CRs</h3></div></div></div><p>
					You can configure Logical volume manager storage (LVM Storage) for managed clusters that you deploy with GitOps Zero Touch Provisioning (ZTP).
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You use LVM Storage to persist event subscriptions when you use PTP events or bare-metal hardware events with HTTP transport.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							Create a Git repository where you manage your custom site configuration data.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To configure LVM Storage for new managed clusters, add the following YAML to <code class="literal">spec.sourceFiles</code> in the <code class="literal">common-ranGen.yaml</code> file:
						</p><pre class="programlisting language-yaml">- fileName: StorageLVMOSubscriptionNS.yaml
  policyName: subscription-policies
- fileName: StorageLVMOSubscriptionOperGroup.yaml
  policyName: subscription-policies
- fileName: StorageLVMOSubscription.yaml
  spec:
    name: lvms-operator
    channel: stable-4.13
  policyName: subscription-policies</pre></li><li class="listitem"><p class="simpara">
							Add the <code class="literal">LVMCluster</code> CR to <code class="literal">spec.sourceFiles</code> in your specific group or individual site configuration file. For example, in the <code class="literal">group-du-sno-ranGen.yaml</code> file, add the following:
						</p><pre class="programlisting language-yaml">- fileName: StorageLVMCluster.yaml
  policyName: "lvmo-config" <span id="CO62-1"><!--Empty--></span><span class="callout">1</span>
  spec:
    storage:
      deviceClasses:
      - name: vg1
        thinPoolConfig:
          name: thin-pool-1
          sizePercent: 90
          overprovisionRatio: 10</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO62-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									This example configuration creates a volume group (<code class="literal">vg1</code>) with all the available devices, except the disk where OpenShift Container Platform is installed. A thin-pool logical volume is also created.
								</div></dd></dl></div></li><li class="listitem">
							Merge any other required changes and files with your custom site repository.
						</li><li class="listitem">
							Commit the <code class="literal">PolicyGenTemplate</code> changes in Git, and then push the changes to your site configuration repository to deploy LVM Storage to new sites using GitOps ZTP.
						</li></ol></div></section><section class="section" id="ztp-advanced-policy-config-ptp_ztp-advanced-policy-config"><div class="titlepage"><div><div><h3 class="title">16.9.8. Configuring PTP events with PolicyGenTemplate CRs</h3></div></div></div><p>
					You can use the GitOps ZTP pipeline to configure PTP events that use HTTP or AMQP transport.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Use HTTP transport instead of AMQP for PTP and bare-metal events where possible. AMQ Interconnect is EOL from 30 June 2024. Extended life cycle support (ELS) for AMQ Interconnect ends 29 November 2029. For more information see, <a class="link" href="https://access.redhat.com/support/policy/updates/jboss_notes#p_Interconnect">Red Hat AMQ Interconnect support status</a>.
					</p></div></div><section class="section" id="ztp-configuring-ptp-fast-events_ztp-advanced-policy-config"><div class="titlepage"><div><div><h4 class="title">16.9.8.1. Configuring PTP events that use HTTP transport</h4></div></div></div><p>
						You can configure PTP events that use HTTP transport on managed clusters that you deploy with the GitOps Zero Touch Provisioning (ZTP) pipeline.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								You have logged in as a user with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have created a Git repository where you manage your custom site configuration data.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Apply the following <code class="literal">PolicyGenTemplate</code> changes to <code class="literal">group-du-3node-ranGen.yaml</code>, <code class="literal">group-du-sno-ranGen.yaml</code>, or <code class="literal">group-du-standard-ranGen.yaml</code> files according to your requirements:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										In <code class="literal">.sourceFiles</code>, add the <code class="literal">PtpOperatorConfig</code> CR file that configures the transport host:
									</p><pre class="programlisting language-yaml">- fileName: PtpOperatorConfigForEvent.yaml
  policyName: "config-policy"
  spec:
    daemonNodeSelector: {}
    ptpEventConfig:
      enableEventPublisher: true
      transportHost: http://ptp-event-publisher-service-NODE_NAME.openshift-ptp.svc.cluster.local:9043</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											In OpenShift Container Platform 4.13 or later, you do not need to set the <code class="literal">transportHost</code> field in the <code class="literal">PtpOperatorConfig</code> resource when you use HTTP transport with PTP events.
										</p></div></div></li><li class="listitem"><p class="simpara">
										Configure the <code class="literal">linuxptp</code> and <code class="literal">phc2sys</code> for the PTP clock type and interface. For example, add the following stanza into <code class="literal">.sourceFiles</code>:
									</p><pre class="programlisting language-yaml">- fileName: PtpConfigSlave.yaml <span id="CO63-1"><!--Empty--></span><span class="callout">1</span>
  policyName: "config-policy"
  metadata:
    name: "du-ptp-slave"
  spec:
    profile:
    - name: "slave"
      interface: "ens5f1" <span id="CO63-2"><!--Empty--></span><span class="callout">2</span>
      ptp4lOpts: "-2 -s --summary_interval -4" <span id="CO63-3"><!--Empty--></span><span class="callout">3</span>
      phc2sysOpts: "-a -r -m -n 24 -N 8 -R 16" <span id="CO63-4"><!--Empty--></span><span class="callout">4</span>
    ptpClockThreshold: <span id="CO63-5"><!--Empty--></span><span class="callout">5</span>
      holdOverTimeout: 30 #secs
      maxOffsetThreshold: 100  #nano secs
      minOffsetThreshold: -100 #nano secs</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO63-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Can be one of <code class="literal">PtpConfigMaster.yaml</code>, <code class="literal">PtpConfigSlave.yaml</code>, or <code class="literal">PtpConfigSlaveCvl.yaml</code> depending on your requirements. <code class="literal">PtpConfigSlaveCvl.yaml</code> configures <code class="literal">linuxptp</code> services for an Intel E810 Columbiaville NIC. For configurations based on <code class="literal">group-du-sno-ranGen.yaml</code> or <code class="literal">group-du-3node-ranGen.yaml</code>, use <code class="literal">PtpConfigSlave.yaml</code>.
											</div></dd><dt><a href="#CO63-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												Device specific interface name.
											</div></dd><dt><a href="#CO63-3"><span class="callout">3</span></a> </dt><dd><div class="para">
												You must append the <code class="literal">--summary_interval -4</code> value to <code class="literal">ptp4lOpts</code> in <code class="literal">.spec.sourceFiles.spec.profile</code> to enable PTP fast events.
											</div></dd><dt><a href="#CO63-4"><span class="callout">4</span></a> </dt><dd><div class="para">
												Required <code class="literal">phc2sysOpts</code> values. <code class="literal">-m</code> prints messages to <code class="literal">stdout</code>. The <code class="literal">linuxptp-daemon</code> <code class="literal">DaemonSet</code> parses the logs and generates Prometheus metrics.
											</div></dd><dt><a href="#CO63-5"><span class="callout">5</span></a> </dt><dd><div class="para">
												Optional. If the <code class="literal">ptpClockThreshold</code> stanza is not present, default values are used for the <code class="literal">ptpClockThreshold</code> fields. The stanza shows default <code class="literal">ptpClockThreshold</code> values. The <code class="literal">ptpClockThreshold</code> values configure how long after the PTP master clock is disconnected before PTP events are triggered. <code class="literal">holdOverTimeout</code> is the time value in seconds before the PTP clock event state changes to <code class="literal">FREERUN</code> when the PTP master clock is disconnected. The <code class="literal">maxOffsetThreshold</code> and <code class="literal">minOffsetThreshold</code> settings configure offset values in nanoseconds that compare against the values for <code class="literal">CLOCK_REALTIME</code> (<code class="literal">phc2sys</code>) or master offset (<code class="literal">ptp4l</code>). When the <code class="literal">ptp4l</code> or <code class="literal">phc2sys</code> offset value is outside this range, the PTP clock state is set to <code class="literal">FREERUN</code>. When the offset value is within this range, the PTP clock state is set to <code class="literal">LOCKED</code>.
											</div></dd></dl></div></li></ol></div></li><li class="listitem">
								Merge any other required changes and files with your custom site repository.
							</li><li class="listitem">
								Push the changes to your site configuration repository to deploy PTP fast events to new sites using GitOps ZTP.
							</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-using-pgt-to-update-source-crs_ztp-advanced-policy-config">Using PolicyGenTemplate CRs to override source CRs content</a>
							</li></ul></div></section><section class="section" id="ztp-configuring-ptp-fast-events-amqp_ztp-advanced-policy-config"><div class="titlepage"><div><div><h4 class="title">16.9.8.2. Configuring PTP events that use AMQP transport</h4></div></div></div><p>
						You can configure PTP events that use AMQP transport on managed clusters that you deploy with the GitOps Zero Touch Provisioning (ZTP) pipeline.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Use HTTP transport instead of AMQP for PTP and bare-metal events where possible. AMQ Interconnect is EOL from 30 June 2024. Extended life cycle support (ELS) for AMQ Interconnect ends 29 November 2029. For more information see, <a class="link" href="https://access.redhat.com/support/policy/updates/jboss_notes#p_Interconnect">Red Hat AMQ Interconnect support status</a>.
						</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								You have logged in as a user with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have created a Git repository where you manage your custom site configuration data.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Add the following YAML into <code class="literal">.spec.sourceFiles</code> in the <code class="literal">common-ranGen.yaml</code> file to configure the AMQP Operator:
							</p><pre class="programlisting language-yaml">#AMQ interconnect operator for fast events
- fileName: AmqSubscriptionNS.yaml
  policyName: "subscriptions-policy"
- fileName: AmqSubscriptionOperGroup.yaml
  policyName: "subscriptions-policy"
- fileName: AmqSubscription.yaml
  policyName: "subscriptions-policy"</pre></li><li class="listitem"><p class="simpara">
								Apply the following <code class="literal">PolicyGenTemplate</code> changes to <code class="literal">group-du-3node-ranGen.yaml</code>, <code class="literal">group-du-sno-ranGen.yaml</code>, or <code class="literal">group-du-standard-ranGen.yaml</code> files according to your requirements:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										In <code class="literal">.sourceFiles</code>, add the <code class="literal">PtpOperatorConfig</code> CR file that configures the AMQ transport host to the <code class="literal">config-policy</code>:
									</p><pre class="programlisting language-yaml">- fileName: PtpOperatorConfigForEvent.yaml
  policyName: "config-policy"
  spec:
    daemonNodeSelector: {}
    ptpEventConfig:
      enableEventPublisher: true
      transportHost: "amqp://amq-router.amq-router.svc.cluster.local"</pre></li><li class="listitem"><p class="simpara">
										Configure the <code class="literal">linuxptp</code> and <code class="literal">phc2sys</code> for the PTP clock type and interface. For example, add the following stanza into <code class="literal">.sourceFiles</code>:
									</p><pre class="programlisting language-yaml">- fileName: PtpConfigSlave.yaml <span id="CO64-1"><!--Empty--></span><span class="callout">1</span>
  policyName: "config-policy"
  metadata:
    name: "du-ptp-slave"
  spec:
    profile:
    - name: "slave"
      interface: "ens5f1" <span id="CO64-2"><!--Empty--></span><span class="callout">2</span>
      ptp4lOpts: "-2 -s --summary_interval -4" <span id="CO64-3"><!--Empty--></span><span class="callout">3</span>
      phc2sysOpts: "-a -r -m -n 24 -N 8 -R 16" <span id="CO64-4"><!--Empty--></span><span class="callout">4</span>
    ptpClockThreshold: <span id="CO64-5"><!--Empty--></span><span class="callout">5</span>
      holdOverTimeout: 30 #secs
      maxOffsetThreshold: 100  #nano secs
      minOffsetThreshold: -100 #nano secs</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO64-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Can be one <code class="literal">PtpConfigMaster.yaml</code>, <code class="literal">PtpConfigSlave.yaml</code>, or <code class="literal">PtpConfigSlaveCvl.yaml</code> depending on your requirements. <code class="literal">PtpConfigSlaveCvl.yaml</code> configures <code class="literal">linuxptp</code> services for an Intel E810 Columbiaville NIC. For configurations based on <code class="literal">group-du-sno-ranGen.yaml</code> or <code class="literal">group-du-3node-ranGen.yaml</code>, use <code class="literal">PtpConfigSlave.yaml</code>.
											</div></dd><dt><a href="#CO64-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												Device specific interface name.
											</div></dd><dt><a href="#CO64-3"><span class="callout">3</span></a> </dt><dd><div class="para">
												You must append the <code class="literal">--summary_interval -4</code> value to <code class="literal">ptp4lOpts</code> in <code class="literal">.spec.sourceFiles.spec.profile</code> to enable PTP fast events.
											</div></dd><dt><a href="#CO64-4"><span class="callout">4</span></a> </dt><dd><div class="para">
												Required <code class="literal">phc2sysOpts</code> values. <code class="literal">-m</code> prints messages to <code class="literal">stdout</code>. The <code class="literal">linuxptp-daemon</code> <code class="literal">DaemonSet</code> parses the logs and generates Prometheus metrics.
											</div></dd><dt><a href="#CO64-5"><span class="callout">5</span></a> </dt><dd><div class="para">
												Optional. If the <code class="literal">ptpClockThreshold</code> stanza is not present, default values are used for the <code class="literal">ptpClockThreshold</code> fields. The stanza shows default <code class="literal">ptpClockThreshold</code> values. The <code class="literal">ptpClockThreshold</code> values configure how long after the PTP master clock is disconnected before PTP events are triggered. <code class="literal">holdOverTimeout</code> is the time value in seconds before the PTP clock event state changes to <code class="literal">FREERUN</code> when the PTP master clock is disconnected. The <code class="literal">maxOffsetThreshold</code> and <code class="literal">minOffsetThreshold</code> settings configure offset values in nanoseconds that compare against the values for <code class="literal">CLOCK_REALTIME</code> (<code class="literal">phc2sys</code>) or master offset (<code class="literal">ptp4l</code>). When the <code class="literal">ptp4l</code> or <code class="literal">phc2sys</code> offset value is outside this range, the PTP clock state is set to <code class="literal">FREERUN</code>. When the offset value is within this range, the PTP clock state is set to <code class="literal">LOCKED</code>.
											</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
								Apply the following <code class="literal">PolicyGenTemplate</code> changes to your specific site YAML files, for example, <code class="literal">example-sno-site.yaml</code>:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										In <code class="literal">.sourceFiles</code>, add the <code class="literal">Interconnect</code> CR file that configures the AMQ router to the <code class="literal">config-policy</code>:
									</p><pre class="programlisting language-yaml">- fileName: AmqInstance.yaml
  policyName: "config-policy"</pre></li></ol></div></li><li class="listitem">
								Merge any other required changes and files with your custom site repository.
							</li><li class="listitem">
								Push the changes to your site configuration repository to deploy PTP fast events to new sites using GitOps ZTP.
							</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#cnf-installing-amq-interconnect-messaging-bus_using-ptp">Installing the AMQ messaging bus</a>
							</li><li class="listitem">
								For more information about container image registries, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/registry/#registry-overview">OpenShift image registry overview</a>.
							</li></ul></div></section></section><section class="section" id="ztp-advanced-policy-config-bare-metal_ztp-advanced-policy-config"><div class="titlepage"><div><div><h3 class="title">16.9.9. Configuring bare-metal events with PolicyGenTemplate CRs</h3></div></div></div><p>
					You can use the GitOps ZTP pipeline to configure bare-metal events that use HTTP or AMQP transport.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Use HTTP transport instead of AMQP for PTP and bare-metal events where possible. AMQ Interconnect is EOL from 30 June 2024. Extended life cycle support (ELS) for AMQ Interconnect ends 29 November 2029. For more information see, <a class="link" href="https://access.redhat.com/support/policy/updates/jboss_notes#p_Interconnect">Red Hat AMQ Interconnect support status</a>.
					</p></div></div><section class="section" id="ztp-creating-hwevents_ztp-advanced-policy-config"><div class="titlepage"><div><div><h4 class="title">16.9.9.1. Configuring bare-metal events that use HTTP transport</h4></div></div></div><p>
						You can configure bare-metal events that use HTTP transport on managed clusters that you deploy with the GitOps Zero Touch Provisioning (ZTP) pipeline.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								You have logged in as a user with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have created a Git repository where you manage your custom site configuration data.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Configure the Bare Metal Event Relay Operator by adding the following YAML to <code class="literal">spec.sourceFiles</code> in the <code class="literal">common-ranGen.yaml</code> file:
							</p><pre class="programlisting language-yaml"># Bare Metal Event Relay operator
- fileName: BareMetalEventRelaySubscriptionNS.yaml
  policyName: "subscriptions-policy"
- fileName: BareMetalEventRelaySubscriptionOperGroup.yaml
  policyName: "subscriptions-policy"
- fileName: BareMetalEventRelaySubscription.yaml
  policyName: "subscriptions-policy"</pre></li><li class="listitem"><p class="simpara">
								Add the <code class="literal">HardwareEvent</code> CR to <code class="literal">spec.sourceFiles</code> in your specific group configuration file, for example, in the <code class="literal">group-du-sno-ranGen.yaml</code> file:
							</p><pre class="programlisting language-yaml">- fileName: HardwareEvent.yaml <span id="CO65-1"><!--Empty--></span><span class="callout">1</span>
  policyName: "config-policy"
  spec:
    nodeSelector: {}
    transportHost: "http://hw-event-publisher-service.openshift-bare-metal-events.svc.cluster.local:9043"
    logLevel: "info"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO65-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Each baseboard management controller (BMC) requires a single <code class="literal">HardwareEvent</code> CR only.
									</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									In OpenShift Container Platform 4.13 or later, you do not need to set the <code class="literal">transportHost</code> field in the <code class="literal">HardwareEvent</code> custom resource (CR) when you use HTTP transport with bare-metal events.
								</p></div></div></li><li class="listitem">
								Merge any other required changes and files with your custom site repository.
							</li><li class="listitem">
								Push the changes to your site configuration repository to deploy bare-metal events to new sites with GitOps ZTP.
							</li><li class="listitem"><p class="simpara">
								Create the Redfish Secret by running the following command:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-bare-metal-events create secret generic redfish-basic-auth \
--from-literal=username=&lt;bmc_username&gt; --from-literal=password=&lt;bmc_password&gt; \
--from-literal=hostaddr="&lt;bmc_host_ip_addr&gt;"</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/monitoring/#nw-rfhe-installing-operator-cli_using-rfhe">Installing the Bare Metal Event Relay using the CLI</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/monitoring/#nw-rfhe-creating-hardware-event_using-rfhe">Creating the bare-metal event and Secret CRs</a>
							</li></ul></div></section><section class="section" id="ztp-creating-hwevents-amqp_ztp-advanced-policy-config"><div class="titlepage"><div><div><h4 class="title">16.9.9.2. Configuring bare-metal events that use AMQP transport</h4></div></div></div><p>
						You can configure bare-metal events that use AMQP transport on managed clusters that you deploy with the GitOps Zero Touch Provisioning (ZTP) pipeline.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								You have logged in as a user with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have created a Git repository where you manage your custom site configuration data.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								To configure the AMQ Interconnect Operator and the Bare Metal Event Relay Operator, add the following YAML to <code class="literal">spec.sourceFiles</code> in the <code class="literal">common-ranGen.yaml</code> file:
							</p><pre class="programlisting language-yaml"># AMQ interconnect operator for fast events
- fileName: AmqSubscriptionNS.yaml
  policyName: "subscriptions-policy"
- fileName: AmqSubscriptionOperGroup.yaml
  policyName: "subscriptions-policy"
- fileName: AmqSubscription.yaml
  policyName: "subscriptions-policy"
# Bare Metal Event Rely operator
- fileName: BareMetalEventRelaySubscriptionNS.yaml
  policyName: "subscriptions-policy"
- fileName: BareMetalEventRelaySubscriptionOperGroup.yaml
  policyName: "subscriptions-policy"
- fileName: BareMetalEventRelaySubscription.yaml
  policyName: "subscriptions-policy"</pre></li><li class="listitem"><p class="simpara">
								Add the <code class="literal">Interconnect</code> CR to <code class="literal">.spec.sourceFiles</code> in the site configuration file, for example, the <code class="literal">example-sno-site.yaml</code> file:
							</p><pre class="programlisting language-yaml">- fileName: AmqInstance.yaml
  policyName: "config-policy"</pre></li><li class="listitem"><p class="simpara">
								Add the <code class="literal">HardwareEvent</code> CR to <code class="literal">spec.sourceFiles</code> in your specific group configuration file, for example, in the <code class="literal">group-du-sno-ranGen.yaml</code> file:
							</p><pre class="programlisting language-yaml">- fileName: HardwareEvent.yaml
  policyName: "config-policy"
  spec:
    nodeSelector: {}
    transportHost: "amqp://&lt;amq_interconnect_name&gt;.&lt;amq_interconnect_namespace&gt;.svc.cluster.local" <span id="CO66-1"><!--Empty--></span><span class="callout">1</span>
    logLevel: "info"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO66-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The <code class="literal">transportHost</code> URL is composed of the existing AMQ Interconnect CR <code class="literal">name</code> and <code class="literal">namespace</code>. For example, in <code class="literal">transportHost: "amqp://amq-router.amq-router.svc.cluster.local"</code>, the AMQ Interconnect <code class="literal">name</code> and <code class="literal">namespace</code> are both set to <code class="literal">amq-router</code>.
									</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Each baseboard management controller (BMC) requires a single <code class="literal">HardwareEvent</code> resource only.
								</p></div></div></li><li class="listitem">
								Commit the <code class="literal">PolicyGenTemplate</code> change in Git, and then push the changes to your site configuration repository to deploy bare-metal events monitoring to new sites using GitOps ZTP.
							</li><li class="listitem"><p class="simpara">
								Create the Redfish Secret by running the following command:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-bare-metal-events create secret generic redfish-basic-auth \
--from-literal=username=&lt;bmc_username&gt; --from-literal=password=&lt;bmc_password&gt; \
--from-literal=hostaddr="&lt;bmc_host_ip_addr&gt;"</pre></li></ol></div></section></section><section class="section" id="ztp-add-local-reg-for-sno-duprofile_ztp-advanced-policy-config"><div class="titlepage"><div><div><h3 class="title">16.9.10. Configuring the Image Registry Operator for local caching of images</h3></div></div></div><p>
					OpenShift Container Platform manages image caching using a local registry. In edge computing use cases, clusters are often subject to bandwidth restrictions when communicating with centralized image registries, which might result in long image download times.
				</p><p>
					Long download times are unavoidable during initial deployment. Over time, there is a risk that CRI-O will erase the <code class="literal">/var/lib/containers/storage</code> directory in the case of an unexpected shutdown. To address long image download times, you can create a local image registry on remote managed clusters using GitOps Zero Touch Provisioning (ZTP). This is useful in Edge computing scenarios where clusters are deployed at the far edge of the network.
				</p><p>
					Before you can set up the local image registry with GitOps ZTP, you need to configure disk partitioning in the <code class="literal">SiteConfig</code> CR that you use to install the remote managed cluster. After installation, you configure the local image registry using a <code class="literal">PolicyGenTemplate</code> CR. Then, the GitOps ZTP pipeline creates Persistent Volume (PV) and Persistent Volume Claim (PVC) CRs and patches the <code class="literal">imageregistry</code> configuration.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The local image registry can only be used for user application images and cannot be used for the OpenShift Container Platform or Operator Lifecycle Manager operator images.
					</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/registry/#registry-overview">OpenShift Container Platform registry overview</a>.
						</li></ul></div><section class="section" id="ztp-configuring-disk-partitioning_ztp-advanced-policy-config"><div class="titlepage"><div><div><h4 class="title">16.9.10.1. Configuring disk partitioning with SiteConfig</h4></div></div></div><p>
						Configure disk partitioning for a managed cluster using a <code class="literal">SiteConfig</code> CR and GitOps Zero Touch Provisioning (ZTP). The disk partition details in the <code class="literal">SiteConfig</code> CR must match the underlying disk.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Use persistent naming for devices to avoid device names such as <code class="literal">/dev/sda</code> and <code class="literal">/dev/sdb</code> being switched at every reboot. You can use <code class="literal">rootDeviceHints</code> to choose the bootable device and then use same device for further partitioning.
						</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								You have logged in to the hub cluster as a user with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have created a Git repository where you manage your custom site configuration data for use with GitOps Zero Touch Provisioning (ZTP).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Add the following YAML that describes the host disk partitioning to the <code class="literal">SiteConfig</code> CR that you use to install the managed cluster:
							</p><pre class="programlisting language-yaml">nodes:
    rootDeviceHints:
      wwn: "0x62cea7f05c98c2002708a0a22ff480ea"
    diskPartition:
      - device: /dev/disk/by-id/wwn-0x62cea7f05c98c2002708a0a22ff480ea <span id="CO67-1"><!--Empty--></span><span class="callout">1</span>
        partitions:
          - mount_point: /var/imageregistry
            size: 102500 <span id="CO67-2"><!--Empty--></span><span class="callout">2</span>
            start: 344844 <span id="CO67-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO67-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										This setting depends on the hardware. The setting can be a serial number or device name. The value must match the value set for <code class="literal">rootDeviceHints</code>.
									</div></dd><dt><a href="#CO67-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The minimum value for <code class="literal">size</code> is 102500 MiB.
									</div></dd><dt><a href="#CO67-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The minimum value for <code class="literal">start</code> is 25000 MiB. The total value of <code class="literal">size</code> and <code class="literal">start</code> must not exceed the disk size, or the installation will fail.
									</div></dd></dl></div></li><li class="listitem">
								Save the <code class="literal">SiteConfig</code> CR and push it to the site configuration repo.
							</li></ol></div><p>
						The GitOps ZTP pipeline provisions the cluster using the <code class="literal">SiteConfig</code> CR and configures the disk partition.
					</p></section><section class="section" id="ztp-configuring-pgt-image-registry_ztp-advanced-policy-config"><div class="titlepage"><div><div><h4 class="title">16.9.10.2. Configuring the image registry using PolicyGenTemplate CRs</h4></div></div></div><p>
						Use <code class="literal">PolicyGenTemplate</code> (PGT) CRs to apply the CRs required to configure the image registry and patch the <code class="literal">imageregistry</code> configuration.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have configured a disk partition in the managed cluster.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								You have logged in to the hub cluster as a user with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have created a Git repository where you manage your custom site configuration data for use with GitOps Zero Touch Provisioning (ZTP).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Configure the storage class, persistent volume claim, persistent volume, and image registry configuration in the appropriate <code class="literal">PolicyGenTemplate</code> CR. For example, to configure an individual site, add the following YAML to the file <code class="literal">example-sno-site.yaml</code>:
							</p><pre class="programlisting language-yaml">sourceFiles:
  # storage class
  - fileName: StorageClass.yaml
    policyName: "sc-for-image-registry"
    metadata:
      name: image-registry-sc
      annotations:
        ran.openshift.io/ztp-deploy-wave: "100" <span id="CO68-1"><!--Empty--></span><span class="callout">1</span>
  # persistent volume claim
  - fileName: StoragePVC.yaml
    policyName: "pvc-for-image-registry"
    metadata:
      name: image-registry-pvc
      namespace: openshift-image-registry
      annotations:
        ran.openshift.io/ztp-deploy-wave: "100"
    spec:
      accessModes:
        - ReadWriteMany
      resources:
        requests:
          storage: 100Gi
      storageClassName: image-registry-sc
      volumeMode: Filesystem
  # persistent volume
  - fileName: ImageRegistryPV.yaml <span id="CO68-2"><!--Empty--></span><span class="callout">2</span>
    policyName: "pv-for-image-registry"
    metadata:
      annotations:
        ran.openshift.io/ztp-deploy-wave: "100"
  - fileName: ImageRegistryConfig.yaml
    policyName: "config-for-image-registry"
    complianceType: musthave
    metadata:
      annotations:
        ran.openshift.io/ztp-deploy-wave: "100"
    spec:
      storage:
        pvc:
          claim: "image-registry-pvc"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO68-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Set the appropriate value for <code class="literal">ztp-deploy-wave</code> depending on whether you are configuring image registries at the site, common, or group level. <code class="literal">ztp-deploy-wave: "100"</code> is suitable for development or testing because it allows you to group the referenced source files together.
									</div></dd><dt><a href="#CO68-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										In <code class="literal">ImageRegistryPV.yaml</code>, ensure that the <code class="literal">spec.local.path</code> field is set to <code class="literal">/var/imageregistry</code> to match the value set for the <code class="literal">mount_point</code> field in the <code class="literal">SiteConfig</code> CR.
									</div></dd></dl></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									Do not set <code class="literal">complianceType: mustonlyhave</code> for the <code class="literal">- fileName: ImageRegistryConfig.yaml</code> configuration. This can cause the registry pod deployment to fail.
								</p></div></div></li><li class="listitem">
								Commit the <code class="literal">PolicyGenTemplate</code> change in Git, and then push to the Git repository being monitored by the GitOps ZTP ArgoCD application.
							</li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
							Use the following steps to troubleshoot errors with the local image registry on the managed clusters:
						</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Verify successful login to the registry while logged in to the managed cluster. Run the following commands:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Export the managed cluster name:
									</p><pre class="programlisting language-terminal">$ cluster=&lt;managed_cluster_name&gt;</pre></li><li class="listitem"><p class="simpara">
										Get the managed cluster <code class="literal">kubeconfig</code> details:
									</p><pre class="programlisting language-terminal">$ oc get secret -n $cluster $cluster-admin-password -o jsonpath='{.data.password}' | base64 -d &gt; kubeadmin-password-$cluster</pre></li><li class="listitem"><p class="simpara">
										Download and export the cluster <code class="literal">kubeconfig</code>:
									</p><pre class="programlisting language-terminal">$ oc get secret -n $cluster $cluster-admin-kubeconfig -o jsonpath='{.data.kubeconfig}' | base64 -d &gt; kubeconfig-$cluster &amp;&amp; export KUBECONFIG=./kubeconfig-$cluster</pre></li><li class="listitem">
										Verify access to the image registry from the managed cluster. See "Accessing the registry".
									</li></ol></div></li><li class="listitem"><p class="simpara">
								Check that the <code class="literal">Config</code> CRD in the <code class="literal">imageregistry.operator.openshift.io</code> group instance is not reporting errors. Run the following command while logged in to the managed cluster:
							</p><pre class="programlisting language-terminal">$ oc get image.config.openshift.io cluster -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Image
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2021-10-08T19:02:39Z"
  generation: 5
  name: cluster
  resourceVersion: "688678648"
  uid: 0406521b-39c0-4cda-ba75-873697da75a4
spec:
  additionalTrustedCA:
    name: acm-ice</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Check that the <code class="literal">PersistentVolumeClaim</code> on the managed cluster is populated with data. Run the following command while logged in to the managed cluster:
							</p><pre class="programlisting language-terminal">$ oc get pv image-registry-sc</pre></li><li class="listitem"><p class="simpara">
								Check that the <code class="literal">registry*</code> pod is running and is located under the <code class="literal">openshift-image-registry</code> namespace.
							</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-image-registry | grep registry*</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">cluster-image-registry-operator-68f5c9c589-42cfg   1/1     Running     0          8d
image-registry-5f8987879-6nx6h                     1/1     Running     0          8d</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Check that the disk partition on the managed cluster is correct:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Open a debug shell to the managed cluster:
									</p><pre class="programlisting language-terminal">$ oc debug node/sno-1.example.com</pre></li><li class="listitem"><p class="simpara">
										Run <code class="literal">lsblk</code> to check the host disk partitions:
									</p><pre class="programlisting language-terminal">sh-4.4# lsblk
NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sda      8:0    0 446.6G  0 disk
  |-sda1   8:1    0     1M  0 part
  |-sda2   8:2    0   127M  0 part
  |-sda3   8:3    0   384M  0 part /boot
  |-sda4   8:4    0 336.3G  0 part /sysroot
  `-sda5   8:5    0 100.1G  0 part /var/imageregistry <span id="CO69-1"><!--Empty--></span><span class="callout">1</span>
sdb      8:16   0 446.6G  0 disk
sr0     11:0    1   104M  0 rom</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO69-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												<code class="literal">/var/imageregistry</code> indicates that the disk is correctly partitioned.
											</div></dd></dl></div></li></ol></div></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/registry/#accessing-the-registry">Accessing the registry</a>
							</li></ul></div></section></section><section class="section" id="ztp-using-hub-cluster-templates_ztp-advanced-policy-config"><div class="titlepage"><div><div><h3 class="title">16.9.11. Using hub templates in PolicyGenTemplate CRs</h3></div></div></div><p>
					Topology Aware Lifecycle Manager supports partial Red Hat Advanced Cluster Management (RHACM) hub cluster template functions in configuration policies used with GitOps Zero Touch Provisioning (ZTP).
				</p><p>
					Hub-side cluster templates allow you to define configuration policies that can be dynamically customized to the target clusters. This reduces the need to create separate policies for many clusters with similiar configurations but with different values.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Policy templates are restricted to the same namespace as the namespace where the policy is defined. This means that you must create the objects referenced in the hub template in the same namespace where the policy is created.
					</p></div></div><p>
					The following supported hub template functions are available for use in GitOps ZTP with TALM:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.6/html-single/governance/index#fromConfigmap-func"><code class="literal">fromConfigmap</code></a> returns the value of the provided data key in the named <code class="literal">ConfigMap</code> resource.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								There is a <a class="link" href="https://kubernetes.io/docs/concepts/configuration/configmap/#motivation">1 MiB size limit</a> for <code class="literal">ConfigMap</code> CRs. The effective size for <code class="literal">ConfigMap</code> CRs is further limited by the <code class="literal">last-applied-configuration</code> annotation. To avoid the <code class="literal">last-applied-configuration</code> limitation, add the following annotation to the template <code class="literal">ConfigMap</code>:
							</p><pre class="programlisting language-yaml">argocd.argoproj.io/sync-options: Replace=true</pre></div></div></li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.6/html-single/governance/index#base64enc-func"><code class="literal">base64enc</code></a> returns the base64-encoded value of the input string
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.6/html-single/governance/index#base64dec-func"><code class="literal">base64dec</code></a> returns the decoded value of the base64-encoded input string
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.6/html-single/governance/index#indent-function"><code class="literal">indent</code></a> returns the input string with added indent spaces
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.6/html-single/governance/index#autoindent-function"><code class="literal">autoindent</code></a> returns the input string with added indent spaces based on the spacing used in the parent template
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.6/html-single/governance/index#toInt-function"><code class="literal">toInt</code></a> casts and returns the integer value of the input value
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.6/html-single/governance/index#toBool-function"><code class="literal">toBool</code></a> converts the input string into a boolean value, and returns the boolean
						</li></ul></div><p>
					Various <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.6/html-single/governance/index#open-source-community-functions">Open source community functions</a> are also available for use with GitOps ZTP.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.8/html-single/governance/index#hub-templates">RHACM support for hub cluster templates in configuration policies</a>
						</li></ul></div><section class="section" id="ztp-example-hub-template-functions_ztp-advanced-policy-config"><div class="titlepage"><div><div><h4 class="title">16.9.11.1. Example hub templates</h4></div></div></div><p>
						The following code examples are valid hub templates. Each of these templates return values from the <code class="literal">ConfigMap</code> CR with the name <code class="literal">test-config</code> in the <code class="literal">default</code> namespace.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Returns the value with the key <code class="literal">common-key</code>:
							</p><pre class="programlisting language-yaml">{{hub fromConfigMap "default" "test-config" "common-key" hub}}</pre></li><li class="listitem"><p class="simpara">
								Returns a string by using the concatenated value of the <code class="literal">.ManagedClusterName</code> field and the string <code class="literal">-name</code>:
							</p><pre class="programlisting language-yaml">{{hub fromConfigMap "default" "test-config" (printf "%s-name" .ManagedClusterName) hub}}</pre></li><li class="listitem"><p class="simpara">
								Casts and returns a boolean value from the concatenated value of the <code class="literal">.ManagedClusterName</code> field and the string <code class="literal">-name</code>:
							</p><pre class="programlisting language-yaml">{{hub fromConfigMap "default" "test-config" (printf "%s-name" .ManagedClusterName) | toBool hub}}</pre></li><li class="listitem"><p class="simpara">
								Casts and returns an integer value from the concatenated value of the <code class="literal">.ManagedClusterName</code> field and the string <code class="literal">-name</code>:
							</p><pre class="programlisting language-yaml">{{hub (printf "%s-name" .ManagedClusterName) | fromConfigMap "default" "test-config" | toInt hub}}</pre></li></ul></div></section><section class="section" id="ztp-specifying-nics-in-pgt-crs-with-hub-cluster-templates_ztp-advanced-policy-config"><div class="titlepage"><div><div><h4 class="title">16.9.11.2. Specifying host NICs in site PolicyGenTemplate CRs with hub cluster templates</h4></div></div></div><p>
						You can manage host NICs in a single <code class="literal">ConfigMap</code> CR and use hub cluster templates to populate the custom NIC values in the generated polices that get applied to the cluster hosts. Using hub cluster templates in site <code class="literal">PolicyGenTemplate</code> (PGT) CRs means that you do not need to create multiple single site PGT CRs for each site.
					</p><p>
						The following example shows you how to use a single <code class="literal">ConfigMap</code> CR to manage cluster host NICs and apply them to the cluster as polices by using a single <code class="literal">PolicyGenTemplate</code> site CR.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							When you use the <code class="literal">fromConfigmap</code> function, the <code class="literal">printf</code> variable is only available for the template resource <code class="literal">data</code> key fields. You cannot use it with <code class="literal">name</code> and <code class="literal">namespace</code> fields.
						</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								You have logged in to the hub cluster as a user with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have created a Git repository where you manage your custom site configuration data. The repository must be accessible from the hub cluster and be defined as a source repository for the GitOps ZTP ArgoCD application.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a <code class="literal">ConfigMap</code> resource that describes the NICs for a group of hosts. For example:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: sriovdata
  namespace: ztp-site
  annotations:
    argocd.argoproj.io/sync-options: Replace=true <span id="CO70-1"><!--Empty--></span><span class="callout">1</span>
data:
  example-sno-du_fh-numVfs: "8"
  example-sno-du_fh-pf: ens1f0
  example-sno-du_fh-priority: "10"
  example-sno-du_fh-vlan: "140"
  example-sno-du_mh-numVfs: "8"
  example-sno-du_mh-pf: ens3f0
  example-sno-du_mh-priority: "10"
  example-sno-du_mh-vlan: "150"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO70-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The <code class="literal">argocd.argoproj.io/sync-options</code> annotation is required only if the <code class="literal">ConfigMap</code> is larger than 1 MiB in size.
									</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									The <code class="literal">ConfigMap</code> must be in the same namespace with the policy that has the hub template substitution.
								</p></div></div></li><li class="listitem">
								Commit the <code class="literal">ConfigMap</code> CR in Git, and then push to the Git repository being monitored by the Argo CD application.
							</li><li class="listitem"><p class="simpara">
								Create a site PGT CR that uses templates to pull the required data from the <code class="literal">ConfigMap</code> object. For example:
							</p><pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "site"
  namespace: "ztp-site"
spec:
  remediationAction: inform
  bindingRules:
    group-du-sno: ""
  mcp: "master"
  sourceFiles:
    - fileName: SriovNetwork.yaml
      policyName: "config-policy"
      metadata:
        name: "sriov-nw-du-fh"
      spec:
        resourceName: du_fh
        vlan: '{{hub fromConfigMap "ztp-site" "sriovdata" (printf "%s-du_fh-vlan" .ManagedClusterName) | toInt hub}}'
    - fileName: SriovNetworkNodePolicy.yaml
      policyName: "config-policy"
      metadata:
        name: "sriov-nnp-du-fh"
      spec:
        deviceType: netdevice
        isRdma: true
        nicSelector:
          pfNames:
          - '{{hub fromConfigMap "ztp-site" "sriovdata" (printf "%s-du_fh-pf" .ManagedClusterName) | autoindent hub}}'
        numVfs: '{{hub fromConfigMap "ztp-site" "sriovdata" (printf "%s-du_fh-numVfs" .ManagedClusterName) | toInt hub}}'
        priority: '{{hub fromConfigMap "ztp-site" "sriovdata" (printf "%s-du_fh-priority" .ManagedClusterName) | toInt hub}}'
        resourceName: du_fh
    - fileName: SriovNetwork.yaml
      policyName: "config-policy"
      metadata:
        name: "sriov-nw-du-mh"
      spec:
        resourceName: du_mh
        vlan: '{{hub fromConfigMap "ztp-site" "sriovdata" (printf "%s-du_mh-vlan" .ManagedClusterName) | toInt hub}}'
    - fileName: SriovNetworkNodePolicy.yaml
      policyName: "config-policy"
      metadata:
        name: "sriov-nnp-du-mh"
      spec:
        deviceType: vfio-pci
        isRdma: false
        nicSelector:
          pfNames:
          - '{{hub fromConfigMap "ztp-site" "sriovdata" (printf "%s-du_mh-pf" .ManagedClusterName)  hub}}'
        numVfs: '{{hub fromConfigMap "ztp-site" "sriovdata" (printf "%s-du_mh-numVfs" .ManagedClusterName) | toInt hub}}'
        priority: '{{hub fromConfigMap "ztp-site" "sriovdata" (printf "%s-du_mh-priority" .ManagedClusterName) | toInt hub}}'
        resourceName: du_mh</pre></li><li class="listitem"><p class="simpara">
								Commit the site <code class="literal">PolicyGenTemplate</code> CR in Git and push to the Git repository that is monitored by the ArgoCD application.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Subsequent changes to the referenced <code class="literal">ConfigMap</code> CR are not automatically synced to the applied policies. You need to manually sync the new <code class="literal">ConfigMap</code> changes to update existing PolicyGenTemplate CRs. See "Syncing new ConfigMap changes to existing PolicyGenTemplate CRs".
								</p></div></div></li></ol></div></section><section class="section" id="ztp-managing-sriov-vlan-with-hub-cluster-templates-in-pgt_ztp-advanced-policy-config"><div class="titlepage"><div><div><h4 class="title">16.9.11.3. Specifying VLAN IDs in group PolicyGenTemplate CRs with hub cluster templates</h4></div></div></div><p>
						You can manage VLAN IDs for managed clusters in a single <code class="literal">ConfigMap</code> CR and use hub cluster templates to populate the VLAN IDs in the generated polices that get applied to the clusters.
					</p><p>
						The following example shows how you how manage VLAN IDs in single <code class="literal">ConfigMap</code> CR and apply them in individual cluster polices by using a single <code class="literal">PolicyGenTemplate</code> group CR.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							When using the <code class="literal">fromConfigmap</code> function, the <code class="literal">printf</code> variable is only available for the template resource <code class="literal">data</code> key fields. You cannot use it with <code class="literal">name</code> and <code class="literal">namespace</code> fields.
						</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								You have logged in to the hub cluster as a user with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have created a Git repository where you manage your custom site configuration data. The repository must be accessible from the hub cluster and be defined as a source repository for the Argo CD application.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a <code class="literal">ConfigMap</code> CR that describes the VLAN IDs for a group of cluster hosts. For example:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: site-data
  namespace: ztp-group
  annotations:
    argocd.argoproj.io/sync-options: Replace=true <span id="CO71-1"><!--Empty--></span><span class="callout">1</span>
data:
  site-1-vlan: "101"
  site-2-vlan: "234"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO71-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The <code class="literal">argocd.argoproj.io/sync-options</code> annotation is required only if the <code class="literal">ConfigMap</code> is larger than 1 MiB in size.
									</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									The <code class="literal">ConfigMap</code> must be in the same namespace with the policy that has the hub template substitution.
								</p></div></div></li><li class="listitem">
								Commit the <code class="literal">ConfigMap</code> CR in Git, and then push to the Git repository being monitored by the Argo CD application.
							</li><li class="listitem"><p class="simpara">
								Create a group PGT CR that uses a hub template to pull the required VLAN IDs from the <code class="literal">ConfigMap</code> object. For example, add the following YAML snippet to the group PGT CR:
							</p><pre class="programlisting language-yaml">- fileName: SriovNetwork.yaml
    policyName: "config-policy"
    metadata:
      name: "sriov-nw-du-mh"
      annotations:
        ran.openshift.io/ztp-deploy-wave: "10"
    spec:
      resourceName: du_mh
      vlan: '{{hub fromConfigMap "" "site-data" (printf "%s-vlan" .ManagedClusterName) | toInt hub}}'</pre></li><li class="listitem"><p class="simpara">
								Commit the group <code class="literal">PolicyGenTemplate</code> CR in Git, and then push to the Git repository being monitored by the Argo CD application.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Subsequent changes to the referenced <code class="literal">ConfigMap</code> CR are not automatically synced to the applied policies. You need to manually sync the new <code class="literal">ConfigMap</code> changes to update existing PolicyGenTemplate CRs. See "Syncing new ConfigMap changes to existing PolicyGenTemplate CRs".
								</p></div></div></li></ol></div></section><section class="section" id="ztp-syncing-new-configmap-changes-to-existing-pgt-crs_ztp-advanced-policy-config"><div class="titlepage"><div><div><h4 class="title">16.9.11.4. Syncing new ConfigMap changes to existing PolicyGenTemplate CRs</h4></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								You have logged in to the hub cluster as a user with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have created a <code class="literal">PolicyGenTemplate</code> CR that pulls information from a <code class="literal">ConfigMap</code> CR using hub cluster templates.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Update the contents of your <code class="literal">ConfigMap</code> CR, and apply the changes in the hub cluster.
							</li><li class="listitem"><p class="simpara">
								To sync the contents of the updated <code class="literal">ConfigMap</code> CR to the deployed policy, do either of the following:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Option 1: Delete the existing policy. ArgoCD uses the <code class="literal">PolicyGenTemplate</code> CR to immediately recreate the deleted policy. For example, run the following command:
									</p><pre class="programlisting language-terminal">$ oc delete policy &lt;policy_name&gt; -n &lt;policy_namespace&gt;</pre></li><li class="listitem"><p class="simpara">
										Option 2: Apply a special annotation <code class="literal">policy.open-cluster-management.io/trigger-update</code> to the policy with a different value every time when you update the <code class="literal">ConfigMap</code>. For example:
									</p><pre class="programlisting language-terminal">$ oc annotate policy &lt;policy_name&gt; -n &lt;policy_namespace&gt; policy.open-cluster-management.io/trigger-update="1"</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											You must apply the updated policy for the changes to take effect. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.6/html-single/governance/index#special-annotation-processing">Special annotation for reprocessing</a>.
										</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
								Optional: If it exists, delete the <code class="literal">ClusterGroupUpdate</code> CR that contains the policy. For example:
							</p><pre class="programlisting language-terminal">$ oc delete clustergroupupgrade &lt;cgu_name&gt; -n &lt;cgu_namespace&gt;</pre><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Create a new <code class="literal">ClusterGroupUpdate</code> CR that includes the policy to apply with the updated <code class="literal">ConfigMap</code> changes. For example, add the following YAML to the file <code class="literal">cgr-example.yaml</code>:
									</p><pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: &lt;cgr_name&gt;
  namespace: &lt;policy_namespace&gt;
spec:
  managedPolicies:
    - &lt;managed_policy&gt;
  enable: true
  clusters:
  - &lt;managed_cluster_1&gt;
  - &lt;managed_cluster_2&gt;
  remediationStrategy:
    maxConcurrency: 2
    timeout: 240</pre></li><li class="listitem"><p class="simpara">
										Apply the updated policy:
									</p><pre class="programlisting language-terminal">$ oc apply -f cgr-example.yaml</pre></li></ol></div></li></ol></div></section></section></section><section class="section" id="cnf-talm-for-cluster-updates"><div class="titlepage"><div><div><h2 class="title">16.10. Updating managed clusters with the Topology Aware Lifecycle Manager</h2></div></div></div><p>
				You can use the Topology Aware Lifecycle Manager (TALM) to manage the software lifecycle of multiple clusters. TALM uses Red Hat Advanced Cluster Management (RHACM) policies to perform changes on the target clusters.
			</p><section class="section" id="cnf-about-topology-aware-lifecycle-manager-config_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h3 class="title">16.10.1. About the Topology Aware Lifecycle Manager configuration</h3></div></div></div><p>
					The Topology Aware Lifecycle Manager (TALM) manages the deployment of Red Hat Advanced Cluster Management (RHACM) policies for one or more OpenShift Container Platform clusters. Using TALM in a large network of clusters allows the phased rollout of policies to the clusters in limited batches. This helps to minimize possible service disruptions when updating. With TALM, you can control the following actions:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The timing of the update
						</li><li class="listitem">
							The number of RHACM-managed clusters
						</li><li class="listitem">
							The subset of managed clusters to apply the policies to
						</li><li class="listitem">
							The update order of the clusters
						</li><li class="listitem">
							The set of policies remediated to the cluster
						</li><li class="listitem">
							The order of policies remediated to the cluster
						</li><li class="listitem">
							The assignment of a canary cluster
						</li></ul></div><p>
					For single-node OpenShift, the Topology Aware Lifecycle Manager (TALM) offers the following features:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Create a backup of a deployment before an upgrade
						</li><li class="listitem">
							Pre-caching images for clusters with limited bandwidth
						</li></ul></div><p>
					TALM supports the orchestration of the OpenShift Container Platform y-stream and z-stream updates, and day-two operations on y-streams and z-streams.
				</p></section><section class="section" id="cnf-about-topology-aware-lifecycle-manager-about-policies_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h3 class="title">16.10.2. About managed policies used with Topology Aware Lifecycle Manager</h3></div></div></div><p>
					The Topology Aware Lifecycle Manager (TALM) uses RHACM policies for cluster updates.
				</p><p>
					TALM can be used to manage the rollout of any policy CR where the <code class="literal">remediationAction</code> field is set to <code class="literal">inform</code>. Supported use cases include the following:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Manual user creation of policy CRs
						</li><li class="listitem">
							Automatically generated policies from the <code class="literal">PolicyGenTemplate</code> custom resource definition (CRD)
						</li></ul></div><p>
					For policies that update an Operator subscription with manual approval, TALM provides additional functionality that approves the installation of the updated Operator.
				</p><p>
					For more information about managed policies, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.8/html-single/governance/index#policy-overview">Policy Overview</a> in the RHACM documentation.
				</p><p>
					For more information about the <code class="literal">PolicyGenTemplate</code> CRD, see the "About the PolicyGenTemplate CRD" section in "Configuring managed clusters with policies and PolicyGenTemplate resources".
				</p></section><section class="section" id="installing-topology-aware-lifecycle-manager-using-web-console_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h3 class="title">16.10.3. Installing the Topology Aware Lifecycle Manager by using the web console</h3></div></div></div><p>
					You can use the OpenShift Container Platform web console to install the Topology Aware Lifecycle Manager.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the latest version of the RHACM Operator.
						</li><li class="listitem">
							Set up a hub cluster with disconnected regitry.
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the OpenShift Container Platform web console, navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span>.
						</li><li class="listitem">
							Search for the <span class="strong strong"><strong>Topology Aware Lifecycle Manager</strong></span> from the list of available Operators, and then click <span class="strong strong"><strong>Install</strong></span>.
						</li><li class="listitem">
							Keep the default selection of <span class="strong strong"><strong>Installation mode</strong></span> ["All namespaces on the cluster (default)"] and <span class="strong strong"><strong>Installed Namespace</strong></span> ("openshift-operators") to ensure that the Operator is installed properly.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Install</strong></span>.
						</li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						To confirm that the installation is successful:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Navigate to the <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span> page.
						</li><li class="listitem">
							Check that the Operator is installed in the <code class="literal">All Namespaces</code> namespace and its status is <code class="literal">Succeeded</code>.
						</li></ol></div><p>
					If the Operator is not installed successfully:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Navigate to the <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span> page and inspect the <code class="literal">Status</code> column for any errors or failures.
						</li><li class="listitem">
							Navigate to the <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Pods</strong></span> page and check the logs in any containers in the <code class="literal">cluster-group-upgrades-controller-manager</code> pod that are reporting issues.
						</li></ol></div></section><section class="section" id="installing-topology-aware-lifecycle-manager-using-cli_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h3 class="title">16.10.4. Installing the Topology Aware Lifecycle Manager by using the CLI</h3></div></div></div><p>
					You can use the OpenShift CLI (<code class="literal">oc</code>) to install the Topology Aware Lifecycle Manager (TALM).
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Install the latest version of the RHACM Operator.
						</li><li class="listitem">
							Set up a hub cluster with disconnected registry.
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">Subscription</code> CR:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Define the <code class="literal">Subscription</code> CR and save the YAML file, for example, <code class="literal">talm-subscription.yaml</code>:
								</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: openshift-topology-aware-lifecycle-manager-subscription
  namespace: openshift-operators
spec:
  channel: "stable"
  name: topology-aware-lifecycle-manager
  source: redhat-operators
  sourceNamespace: openshift-marketplace</pre></li><li class="listitem"><p class="simpara">
									Create the <code class="literal">Subscription</code> CR by running the following command:
								</p><pre class="programlisting language-terminal">$ oc create -f talm-subscription.yaml</pre></li></ol></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Verify that the installation succeeded by inspecting the CSV resource:
						</p><pre class="programlisting language-terminal">$ oc get csv -n openshift-operators</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                                   DISPLAY                            VERSION               REPLACES                           PHASE
topology-aware-lifecycle-manager.4.13.x   Topology Aware Lifecycle Manager   4.13.x                                      Succeeded</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Verify that the TALM is up and running:
						</p><pre class="programlisting language-terminal">$ oc get deploy -n openshift-operators</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAMESPACE                                          NAME                                             READY   UP-TO-DATE   AVAILABLE   AGE
openshift-operators                                cluster-group-upgrades-controller-manager        1/1     1            1           14s</pre>

							</p></div></li></ol></div></section><section class="section" id="talo-about-cgu-crs_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h3 class="title">16.10.5. About the ClusterGroupUpgrade CR</h3></div></div></div><p>
					The Topology Aware Lifecycle Manager (TALM) builds the remediation plan from the <code class="literal">ClusterGroupUpgrade</code> CR for a group of clusters. You can define the following specifications in a <code class="literal">ClusterGroupUpgrade</code> CR:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Clusters in the group
						</li><li class="listitem">
							Blocking <code class="literal">ClusterGroupUpgrade</code> CRs
						</li><li class="listitem">
							Applicable list of managed policies
						</li><li class="listitem">
							Number of concurrent updates
						</li><li class="listitem">
							Applicable canary updates
						</li><li class="listitem">
							Actions to perform before and after the update
						</li><li class="listitem">
							Update timing
						</li></ul></div><p>
					You can control the start time of an update using the <code class="literal">enable</code> field in the <code class="literal">ClusterGroupUpgrade</code> CR. For example, if you have a scheduled maintenance window of four hours, you can prepare a <code class="literal">ClusterGroupUpgrade</code> CR with the <code class="literal">enable</code> field set to <code class="literal">false</code>.
				</p><p>
					You can set the timeout by configuring the <code class="literal">spec.remediationStrategy.timeout</code> setting as follows:
				</p><pre class="programlisting language-yaml">spec
  remediationStrategy:
          maxConcurrency: 1
          timeout: 240</pre><p>
					You can use the <code class="literal">batchTimeoutAction</code> to determine what happens if an update fails for a cluster. You can specify <code class="literal">continue</code> to skip the failing cluster and continue to upgrade other clusters, or <code class="literal">abort</code> to stop policy remediation for all clusters. Once the timeout elapses, TALM removes all <code class="literal">enforce</code> policies to ensure that no further updates are made to clusters.
				</p><p>
					To apply the changes, you set the <code class="literal">enabled</code> field to <code class="literal">true</code>.
				</p><p>
					For more information see the "Applying update policies to managed clusters" section.
				</p><p>
					As TALM works through remediation of the policies to the specified clusters, the <code class="literal">ClusterGroupUpgrade</code> CR can report true or false statuses for a number of conditions.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						After TALM completes a cluster update, the cluster does not update again under the control of the same <code class="literal">ClusterGroupUpgrade</code> CR. You must create a new <code class="literal">ClusterGroupUpgrade</code> CR in the following cases:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								When you need to update the cluster again
							</li><li class="listitem">
								When the cluster changes to non-compliant with the <code class="literal">inform</code> policy after being updated
							</li></ul></div></div></div><section class="section" id="selecting_clusters_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h4 class="title">16.10.5.1. Selecting clusters</h4></div></div></div><p>
						TALM builds a remediation plan and selects clusters based on the following fields:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								The <code class="literal">clusterLabelSelector</code> field specifies the labels of the clusters that you want to update. This consists of a list of the standard label selectors from <code class="literal">k8s.io/apimachinery/pkg/apis/meta/v1</code>. Each selector in the list uses either label value pairs or label expressions. Matches from each selector are added to the final list of clusters along with the matches from the <code class="literal">clusterSelector</code> field and the <code class="literal">cluster</code> field.
							</li><li class="listitem">
								The <code class="literal">clusters</code> field specifies a list of clusters to update.
							</li><li class="listitem">
								The <code class="literal">canaries</code> field specifies the clusters for canary updates.
							</li><li class="listitem">
								The <code class="literal">maxConcurrency</code> field specifies the number of clusters to update in a batch.
							</li><li class="listitem">
								The <code class="literal">actions</code> field specifies <code class="literal">beforeEnable</code> actions that TALM takes as it begins the update process, and <code class="literal">afterCompletion</code> actions that TALM takes as it completes policy remediation for each cluster.
							</li></ul></div><p>
						You can use the <code class="literal">clusters</code>, <code class="literal">clusterLabelSelector</code>, and <code class="literal">clusterSelector</code> fields together to create a combined list of clusters.
					</p><p>
						The remediation plan starts with the clusters listed in the <code class="literal">canaries</code> field. Each canary cluster forms a single-cluster batch.
					</p><div class="formalpara"><p class="title"><strong>Sample <code class="literal">ClusterGroupUpgrade</code> CR with the enabled <code class="literal">field</code> set to <code class="literal">false</code></strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  creationTimestamp: '2022-11-18T16:27:15Z'
  finalizers:
    - ran.openshift.io/cleanup-finalizer
  generation: 1
  name: talm-cgu
  namespace: talm-namespace
  resourceVersion: '40451823'
  uid: cca245a5-4bca-45fa-89c0-aa6af81a596c
Spec:
  actions:
    afterCompletion: <span id="CO72-1"><!--Empty--></span><span class="callout">1</span>
      addClusterLabels:
        upgrade-done: ""
      deleteClusterLabels:
        upgrade-running: ""
      deleteObjects: true
    beforeEnable: <span id="CO72-2"><!--Empty--></span><span class="callout">2</span>
      addClusterLabels:
        upgrade-running: ""
  backup: false
  clusters: <span id="CO72-3"><!--Empty--></span><span class="callout">3</span>
    - spoke1
  enable: false <span id="CO72-4"><!--Empty--></span><span class="callout">4</span>
  managedPolicies: <span id="CO72-5"><!--Empty--></span><span class="callout">5</span>
    - talm-policy
  preCaching: false
  remediationStrategy: <span id="CO72-6"><!--Empty--></span><span class="callout">6</span>
    canaries: <span id="CO72-7"><!--Empty--></span><span class="callout">7</span>
        - spoke1
    maxConcurrency: 2 <span id="CO72-8"><!--Empty--></span><span class="callout">8</span>
    timeout: 240
  clusterLabelSelectors: <span id="CO72-9"><!--Empty--></span><span class="callout">9</span>
    - matchExpressions:
      - key: label1
      operator: In
      values:
        - value1a
        - value1b
  batchTimeoutAction: <span id="CO72-10"><!--Empty--></span><span class="callout">10</span>
status: <span id="CO72-11"><!--Empty--></span><span class="callout">11</span>
    computedMaxConcurrency: 2
    conditions:
      - lastTransitionTime: '2022-11-18T16:27:15Z'
        message: All selected clusters are valid
        reason: ClusterSelectionCompleted
        status: 'True'
        type: ClustersSelected <span id="CO72-12"><!--Empty--></span><span class="callout">12</span>
      - lastTransitionTime: '2022-11-18T16:27:15Z'
        message: Completed validation
        reason: ValidationCompleted
        status: 'True'
        type: Validated <span id="CO72-13"><!--Empty--></span><span class="callout">13</span>
      - lastTransitionTime: '2022-11-18T16:37:16Z'
        message: Not enabled
        reason: NotEnabled
        status: 'False'
        type: Progressing
    managedPoliciesForUpgrade:
      - name: talm-policy
        namespace: talm-namespace
    managedPoliciesNs:
      talm-policy: talm-namespace
    remediationPlan:
      - - spoke1
      - - spoke2
        - spoke3
    status:</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO72-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specifies the action that TALM takes when it completes policy remediation for each cluster.
							</div></dd><dt><a href="#CO72-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specifies the action that TALM takes as it begins the update process.
							</div></dd><dt><a href="#CO72-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Defines the list of clusters to update.
							</div></dd><dt><a href="#CO72-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								The <code class="literal">enable</code> field is set to <code class="literal">false</code>.
							</div></dd><dt><a href="#CO72-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Lists the user-defined set of policies to remediate.
							</div></dd><dt><a href="#CO72-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								Defines the specifics of the cluster updates.
							</div></dd><dt><a href="#CO72-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								Defines the clusters for canary updates.
							</div></dd><dt><a href="#CO72-8"><span class="callout">8</span></a> </dt><dd><div class="para">
								Defines the maximum number of concurrent updates in a batch. The number of remediation batches is the number of canary clusters, plus the number of clusters, except the canary clusters, divided by the <code class="literal">maxConcurrency</code> value. The clusters that are already compliant with all the managed policies are excluded from the remediation plan.
							</div></dd><dt><a href="#CO72-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								Displays the parameters for selecting clusters.
							</div></dd><dt><a href="#CO72-10"><span class="callout">10</span></a> </dt><dd><div class="para">
								Controls what happens if a batch times out. Possible values are <code class="literal">abort</code> or <code class="literal">continue</code>. If unspecified, the default is <code class="literal">continue</code>.
							</div></dd><dt><a href="#CO72-11"><span class="callout">11</span></a> </dt><dd><div class="para">
								Displays information about the status of the updates.
							</div></dd><dt><a href="#CO72-12"><span class="callout">12</span></a> </dt><dd><div class="para">
								The <code class="literal">ClustersSelected</code> condition shows that all selected clusters are valid.
							</div></dd><dt><a href="#CO72-13"><span class="callout">13</span></a> </dt><dd><div class="para">
								The <code class="literal">Validated</code> condition shows that all selected clusters have been validated.
							</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Any failures during the update of a canary cluster stops the update process.
						</p></div></div><p>
						When the remediation plan is successfully created, you can you set the <code class="literal">enable</code> field to <code class="literal">true</code> and TALM starts to update the non-compliant clusters with the specified managed policies.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You can only make changes to the <code class="literal">spec</code> fields if the <code class="literal">enable</code> field of the <code class="literal">ClusterGroupUpgrade</code> CR is set to <code class="literal">false</code>.
						</p></div></div></section><section class="section" id="validating_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h4 class="title">16.10.5.2. Validating</h4></div></div></div><p>
						TALM checks that all specified managed policies are available and correct, and uses the <code class="literal">Validated</code> condition to report the status and reasons as follows:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								<code class="literal">true</code>
							</p><p class="simpara">
								Validation is completed.
							</p></li><li class="listitem"><p class="simpara">
								<code class="literal">false</code>
							</p><p class="simpara">
								Policies are missing or invalid, or an invalid platform image has been specified.
							</p></li></ul></div></section><section class="section" id="precaching_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h4 class="title">16.10.5.3. Pre-caching</h4></div></div></div><p>
						Clusters might have limited bandwidth to access the container image registry, which can cause a timeout before the updates are completed. On single-node OpenShift clusters, you can use pre-caching to avoid this. The container image pre-caching starts when you create a <code class="literal">ClusterGroupUpgrade</code> CR with the <code class="literal">preCaching</code> field set to <code class="literal">true</code>. TALM compares the available disk space with the estimated OpenShift Container Platform image size to ensure that there is enough space. If a cluster has insufficient space, TALM cancels pre-caching for that cluster and does not remediate policies on it.
					</p><p>
						TALM uses the <code class="literal">PrecacheSpecValid</code> condition to report status information as follows:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								<code class="literal">true</code>
							</p><p class="simpara">
								The pre-caching spec is valid and consistent.
							</p></li><li class="listitem"><p class="simpara">
								<code class="literal">false</code>
							</p><p class="simpara">
								The pre-caching spec is incomplete.
							</p></li></ul></div><p>
						TALM uses the <code class="literal">PrecachingSucceeded</code> condition to report status information as follows:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								<code class="literal">true</code>
							</p><p class="simpara">
								TALM has concluded the pre-caching process. If pre-caching fails for any cluster, the update fails for that cluster but proceeds for all other clusters. A message informs you if pre-caching has failed for any clusters.
							</p></li><li class="listitem"><p class="simpara">
								<code class="literal">false</code>
							</p><p class="simpara">
								Pre-caching is still in progress for one or more clusters or has failed for all clusters.
							</p></li></ul></div><p>
						For more information see the "Using the container image pre-cache feature" section.
					</p></section><section class="section" id="creating_backup_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h4 class="title">16.10.5.4. Creating a backup</h4></div></div></div><p>
						For single-node OpenShift, TALM can create a backup of a deployment before an update. If the update fails, you can recover the previous version and restore a cluster to a working state without requiring a reprovision of applications. To use the backup feature you first create a <code class="literal">ClusterGroupUpgrade</code> CR with the <code class="literal">backup</code> field set to <code class="literal">true</code>. To ensure that the contents of the backup are up to date, the backup is not taken until you set the <code class="literal">enable</code> field in the <code class="literal">ClusterGroupUpgrade</code> CR to <code class="literal">true</code>.
					</p><p>
						TALM uses the <code class="literal">BackupSucceeded</code> condition to report the status and reasons as follows:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								<code class="literal">true</code>
							</p><p class="simpara">
								Backup is completed for all clusters or the backup run has completed but failed for one or more clusters. If backup fails for any cluster, the update fails for that cluster but proceeds for all other clusters.
							</p></li><li class="listitem"><p class="simpara">
								<code class="literal">false</code>
							</p><p class="simpara">
								Backup is still in progress for one or more clusters or has failed for all clusters.
							</p></li></ul></div><p>
						For more information, see the "Creating a backup of cluster resources before upgrade" section.
					</p></section><section class="section" id="updating_clusters_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h4 class="title">16.10.5.5. Updating clusters</h4></div></div></div><p>
						TALM enforces the policies following the remediation plan. Enforcing the policies for subsequent batches starts immediately after all the clusters of the current batch are compliant with all the managed policies. If the batch times out, TALM moves on to the next batch. The timeout value of a batch is the <code class="literal">spec.timeout</code> field divided by the number of batches in the remediation plan.
					</p><p>
						TALM uses the <code class="literal">Progressing</code> condition to report the status and reasons as follows:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								<code class="literal">true</code>
							</p><p class="simpara">
								TALM is remediating non-compliant policies.
							</p></li><li class="listitem"><p class="simpara">
								<code class="literal">false</code>
							</p><p class="simpara">
								The update is not in progress. Possible reasons for this are:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										All clusters are compliant with all the managed policies.
									</li><li class="listitem">
										The update has timed out as policy remediation took too long.
									</li><li class="listitem">
										Blocking CRs are missing from the system or have not yet completed.
									</li><li class="listitem">
										The <code class="literal">ClusterGroupUpgrade</code> CR is not enabled.
									</li><li class="listitem">
										Backup is still in progress.
									</li></ul></div></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The managed policies apply in the order that they are listed in the <code class="literal">managedPolicies</code> field in the <code class="literal">ClusterGroupUpgrade</code> CR. One managed policy is applied to the specified clusters at a time. When a cluster complies with the current policy, the next managed policy is applied to it.
						</p></div></div><div class="formalpara"><p class="title"><strong>Sample <code class="literal">ClusterGroupUpgrade</code> CR in the <code class="literal">Progressing</code> state</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  creationTimestamp: '2022-11-18T16:27:15Z'
  finalizers:
    - ran.openshift.io/cleanup-finalizer
  generation: 1
  name: talm-cgu
  namespace: talm-namespace
  resourceVersion: '40451823'
  uid: cca245a5-4bca-45fa-89c0-aa6af81a596c
Spec:
  actions:
    afterCompletion:
      deleteObjects: true
    beforeEnable: {}
  backup: false
  clusters:
    - spoke1
  enable: true
  managedPolicies:
    - talm-policy
  preCaching: true
  remediationStrategy:
    canaries:
        - spoke1
    maxConcurrency: 2
    timeout: 240
  clusterLabelSelectors:
    - matchExpressions:
      - key: label1
      operator: In
      values:
        - value1a
        - value1b
  batchTimeoutAction:
status:
    clusters:
      - name: spoke1
        state: complete
    computedMaxConcurrency: 2
    conditions:
      - lastTransitionTime: '2022-11-18T16:27:15Z'
        message: All selected clusters are valid
        reason: ClusterSelectionCompleted
        status: 'True'
        type: ClustersSelected
      - lastTransitionTime: '2022-11-18T16:27:15Z'
        message: Completed validation
        reason: ValidationCompleted
        status: 'True'
        type: Validated
      - lastTransitionTime: '2022-11-18T16:37:16Z'
        message: Remediating non-compliant policies
        reason: InProgress
        status: 'True'
        type: Progressing <span id="CO73-1"><!--Empty--></span><span class="callout">1</span>
    managedPoliciesForUpgrade:
      - name: talm-policy
        namespace: talm-namespace
    managedPoliciesNs:
      talm-policy: talm-namespace
    remediationPlan:
      - - spoke1
      - - spoke2
        - spoke3
    status:
      currentBatch: 2
      currentBatchRemediationProgress:
        spoke2:
          state: Completed
        spoke3:
          policyIndex: 0
          state: InProgress
      currentBatchStartedAt: '2022-11-18T16:27:16Z'
      startedAt: '2022-11-18T16:27:15Z'</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO73-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The <code class="literal">Progressing</code> fields show that TALM is in the process of remediating policies.
							</div></dd></dl></div></section><section class="section" id="update_status_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h4 class="title">16.10.5.6. Update status</h4></div></div></div><p>
						TALM uses the <code class="literal">Succeeded</code> condition to report the status and reasons as follows:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								<code class="literal">true</code>
							</p><p class="simpara">
								All clusters are compliant with the specified managed policies.
							</p></li><li class="listitem"><p class="simpara">
								<code class="literal">false</code>
							</p><p class="simpara">
								Policy remediation failed as there were no clusters available for remediation, or because policy remediation took too long for one of the following reasons:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										The current batch contains canary updates and the cluster in the batch does not comply with all the managed policies within the batch timeout.
									</li><li class="listitem">
										Clusters did not comply with the managed policies within the <code class="literal">timeout</code> value specified in the <code class="literal">remediationStrategy</code> field.
									</li></ul></div></li></ul></div><div class="formalpara"><p class="title"><strong>Sample <code class="literal">ClusterGroupUpgrade</code> CR in the <code class="literal">Succeeded</code> state</strong></p><p>
							
<pre class="programlisting language-yaml">    apiVersion: ran.openshift.io/v1alpha1
    kind: ClusterGroupUpgrade
    metadata:
      name: cgu-upgrade-complete
      namespace: default
    spec:
      clusters:
      - spoke1
      - spoke4
      enable: true
      managedPolicies:
      - policy1-common-cluster-version-policy
      - policy2-common-pao-sub-policy
      remediationStrategy:
        maxConcurrency: 1
        timeout: 240
    status: <span id="CO74-1"><!--Empty--></span><span class="callout">1</span>
      clusters:
        - name: spoke1
          state: complete
        - name: spoke4
          state: complete
      conditions:
      - message: All selected clusters are valid
        reason: ClusterSelectionCompleted
        status: "True"
        type: ClustersSelected
      - message: Completed validation
        reason: ValidationCompleted
        status: "True"
        type: Validated
      - message: All clusters are compliant with all the managed policies
        reason: Completed
        status: "False"
        type: Progressing <span id="CO74-2"><!--Empty--></span><span class="callout">2</span>
      - message: All clusters are compliant with all the managed policies
        reason: Completed
        status: "True"
        type: Succeeded <span id="CO74-3"><!--Empty--></span><span class="callout">3</span>
      managedPoliciesForUpgrade:
      - name: policy1-common-cluster-version-policy
        namespace: default
      - name: policy2-common-pao-sub-policy
        namespace: default
      remediationPlan:
      - - spoke1
      - - spoke4
      status:
        completedAt: '2022-11-18T16:27:16Z'
        startedAt: '2022-11-18T16:27:15Z'</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO74-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								In the <code class="literal">Progressing</code> fields, the status is <code class="literal">false</code> as the update has completed; clusters are compliant with all the managed policies.
							</div></dd><dt><a href="#CO74-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								The <code class="literal">Succeeded</code> fields show that the validations completed successfully.
							</div></dd><dt><a href="#CO74-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The <code class="literal">status</code> field includes a list of clusters and their respective statuses. The status of a cluster can be <code class="literal">complete</code> or <code class="literal">timedout</code>.
							</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Sample <code class="literal">ClusterGroupUpgrade</code> CR in the <code class="literal">timedout</code> state</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  creationTimestamp: '2022-11-18T16:27:15Z'
  finalizers:
    - ran.openshift.io/cleanup-finalizer
  generation: 1
  name: talm-cgu
  namespace: talm-namespace
  resourceVersion: '40451823'
  uid: cca245a5-4bca-45fa-89c0-aa6af81a596c
spec:
  actions:
    afterCompletion:
      deleteObjects: true
    beforeEnable: {}
  backup: false
  clusters:
    - spoke1
    - spoke2
  enable: true
  managedPolicies:
    - talm-policy
  preCaching: false
  remediationStrategy:
    maxConcurrency: 2
    timeout: 240
status:
  clusters:
    - name: spoke1
      state: complete
    - currentPolicy: <span id="CO75-1"><!--Empty--></span><span class="callout">1</span>
        name: talm-policy
        status: NonCompliant
      name: spoke2
      state: timedout
  computedMaxConcurrency: 2
  conditions:
    - lastTransitionTime: '2022-11-18T16:27:15Z'
      message: All selected clusters are valid
      reason: ClusterSelectionCompleted
      status: 'True'
      type: ClustersSelected
    - lastTransitionTime: '2022-11-18T16:27:15Z'
      message: Completed validation
      reason: ValidationCompleted
      status: 'True'
      type: Validated
    - lastTransitionTime: '2022-11-18T16:37:16Z'
      message: Policy remediation took too long
      reason: TimedOut
      status: 'False'
      type: Progressing
    - lastTransitionTime: '2022-11-18T16:37:16Z'
      message: Policy remediation took too long
      reason: TimedOut
      status: 'False'
      type: Succeeded <span id="CO75-2"><!--Empty--></span><span class="callout">2</span>
  managedPoliciesForUpgrade:
    - name: talm-policy
      namespace: talm-namespace
  managedPoliciesNs:
    talm-policy: talm-namespace
  remediationPlan:
    - - spoke1
      - spoke2
  status:
        startedAt: '2022-11-18T16:27:15Z'
        completedAt: '2022-11-18T20:27:15Z'</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO75-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								If a cluster’s state is <code class="literal">timedout</code>, the <code class="literal">currentPolicy</code> field shows the name of the policy and the policy status.
							</div></dd><dt><a href="#CO75-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The status for <code class="literal">succeeded</code> is <code class="literal">false</code> and the message indicates that policy remediation took too long.
							</div></dd></dl></div></section><section class="section" id="cnf-about-topology-aware-lifecycle-manager-blocking-crs_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h4 class="title">16.10.5.7. Blocking ClusterGroupUpgrade CRs</h4></div></div></div><p>
						You can create multiple <code class="literal">ClusterGroupUpgrade</code> CRs and control their order of application.
					</p><p>
						For example, if you create <code class="literal">ClusterGroupUpgrade</code> CR C that blocks the start of <code class="literal">ClusterGroupUpgrade</code> CR A, then <code class="literal">ClusterGroupUpgrade</code> CR A cannot start until the status of <code class="literal">ClusterGroupUpgrade</code> CR C becomes <code class="literal">UpgradeComplete</code>.
					</p><p>
						One <code class="literal">ClusterGroupUpgrade</code> CR can have multiple blocking CRs. In this case, all the blocking CRs must complete before the upgrade for the current CR can start.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Install the Topology Aware Lifecycle Manager (TALM).
							</li><li class="listitem">
								Provision one or more managed clusters.
							</li><li class="listitem">
								Log in as a user with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								Create RHACM policies in the hub cluster.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Save the content of the <code class="literal">ClusterGroupUpgrade</code> CRs in the <code class="literal">cgu-a.yaml</code>, <code class="literal">cgu-b.yaml</code>, and <code class="literal">cgu-c.yaml</code> files.
							</p><pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: cgu-a
  namespace: default
spec:
  blockingCRs: <span id="CO76-1"><!--Empty--></span><span class="callout">1</span>
  - name: cgu-c
    namespace: default
  clusters:
  - spoke1
  - spoke2
  - spoke3
  enable: false
  managedPolicies:
  - policy1-common-cluster-version-policy
  - policy2-common-pao-sub-policy
  - policy3-common-ptp-sub-policy
  remediationStrategy:
    canaries:
    - spoke1
    maxConcurrency: 2
    timeout: 240
status:
  conditions:
  - message: The ClusterGroupUpgrade CR is not enabled
    reason: UpgradeNotStarted
    status: "False"
    type: Ready
  copiedPolicies:
  - cgu-a-policy1-common-cluster-version-policy
  - cgu-a-policy2-common-pao-sub-policy
  - cgu-a-policy3-common-ptp-sub-policy
  managedPoliciesForUpgrade:
  - name: policy1-common-cluster-version-policy
    namespace: default
  - name: policy2-common-pao-sub-policy
    namespace: default
  - name: policy3-common-ptp-sub-policy
    namespace: default
  placementBindings:
  - cgu-a-policy1-common-cluster-version-policy
  - cgu-a-policy2-common-pao-sub-policy
  - cgu-a-policy3-common-ptp-sub-policy
  placementRules:
  - cgu-a-policy1-common-cluster-version-policy
  - cgu-a-policy2-common-pao-sub-policy
  - cgu-a-policy3-common-ptp-sub-policy
  remediationPlan:
  - - spoke1
  - - spoke2</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO76-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Defines the blocking CRs. The <code class="literal">cgu-a</code> update cannot start until <code class="literal">cgu-c</code> is complete.
									</div></dd></dl></div><pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: cgu-b
  namespace: default
spec:
  blockingCRs: <span id="CO77-1"><!--Empty--></span><span class="callout">1</span>
  - name: cgu-a
    namespace: default
  clusters:
  - spoke4
  - spoke5
  enable: false
  managedPolicies:
  - policy1-common-cluster-version-policy
  - policy2-common-pao-sub-policy
  - policy3-common-ptp-sub-policy
  - policy4-common-sriov-sub-policy
  remediationStrategy:
    maxConcurrency: 1
    timeout: 240
status:
  conditions:
  - message: The ClusterGroupUpgrade CR is not enabled
    reason: UpgradeNotStarted
    status: "False"
    type: Ready
  copiedPolicies:
  - cgu-b-policy1-common-cluster-version-policy
  - cgu-b-policy2-common-pao-sub-policy
  - cgu-b-policy3-common-ptp-sub-policy
  - cgu-b-policy4-common-sriov-sub-policy
  managedPoliciesForUpgrade:
  - name: policy1-common-cluster-version-policy
    namespace: default
  - name: policy2-common-pao-sub-policy
    namespace: default
  - name: policy3-common-ptp-sub-policy
    namespace: default
  - name: policy4-common-sriov-sub-policy
    namespace: default
  placementBindings:
  - cgu-b-policy1-common-cluster-version-policy
  - cgu-b-policy2-common-pao-sub-policy
  - cgu-b-policy3-common-ptp-sub-policy
  - cgu-b-policy4-common-sriov-sub-policy
  placementRules:
  - cgu-b-policy1-common-cluster-version-policy
  - cgu-b-policy2-common-pao-sub-policy
  - cgu-b-policy3-common-ptp-sub-policy
  - cgu-b-policy4-common-sriov-sub-policy
  remediationPlan:
  - - spoke4
  - - spoke5
  status: {}</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO77-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The <code class="literal">cgu-b</code> update cannot start until <code class="literal">cgu-a</code> is complete.
									</div></dd></dl></div><pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: cgu-c
  namespace: default
spec: <span id="CO78-1"><!--Empty--></span><span class="callout">1</span>
  clusters:
  - spoke6
  enable: false
  managedPolicies:
  - policy1-common-cluster-version-policy
  - policy2-common-pao-sub-policy
  - policy3-common-ptp-sub-policy
  - policy4-common-sriov-sub-policy
  remediationStrategy:
    maxConcurrency: 1
    timeout: 240
status:
  conditions:
  - message: The ClusterGroupUpgrade CR is not enabled
    reason: UpgradeNotStarted
    status: "False"
    type: Ready
  copiedPolicies:
  - cgu-c-policy1-common-cluster-version-policy
  - cgu-c-policy4-common-sriov-sub-policy
  managedPoliciesCompliantBeforeUpgrade:
  - policy2-common-pao-sub-policy
  - policy3-common-ptp-sub-policy
  managedPoliciesForUpgrade:
  - name: policy1-common-cluster-version-policy
    namespace: default
  - name: policy4-common-sriov-sub-policy
    namespace: default
  placementBindings:
  - cgu-c-policy1-common-cluster-version-policy
  - cgu-c-policy4-common-sriov-sub-policy
  placementRules:
  - cgu-c-policy1-common-cluster-version-policy
  - cgu-c-policy4-common-sriov-sub-policy
  remediationPlan:
  - - spoke6
  status: {}</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO78-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The <code class="literal">cgu-c</code> update does not have any blocking CRs. TALM starts the <code class="literal">cgu-c</code> update when the <code class="literal">enable</code> field is set to <code class="literal">true</code>.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create the <code class="literal">ClusterGroupUpgrade</code> CRs by running the following command for each relevant CR:
							</p><pre class="programlisting language-terminal">$ oc apply -f &lt;name&gt;.yaml</pre></li><li class="listitem"><p class="simpara">
								Start the update process by running the following command for each relevant CR:
							</p><pre class="programlisting language-terminal">$ oc --namespace=default patch clustergroupupgrade.ran.openshift.io/&lt;name&gt; \
--type merge -p '{"spec":{"enable":true}}'</pre><p class="simpara">
								The following examples show <code class="literal">ClusterGroupUpgrade</code> CRs where the <code class="literal">enable</code> field is set to <code class="literal">true</code>:
							</p><div class="formalpara"><p class="title"><strong>Example for <code class="literal">cgu-a</code> with blocking CRs</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: cgu-a
  namespace: default
spec:
  blockingCRs:
  - name: cgu-c
    namespace: default
  clusters:
  - spoke1
  - spoke2
  - spoke3
  enable: true
  managedPolicies:
  - policy1-common-cluster-version-policy
  - policy2-common-pao-sub-policy
  - policy3-common-ptp-sub-policy
  remediationStrategy:
    canaries:
    - spoke1
    maxConcurrency: 2
    timeout: 240
status:
  conditions:
  - message: 'The ClusterGroupUpgrade CR is blocked by other CRs that have not yet
      completed: [cgu-c]' <span id="CO79-1"><!--Empty--></span><span class="callout">1</span>
    reason: UpgradeCannotStart
    status: "False"
    type: Ready
  copiedPolicies:
  - cgu-a-policy1-common-cluster-version-policy
  - cgu-a-policy2-common-pao-sub-policy
  - cgu-a-policy3-common-ptp-sub-policy
  managedPoliciesForUpgrade:
  - name: policy1-common-cluster-version-policy
    namespace: default
  - name: policy2-common-pao-sub-policy
    namespace: default
  - name: policy3-common-ptp-sub-policy
    namespace: default
  placementBindings:
  - cgu-a-policy1-common-cluster-version-policy
  - cgu-a-policy2-common-pao-sub-policy
  - cgu-a-policy3-common-ptp-sub-policy
  placementRules:
  - cgu-a-policy1-common-cluster-version-policy
  - cgu-a-policy2-common-pao-sub-policy
  - cgu-a-policy3-common-ptp-sub-policy
  remediationPlan:
  - - spoke1
  - - spoke2
  status: {}</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO79-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Shows the list of blocking CRs.
									</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example for <code class="literal">cgu-b</code> with blocking CRs</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: cgu-b
  namespace: default
spec:
  blockingCRs:
  - name: cgu-a
    namespace: default
  clusters:
  - spoke4
  - spoke5
  enable: true
  managedPolicies:
  - policy1-common-cluster-version-policy
  - policy2-common-pao-sub-policy
  - policy3-common-ptp-sub-policy
  - policy4-common-sriov-sub-policy
  remediationStrategy:
    maxConcurrency: 1
    timeout: 240
status:
  conditions:
  - message: 'The ClusterGroupUpgrade CR is blocked by other CRs that have not yet
      completed: [cgu-a]' <span id="CO80-1"><!--Empty--></span><span class="callout">1</span>
    reason: UpgradeCannotStart
    status: "False"
    type: Ready
  copiedPolicies:
  - cgu-b-policy1-common-cluster-version-policy
  - cgu-b-policy2-common-pao-sub-policy
  - cgu-b-policy3-common-ptp-sub-policy
  - cgu-b-policy4-common-sriov-sub-policy
  managedPoliciesForUpgrade:
  - name: policy1-common-cluster-version-policy
    namespace: default
  - name: policy2-common-pao-sub-policy
    namespace: default
  - name: policy3-common-ptp-sub-policy
    namespace: default
  - name: policy4-common-sriov-sub-policy
    namespace: default
  placementBindings:
  - cgu-b-policy1-common-cluster-version-policy
  - cgu-b-policy2-common-pao-sub-policy
  - cgu-b-policy3-common-ptp-sub-policy
  - cgu-b-policy4-common-sriov-sub-policy
  placementRules:
  - cgu-b-policy1-common-cluster-version-policy
  - cgu-b-policy2-common-pao-sub-policy
  - cgu-b-policy3-common-ptp-sub-policy
  - cgu-b-policy4-common-sriov-sub-policy
  remediationPlan:
  - - spoke4
  - - spoke5
  status: {}</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO80-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Shows the list of blocking CRs.
									</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example for <code class="literal">cgu-c</code> with blocking CRs</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: cgu-c
  namespace: default
spec:
  clusters:
  - spoke6
  enable: true
  managedPolicies:
  - policy1-common-cluster-version-policy
  - policy2-common-pao-sub-policy
  - policy3-common-ptp-sub-policy
  - policy4-common-sriov-sub-policy
  remediationStrategy:
    maxConcurrency: 1
    timeout: 240
status:
  conditions:
  - message: The ClusterGroupUpgrade CR has upgrade policies that are still non compliant <span id="CO81-1"><!--Empty--></span><span class="callout">1</span>
    reason: UpgradeNotCompleted
    status: "False"
    type: Ready
  copiedPolicies:
  - cgu-c-policy1-common-cluster-version-policy
  - cgu-c-policy4-common-sriov-sub-policy
  managedPoliciesCompliantBeforeUpgrade:
  - policy2-common-pao-sub-policy
  - policy3-common-ptp-sub-policy
  managedPoliciesForUpgrade:
  - name: policy1-common-cluster-version-policy
    namespace: default
  - name: policy4-common-sriov-sub-policy
    namespace: default
  placementBindings:
  - cgu-c-policy1-common-cluster-version-policy
  - cgu-c-policy4-common-sriov-sub-policy
  placementRules:
  - cgu-c-policy1-common-cluster-version-policy
  - cgu-c-policy4-common-sriov-sub-policy
  remediationPlan:
  - - spoke6
  status:
    currentBatch: 1
    remediationPlanForBatch:
      spoke6: 0</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO81-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The <code class="literal">cgu-c</code> update does not have any blocking CRs.
									</div></dd></dl></div></li></ol></div></section></section><section class="section" id="talo-policies-concept_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h3 class="title">16.10.6. Update policies on managed clusters</h3></div></div></div><p>
					The Topology Aware Lifecycle Manager (TALM) remediates a set of <code class="literal">inform</code> policies for the clusters specified in the <code class="literal">ClusterGroupUpgrade</code> CR. TALM remediates <code class="literal">inform</code> policies by making <code class="literal">enforce</code> copies of the managed RHACM policies. Each copied policy has its own corresponding RHACM placement rule and RHACM placement binding.
				</p><p>
					One by one, TALM adds each cluster from the current batch to the placement rule that corresponds with the applicable managed policy. If a cluster is already compliant with a policy, TALM skips applying that policy on the compliant cluster. TALM then moves on to applying the next policy to the non-compliant cluster. After TALM completes the updates in a batch, all clusters are removed from the placement rules associated with the copied policies. Then, the update of the next batch starts.
				</p><p>
					If a spoke cluster does not report any compliant state to RHACM, the managed policies on the hub cluster can be missing status information that TALM needs. TALM handles these cases in the following ways:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							If a policy’s <code class="literal">status.compliant</code> field is missing, TALM ignores the policy and adds a log entry. Then, TALM continues looking at the policy’s <code class="literal">status.status</code> field.
						</li><li class="listitem">
							If a policy’s <code class="literal">status.status</code> is missing, TALM produces an error.
						</li><li class="listitem">
							If a cluster’s compliance status is missing in the policy’s <code class="literal">status.status</code> field, TALM considers that cluster to be non-compliant with that policy.
						</li></ul></div><p>
					The <code class="literal">ClusterGroupUpgrade</code> CR’s <code class="literal">batchTimeoutAction</code> determines what happens if an upgrade fails for a cluster. You can specify <code class="literal">continue</code> to skip the failing cluster and continue to upgrade other clusters, or specify <code class="literal">abort</code> to stop the policy remediation for all clusters. Once the timeout elapses, TALM removes all enforce policies to ensure that no further updates are made to clusters.
				</p><div class="formalpara"><p class="title"><strong>Example upgrade policy</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: policy.open-cluster-management.io/v1
kind: Policy
metadata:
  name: ocp-4.9.4
  namespace: platform-upgrade
spec:
  disabled: false
  policy-templates:
  - objectDefinition:
      apiVersion: policy.open-cluster-management.io/v1
      kind: ConfigurationPolicy
      metadata:
        name: upgrade
      spec:
        namespaceselector:
          exclude:
          - kube-*
          include:
          - '*'
        object-templates:
        - complianceType: musthave
          objectDefinition:
            apiVersion: config.openshift.io/v1
            kind: ClusterVersion
            metadata:
              name: version
            spec:
              channel: stable-4.9
              desiredUpdate:
                version: 4.9.4
              upstream: https://api.openshift.com/api/upgrades_info/v1/graph
            status:
              history:
                - state: Completed
                  version: 4.9.4
        remediationAction: inform
        severity: low
  remediationAction: inform</pre>

					</p></div><p>
					For more information about RHACM policies, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.8/html-single/governance/index#policy-overview">Policy overview</a>.
				</p><div class="_additional-resources _additional-resources"><p class="title"><strong>Additional resources</strong></p><p>
						For more information about the <code class="literal">PolicyGenTemplate</code> CRD, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-the-policygentemplate_ztp-configuring-managed-clusters-policies">About the PolicyGenTemplate CRD</a>.
					</p></div><section class="section" id="talo-about-subscription-crs_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h4 class="title">16.10.6.1. Configuring Operator subscriptions for managed clusters that you install with TALM</h4></div></div></div><p>
						TALM can only approve the install plan for an Operator if the <code class="literal">Subscription</code> CR of the Operator contains a valid <code class="literal">status</code> field. You can use the following fields:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<code class="literal">status.state.AtLatestKnown</code> for the latest Operator version
							</li><li class="listitem">
								<code class="literal">status.installedCSV</code> for a specific Operator version
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Add one of the following status fields to the <code class="literal">Subscription</code> CR of the Operator:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Add the <code class="literal">status.state.AtLatestKnown</code> field for the latest Operator version:
									</p><div class="formalpara"><p class="title"><strong>Example Subscription CR</strong></p><p>
											
<pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: cluster-logging
  namespace: openshift-logging
  annotations:
    ran.openshift.io/ztp-deploy-wave: "2"
spec:
  channel: "stable"
  name: cluster-logging
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  installPlanApproval: Manual
status:
  state: AtLatestKnown <span id="CO82-1"><!--Empty--></span><span class="callout">1</span></pre>

										</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO82-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												The <code class="literal">status.state: AtLatestKnown</code> field is used for the latest Operator version available from the Operator catalog.
											</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											When a new version of the Operator is available in the registry, the associated policy becomes non-compliant.
										</p></div></div></li><li class="listitem"><p class="simpara">
										Add the <code class="literal">status.installedCSV</code> field for a specific Operator version:
									</p><div class="formalpara"><p class="title"><strong>Example Subscription CR</strong></p><p>
											
<pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: cluster-logging
  namespace: openshift-logging
  annotations:
    ran.openshift.io/ztp-deploy-wave: "2"
spec:
  channel: "stable"
  name: cluster-logging
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  installPlanApproval: Manual
status:
  installedCSV: cluster-logging.v5.7.5 <span id="CO83-1"><!--Empty--></span><span class="callout">1</span></pre>

										</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO83-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												The <code class="literal">status.installedCSV</code> field with the CSV value is used for a specific version of an Operator, for example, <code class="literal">status.installedCSV: cluster-logging.v5.7.5</code>.
											</div></dd></dl></div></li></ol></div></li><li class="listitem">
								Apply the changed <code class="literal">Subscription</code> policy to your managed clusters with a <code class="literal">ClusterGroupUpgrade</code> CR.
							</li></ol></div></section><section class="section" id="talo-apply-policies_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h4 class="title">16.10.6.2. Applying update policies to managed clusters</h4></div></div></div><p>
						You can update your managed clusters by applying your policies.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Install the Topology Aware Lifecycle Manager (TALM).
							</li><li class="listitem">
								Provision one or more managed clusters.
							</li><li class="listitem">
								Log in as a user with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								Create RHACM policies in the hub cluster.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Save the contents of the <code class="literal">ClusterGroupUpgrade</code> CR in the <code class="literal">cgu-1.yaml</code> file.
							</p><pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: cgu-1
  namespace: default
spec:
  managedPolicies: <span id="CO84-1"><!--Empty--></span><span class="callout">1</span>
    - policy1-common-cluster-version-policy
    - policy2-common-nto-sub-policy
    - policy3-common-ptp-sub-policy
    - policy4-common-sriov-sub-policy
  enable: false
  clusters: <span id="CO84-2"><!--Empty--></span><span class="callout">2</span>
  - spoke1
  - spoke2
  - spoke5
  - spoke6
  remediationStrategy:
    maxConcurrency: 2 <span id="CO84-3"><!--Empty--></span><span class="callout">3</span>
    timeout: 240 <span id="CO84-4"><!--Empty--></span><span class="callout">4</span>
  batchTimeoutAction: <span id="CO84-5"><!--Empty--></span><span class="callout">5</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO84-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The name of the policies to apply.
									</div></dd><dt><a href="#CO84-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The list of clusters to update.
									</div></dd><dt><a href="#CO84-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The <code class="literal">maxConcurrency</code> field signifies the number of clusters updated at the same time.
									</div></dd><dt><a href="#CO84-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										The update timeout in minutes.
									</div></dd><dt><a href="#CO84-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										Controls what happens if a batch times out. Possible values are <code class="literal">abort</code> or <code class="literal">continue</code>. If unspecified, the default is <code class="literal">continue</code>.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create the <code class="literal">ClusterGroupUpgrade</code> CR by running the following command:
							</p><pre class="programlisting language-terminal">$ oc create -f cgu-1.yaml</pre><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Check if the <code class="literal">ClusterGroupUpgrade</code> CR was created in the hub cluster by running the following command:
									</p><pre class="programlisting language-terminal">$ oc get cgu --all-namespaces</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">NAMESPACE   NAME  AGE  STATE      DETAILS
default     cgu-1 8m55 NotEnabled Not Enabled</pre>

										</p></div></li><li class="listitem"><p class="simpara">
										Check the status of the update by running the following command:
									</p><pre class="programlisting language-terminal">$ oc get cgu -n default cgu-1 -ojsonpath='{.status}' | jq</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-json">{
  "computedMaxConcurrency": 2,
  "conditions": [
    {
      "lastTransitionTime": "2022-02-25T15:34:07Z",
      "message": "Not enabled", <span id="CO85-1"><!--Empty--></span><span class="callout">1</span>
      "reason": "NotEnabled",
      "status": "False",
      "type": "Progressing"
    }
  ],
  "copiedPolicies": [
    "cgu-policy1-common-cluster-version-policy",
    "cgu-policy2-common-nto-sub-policy",
    "cgu-policy3-common-ptp-sub-policy",
    "cgu-policy4-common-sriov-sub-policy"
  ],
  "managedPoliciesContent": {
    "policy1-common-cluster-version-policy": "null",
    "policy2-common-nto-sub-policy": "[{\"kind\":\"Subscription\",\"name\":\"node-tuning-operator\",\"namespace\":\"openshift-cluster-node-tuning-operator\"}]",
    "policy3-common-ptp-sub-policy": "[{\"kind\":\"Subscription\",\"name\":\"ptp-operator-subscription\",\"namespace\":\"openshift-ptp\"}]",
    "policy4-common-sriov-sub-policy": "[{\"kind\":\"Subscription\",\"name\":\"sriov-network-operator-subscription\",\"namespace\":\"openshift-sriov-network-operator\"}]"
  },
  "managedPoliciesForUpgrade": [
    {
      "name": "policy1-common-cluster-version-policy",
      "namespace": "default"
    },
    {
      "name": "policy2-common-nto-sub-policy",
      "namespace": "default"
    },
    {
      "name": "policy3-common-ptp-sub-policy",
      "namespace": "default"
    },
    {
      "name": "policy4-common-sriov-sub-policy",
      "namespace": "default"
    }
  ],
  "managedPoliciesNs": {
    "policy1-common-cluster-version-policy": "default",
    "policy2-common-nto-sub-policy": "default",
    "policy3-common-ptp-sub-policy": "default",
    "policy4-common-sriov-sub-policy": "default"
  },
  "placementBindings": [
    "cgu-policy1-common-cluster-version-policy",
    "cgu-policy2-common-nto-sub-policy",
    "cgu-policy3-common-ptp-sub-policy",
    "cgu-policy4-common-sriov-sub-policy"
  ],
  "placementRules": [
    "cgu-policy1-common-cluster-version-policy",
    "cgu-policy2-common-nto-sub-policy",
    "cgu-policy3-common-ptp-sub-policy",
    "cgu-policy4-common-sriov-sub-policy"
  ],
  "precaching": {
    "spec": {}
  },
  "remediationPlan": [
    [
      "spoke1",
      "spoke2"
    ],
    [
      "spoke5",
      "spoke6"
    ]
  ],
  "status": {}
}</pre>

										</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO85-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												The <code class="literal">spec.enable</code> field in the <code class="literal">ClusterGroupUpgrade</code> CR is set to <code class="literal">false</code>.
											</div></dd></dl></div></li><li class="listitem"><p class="simpara">
										Check the status of the policies by running the following command:
									</p><pre class="programlisting language-terminal">$ oc get policies -A</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">NAMESPACE   NAME                                                 REMEDIATION ACTION   COMPLIANCE STATE   AGE
default     cgu-policy1-common-cluster-version-policy            enforce                                 17m <span id="CO86-1"><!--Empty--></span><span class="callout">1</span>
default     cgu-policy2-common-nto-sub-policy                    enforce                                 17m
default     cgu-policy3-common-ptp-sub-policy                    enforce                                 17m
default     cgu-policy4-common-sriov-sub-policy                  enforce                                 17m
default     policy1-common-cluster-version-policy                inform               NonCompliant       15h
default     policy2-common-nto-sub-policy                        inform               NonCompliant       15h
default     policy3-common-ptp-sub-policy                        inform               NonCompliant       18m
default     policy4-common-sriov-sub-policy                      inform               NonCompliant       18m</pre>

										</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO86-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												The <code class="literal">spec.remediationAction</code> field of policies currently applied on the clusters is set to <code class="literal">enforce</code>. The managed policies in <code class="literal">inform</code> mode from the <code class="literal">ClusterGroupUpgrade</code> CR remain in <code class="literal">inform</code> mode during the update.
											</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
								Change the value of the <code class="literal">spec.enable</code> field to <code class="literal">true</code> by running the following command:
							</p><pre class="programlisting language-terminal">$ oc --namespace=default patch clustergroupupgrade.ran.openshift.io/cgu-1 \
--patch '{"spec":{"enable":true}}' --type=merge</pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Check the status of the update again by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get cgu -n default cgu-1 -ojsonpath='{.status}' | jq</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-json">{
  "computedMaxConcurrency": 2,
  "conditions": [ <span id="CO87-1"><!--Empty--></span><span class="callout">1</span>
    {
      "lastTransitionTime": "2022-02-25T15:33:07Z",
      "message": "All selected clusters are valid",
      "reason": "ClusterSelectionCompleted",
      "status": "True",
      "type": "ClustersSelected",
      "lastTransitionTime": "2022-02-25T15:33:07Z",
      "message": "Completed validation",
      "reason": "ValidationCompleted",
      "status": "True",
      "type": "Validated",
      "lastTransitionTime": "2022-02-25T15:34:07Z",
      "message": "Remediating non-compliant policies",
      "reason": "InProgress",
      "status": "True",
      "type": "Progressing"
    }
  ],
  "copiedPolicies": [
    "cgu-policy1-common-cluster-version-policy",
    "cgu-policy2-common-nto-sub-policy",
    "cgu-policy3-common-ptp-sub-policy",
    "cgu-policy4-common-sriov-sub-policy"
  ],
  "managedPoliciesContent": {
    "policy1-common-cluster-version-policy": "null",
    "policy2-common-nto-sub-policy": "[{\"kind\":\"Subscription\",\"name\":\"node-tuning-operator\",\"namespace\":\"openshift-cluster-node-tuning-operator\"}]",
    "policy3-common-ptp-sub-policy": "[{\"kind\":\"Subscription\",\"name\":\"ptp-operator-subscription\",\"namespace\":\"openshift-ptp\"}]",
    "policy4-common-sriov-sub-policy": "[{\"kind\":\"Subscription\",\"name\":\"sriov-network-operator-subscription\",\"namespace\":\"openshift-sriov-network-operator\"}]"
  },
  "managedPoliciesForUpgrade": [
    {
      "name": "policy1-common-cluster-version-policy",
      "namespace": "default"
    },
    {
      "name": "policy2-common-nto-sub-policy",
      "namespace": "default"
    },
    {
      "name": "policy3-common-ptp-sub-policy",
      "namespace": "default"
    },
    {
      "name": "policy4-common-sriov-sub-policy",
      "namespace": "default"
    }
  ],
  "managedPoliciesNs": {
    "policy1-common-cluster-version-policy": "default",
    "policy2-common-nto-sub-policy": "default",
    "policy3-common-ptp-sub-policy": "default",
    "policy4-common-sriov-sub-policy": "default"
  },
  "placementBindings": [
    "cgu-policy1-common-cluster-version-policy",
    "cgu-policy2-common-nto-sub-policy",
    "cgu-policy3-common-ptp-sub-policy",
    "cgu-policy4-common-sriov-sub-policy"
  ],
  "placementRules": [
    "cgu-policy1-common-cluster-version-policy",
    "cgu-policy2-common-nto-sub-policy",
    "cgu-policy3-common-ptp-sub-policy",
    "cgu-policy4-common-sriov-sub-policy"
  ],
  "precaching": {
    "spec": {}
  },
  "remediationPlan": [
    [
      "spoke1",
      "spoke2"
    ],
    [
      "spoke5",
      "spoke6"
    ]
  ],
  "status": {
    "currentBatch": 1,
    "currentBatchStartedAt": "2022-02-25T15:54:16Z",
    "remediationPlanForBatch": {
      "spoke1": 0,
      "spoke2": 1
    },
    "startedAt": "2022-02-25T15:54:16Z"
  }
}</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO87-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Reflects the update progress of the current batch. Run this command again to receive updated information about the progress.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								If the policies include Operator subscriptions, you can check the installation progress directly on the single-node cluster.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Export the <code class="literal">KUBECONFIG</code> file of the single-node cluster you want to check the installation progress for by running the following command:
									</p><pre class="programlisting language-terminal">$ export KUBECONFIG=&lt;cluster_kubeconfig_absolute_path&gt;</pre></li><li class="listitem"><p class="simpara">
										Check all the subscriptions present on the single-node cluster and look for the one in the policy you are trying to install through the <code class="literal">ClusterGroupUpgrade</code> CR by running the following command:
									</p><pre class="programlisting language-terminal">$ oc get subs -A | grep -i &lt;subscription_name&gt;</pre><div class="formalpara"><p class="title"><strong>Example output for <code class="literal">cluster-logging</code> policy</strong></p><p>
											
<pre class="programlisting language-terminal">NAMESPACE                              NAME                         PACKAGE                      SOURCE             CHANNEL
openshift-logging                      cluster-logging              cluster-logging              redhat-operators   stable</pre>

										</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
								If one of the managed policies includes a <code class="literal">ClusterVersion</code> CR, check the status of platform updates in the current batch by running the following command against the spoke cluster:
							</p><pre class="programlisting language-terminal">$ oc get clusterversion</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
version   4.9.5     True        True          43s     Working towards 4.9.7: 71 of 735 done (9% complete)</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Check the Operator subscription by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get subs -n &lt;operator-namespace&gt; &lt;operator-subscription&gt; -ojsonpath="{.status}"</pre></li><li class="listitem"><p class="simpara">
								Check the install plans present on the single-node cluster that is associated with the desired subscription by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get installplan -n &lt;subscription_namespace&gt;</pre><div class="formalpara"><p class="title"><strong>Example output for <code class="literal">cluster-logging</code> Operator</strong></p><p>
									
<pre class="programlisting language-terminal">NAMESPACE                              NAME            CSV                                 APPROVAL   APPROVED
openshift-logging                      install-6khtw   cluster-logging.5.3.3-4             Manual     true <span id="CO88-1"><!--Empty--></span><span class="callout">1</span></pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO88-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The install plans have their <code class="literal">Approval</code> field set to <code class="literal">Manual</code> and their <code class="literal">Approved</code> field changes from <code class="literal">false</code> to <code class="literal">true</code> after TALM approves the install plan.
									</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									When TALM is remediating a policy containing a subscription, it automatically approves any install plans attached to that subscription. Where multiple install plans are needed to get the operator to the latest known version, TALM might approve multiple install plans, upgrading through one or more intermediate versions to get to the final version.
								</p></div></div></li><li class="listitem"><p class="simpara">
								Check if the cluster service version for the Operator of the policy that the <code class="literal">ClusterGroupUpgrade</code> is installing reached the <code class="literal">Succeeded</code> phase by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get csv -n &lt;operator_namespace&gt;</pre><div class="formalpara"><p class="title"><strong>Example output for OpenShift Logging Operator</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                    DISPLAY                     VERSION   REPLACES   PHASE
cluster-logging.5.4.2   Red Hat OpenShift Logging   5.4.2                Succeeded</pre>

								</p></div></li></ol></div></section></section><section class="section" id="talo-backup-feature-concept_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h3 class="title">16.10.7. Creating a backup of cluster resources before upgrade</h3></div></div></div><p>
					For single-node OpenShift, the Topology Aware Lifecycle Manager (TALM) can create a backup of a deployment before an upgrade. If the upgrade fails, you can recover the previous version and restore a cluster to a working state without requiring a reprovision of applications.
				</p><p>
					To use the backup feature you first create a <code class="literal">ClusterGroupUpgrade</code> CR with the <code class="literal">backup</code> field set to <code class="literal">true</code>. To ensure that the contents of the backup are up to date, the backup is not taken until you set the <code class="literal">enable</code> field in the <code class="literal">ClusterGroupUpgrade</code> CR to <code class="literal">true</code>.
				</p><p>
					TALM uses the <code class="literal">BackupSucceeded</code> condition to report the status and reasons as follows:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							<code class="literal">true</code>
						</p><p class="simpara">
							Backup is completed for all clusters or the backup run has completed but failed for one or more clusters. If backup fails for any cluster, the update does not proceed for that cluster.
						</p></li><li class="listitem"><p class="simpara">
							<code class="literal">false</code>
						</p><p class="simpara">
							Backup is still in progress for one or more clusters or has failed for all clusters. The backup process running in the spoke clusters can have the following statuses:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p class="simpara">
									<code class="literal">PreparingToStart</code>
								</p><p class="simpara">
									The first reconciliation pass is in progress. The TALM deletes any spoke backup namespace and hub view resources that have been created in a failed upgrade attempt.
								</p></li><li class="listitem"><p class="simpara">
									<code class="literal">Starting</code>
								</p><p class="simpara">
									The backup prerequisites and backup job are being created.
								</p></li><li class="listitem"><p class="simpara">
									<code class="literal">Active</code>
								</p><p class="simpara">
									The backup is in progress.
								</p></li><li class="listitem"><p class="simpara">
									<code class="literal">Succeeded</code>
								</p><p class="simpara">
									The backup succeeded.
								</p></li><li class="listitem"><p class="simpara">
									<code class="literal">BackupTimeout</code>
								</p><p class="simpara">
									Artifact backup is partially done.
								</p></li><li class="listitem"><p class="simpara">
									<code class="literal">UnrecoverableError</code>
								</p><p class="simpara">
									The backup has ended with a non-zero exit code.
								</p></li></ul></div></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If the backup of a cluster fails and enters the <code class="literal">BackupTimeout</code> or <code class="literal">UnrecoverableError</code> state, the cluster update does not proceed for that cluster. Updates to other clusters are not affected and continue.
					</p></div></div><section class="section" id="talo-backup-start_and_update_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h4 class="title">16.10.7.1. Creating a ClusterGroupUpgrade CR with backup</h4></div></div></div><p>
						You can create a backup of a deployment before an upgrade on single-node OpenShift clusters. If the upgrade fails you can use the <code class="literal">upgrade-recovery.sh</code> script generated by Topology Aware Lifecycle Manager (TALM) to return the system to its preupgrade state. The backup consists of the following items:
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Cluster backup</span></dt><dd>
									A snapshot of <code class="literal">etcd</code> and static pod manifests.
								</dd><dt><span class="term">Content backup</span></dt><dd>
									Backups of folders, for example, <code class="literal">/etc</code>, <code class="literal">/usr/local</code>, <code class="literal">/var/lib/kubelet</code>.
								</dd><dt><span class="term">Changed files backup</span></dt><dd>
									Any files managed by <code class="literal">machine-config</code> that have been changed.
								</dd><dt><span class="term">Deployment</span></dt><dd>
									A pinned <code class="literal">ostree</code> deployment.
								</dd><dt><span class="term">Images (Optional)</span></dt><dd>
									Any container images that are in use.
								</dd></dl></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Install the Topology Aware Lifecycle Manager (TALM).
							</li><li class="listitem">
								Provision one or more managed clusters.
							</li><li class="listitem">
								Log in as a user with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								Install Red Hat Advanced Cluster Management (RHACM).
							</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							It is highly recommended that you create a recovery partition. The following is an example <code class="literal">SiteConfig</code> custom resource (CR) for a recovery partition of 50 GB:
						</p><pre class="programlisting language-yaml">nodes:
    - hostName: "snonode.sno-worker-0.e2e.bos.redhat.com"
    role: "master"
    rootDeviceHints:
        hctl: "0:2:0:0"
        deviceName: /dev/disk/by-id/scsi-3600508b400105e210000900000490000
...
    #Disk /dev/disk/by-id/scsi-3600508b400105e210000900000490000:
    #893.3 GiB, 959119884288 bytes, 1873281024 sectors
    diskPartition:
        - device: /dev/disk/by-id/scsi-3600508b400105e210000900000490000
        partitions:
        - mount_point: /var/recovery
            size: 51200
            start: 800000</pre></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Save the contents of the <code class="literal">ClusterGroupUpgrade</code> CR with the <code class="literal">backup</code> and <code class="literal">enable</code> fields set to <code class="literal">true</code> in the <code class="literal">clustergroupupgrades-group-du.yaml</code> file:
							</p><pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: du-upgrade-4918
  namespace: ztp-group-du-sno
spec:
  preCaching: true
  backup: true
  clusters:
  - cnfdb1
  - cnfdb2
  enable: true
  managedPolicies:
  - du-upgrade-platform-upgrade
  remediationStrategy:
    maxConcurrency: 2
    timeout: 240</pre></li><li class="listitem"><p class="simpara">
								To start the update, apply the <code class="literal">ClusterGroupUpgrade</code> CR by running the following command:
							</p><pre class="programlisting language-terminal">$ oc apply -f clustergroupupgrades-group-du.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Check the status of the upgrade in the hub cluster by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get cgu -n ztp-group-du-sno du-upgrade-4918 -o jsonpath='{.status}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-json">{
    "backup": {
        "clusters": [
            "cnfdb2",
            "cnfdb1"
    ],
    "status": {
        "cnfdb1": "Succeeded",
        "cnfdb2": "Failed" <span id="CO89-1"><!--Empty--></span><span class="callout">1</span>
    }
},
"computedMaxConcurrency": 1,
"conditions": [
    {
        "lastTransitionTime": "2022-04-05T10:37:19Z",
        "message": "Backup failed for 1 cluster", <span id="CO89-2"><!--Empty--></span><span class="callout">2</span>
        "reason": "PartiallyDone", <span id="CO89-3"><!--Empty--></span><span class="callout">3</span>
        "status": "True", <span id="CO89-4"><!--Empty--></span><span class="callout">4</span>
        "type": "Succeeded"
    }
],
"precaching": {
    "spec": {}
},
"status": {}</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO89-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Backup has failed for one cluster.
									</div></dd><dt><a href="#CO89-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The message confirms that the backup failed for one cluster.
									</div></dd><dt><a href="#CO89-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The backup was partially successful.
									</div></dd><dt><a href="#CO89-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										The backup process has finished.
									</div></dd></dl></div></li></ul></div></section><section class="section" id="talo-backup-recovery_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h4 class="title">16.10.7.2. Recovering a cluster after a failed upgrade</h4></div></div></div><p>
						If an upgrade of a cluster fails, you can manually log in to the cluster and use the backup to return the cluster to its preupgrade state. There are two stages:
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Rollback</span></dt><dd>
									If the attempted upgrade included a change to the platform OS deployment, you must roll back to the previous version before running the recovery script.
								</dd></dl></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							A rollback is only applicable to upgrades from TALM and single-node OpenShift. This process does not apply to rollbacks from any other upgrade type.
						</p></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">Recovery</span></dt><dd>
									The recovery shuts down containers and uses files from the backup partition to relaunch containers and restore clusters.
								</dd></dl></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Install the Topology Aware Lifecycle Manager (TALM).
							</li><li class="listitem">
								Provision one or more managed clusters.
							</li><li class="listitem">
								Install Red Hat Advanced Cluster Management (RHACM).
							</li><li class="listitem">
								Log in as a user with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								Run an upgrade that is configured for backup.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Delete the previously created <code class="literal">ClusterGroupUpgrade</code> custom resource (CR) by running the following command:
							</p><pre class="programlisting language-terminal">$ oc delete cgu/du-upgrade-4918 -n ztp-group-du-sno</pre></li><li class="listitem">
								Log in to the cluster that you want to recover.
							</li><li class="listitem"><p class="simpara">
								Check the status of the platform OS deployment by running the following command:
							</p><pre class="programlisting language-terminal">$ ostree admin status</pre><div class="formalpara"><p class="title"><strong>Example outputs</strong></p><p>
									
<pre class="programlisting language-terminal">[root@lab-test-spoke2-node-0 core]# ostree admin status
* rhcos c038a8f08458bbed83a77ece033ad3c55597e3f64edad66ea12fda18cbdceaf9.0
    Version: 49.84.202202230006-0
    Pinned: yes <span id="CO90-1"><!--Empty--></span><span class="callout">1</span>
    origin refspec: c038a8f08458bbed83a77ece033ad3c55597e3f64edad66ea12fda18cbdceaf9</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO90-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The current deployment is pinned. A platform OS deployment rollback is not necessary.
									</div></dd></dl></div><pre class="programlisting language-terminal">[root@lab-test-spoke2-node-0 core]# ostree admin status
* rhcos f750ff26f2d5550930ccbe17af61af47daafc8018cd9944f2a3a6269af26b0fa.0
    Version: 410.84.202204050541-0
    origin refspec: f750ff26f2d5550930ccbe17af61af47daafc8018cd9944f2a3a6269af26b0fa
rhcos ad8f159f9dc4ea7e773fd9604c9a16be0fe9b266ae800ac8470f63abc39b52ca.0 (rollback) <span id="CO91-1"><!--Empty--></span><span class="callout">1</span>
    Version: 410.84.202203290245-0
    Pinned: yes <span id="CO91-2"><!--Empty--></span><span class="callout">2</span>
    origin refspec: ad8f159f9dc4ea7e773fd9604c9a16be0fe9b266ae800ac8470f63abc39b52ca</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO91-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										This platform OS deployment is marked for rollback.
									</div></dd><dt><a href="#CO91-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The previous deployment is pinned and can be rolled back.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								To trigger a rollback of the platform OS deployment, run the following command:
							</p><pre class="programlisting language-terminal">$ rpm-ostree rollback -r</pre></li><li class="listitem"><p class="simpara">
								The first phase of the recovery shuts down containers and restores files from the backup partition to the targeted directories. To begin the recovery, run the following command:
							</p><pre class="programlisting language-terminal">$ /var/recovery/upgrade-recovery.sh</pre></li><li class="listitem"><p class="simpara">
								When prompted, reboot the cluster by running the following command:
							</p><pre class="programlisting language-terminal">$ systemctl reboot</pre></li><li class="listitem"><p class="simpara">
								After the reboot, restart the recovery by running the following command:
							</p><pre class="programlisting language-terminal">$ /var/recovery/upgrade-recovery.sh  --resume</pre></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If the recovery utility fails, you can retry with the <code class="literal">--restart</code> option:
						</p><pre class="programlisting language-terminal">$ /var/recovery/upgrade-recovery.sh --restart</pre></div></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								To check the status of the recovery run the following command:
							</p><pre class="programlisting language-terminal">$ oc get clusterversion,nodes,clusteroperator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                                         VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
clusterversion.config.openshift.io/version   4.9.23    True        False         86d     Cluster version is 4.9.23 <span id="CO92-1"><!--Empty--></span><span class="callout">1</span>


NAME                          STATUS   ROLES           AGE   VERSION
node/lab-test-spoke1-node-0   Ready    master,worker   86d   v1.22.3+b93fd35 <span id="CO92-2"><!--Empty--></span><span class="callout">2</span>

NAME                                                                           VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
clusteroperator.config.openshift.io/authentication                             4.9.23    True        False         False      2d7h    <span id="CO92-3"><!--Empty--></span><span class="callout">3</span>
clusteroperator.config.openshift.io/baremetal                                  4.9.23    True        False         False      86d


..............</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO92-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The cluster version is available and has the correct version.
									</div></dd><dt><a href="#CO92-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The node status is <code class="literal">Ready</code>.
									</div></dd><dt><a href="#CO92-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The <code class="literal">ClusterOperator</code> object’s availability is <code class="literal">True</code>.
									</div></dd></dl></div></li></ul></div></section></section><section class="section" id="talo-precache-feature-concept_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h3 class="title">16.10.8. Using the container image pre-cache feature</h3></div></div></div><p>
					Single-node OpenShift clusters might have limited bandwidth to access the container image registry, which can cause a timeout before the updates are completed.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The time of the update is not set by TALM. You can apply the <code class="literal">ClusterGroupUpgrade</code> CR at the beginning of the update by manual application or by external automation.
					</p></div></div><p>
					The container image pre-caching starts when the <code class="literal">preCaching</code> field is set to <code class="literal">true</code> in the <code class="literal">ClusterGroupUpgrade</code> CR.
				</p><p>
					TALM uses the <code class="literal">PrecacheSpecValid</code> condition to report status information as follows:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							<code class="literal">true</code>
						</p><p class="simpara">
							The pre-caching spec is valid and consistent.
						</p></li><li class="listitem"><p class="simpara">
							<code class="literal">false</code>
						</p><p class="simpara">
							The pre-caching spec is incomplete.
						</p></li></ul></div><p>
					TALM uses the <code class="literal">PrecachingSucceeded</code> condition to report status information as follows:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							<code class="literal">true</code>
						</p><p class="simpara">
							TALM has concluded the pre-caching process. If pre-caching fails for any cluster, the update fails for that cluster but proceeds for all other clusters. A message informs you if pre-caching has failed for any clusters.
						</p></li><li class="listitem"><p class="simpara">
							<code class="literal">false</code>
						</p><p class="simpara">
							Pre-caching is still in progress for one or more clusters or has failed for all clusters.
						</p></li></ul></div><p>
					After a successful pre-caching process, you can start remediating policies. The remediation actions start when the <code class="literal">enable</code> field is set to <code class="literal">true</code>. If there is a pre-caching failure on a cluster, the upgrade fails for that cluster. The upgrade process continues for all other clusters that have a successful pre-cache.
				</p><p>
					The pre-caching process can be in the following statuses:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							<code class="literal">NotStarted</code>
						</p><p class="simpara">
							This is the initial state all clusters are automatically assigned to on the first reconciliation pass of the <code class="literal">ClusterGroupUpgrade</code> CR. In this state, TALM deletes any pre-caching namespace and hub view resources of spoke clusters that remain from previous incomplete updates. TALM then creates a new <code class="literal">ManagedClusterView</code> resource for the spoke pre-caching namespace to verify its deletion in the <code class="literal">PrecachePreparing</code> state.
						</p></li><li class="listitem"><p class="simpara">
							<code class="literal">PreparingToStart</code>
						</p><p class="simpara">
							Cleaning up any remaining resources from previous incomplete updates is in progress.
						</p></li><li class="listitem"><p class="simpara">
							<code class="literal">Starting</code>
						</p><p class="simpara">
							Pre-caching job prerequisites and the job are created.
						</p></li><li class="listitem"><p class="simpara">
							<code class="literal">Active</code>
						</p><p class="simpara">
							The job is in "Active" state.
						</p></li><li class="listitem"><p class="simpara">
							<code class="literal">Succeeded</code>
						</p><p class="simpara">
							The pre-cache job succeeded.
						</p></li><li class="listitem"><p class="simpara">
							<code class="literal">PrecacheTimeout</code>
						</p><p class="simpara">
							The artifact pre-caching is partially done.
						</p></li><li class="listitem"><p class="simpara">
							<code class="literal">UnrecoverableError</code>
						</p><p class="simpara">
							The job ends with a non-zero exit code.
						</p></li></ul></div><section class="section" id="talo-precache-feature-image-filter_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h4 class="title">16.10.8.1. Using the container image pre-cache filter</h4></div></div></div><p>
						The pre-cache feature typically downloads more images than a cluster needs for an update. You can control which pre-cache images are downloaded to a cluster. This decreases download time, and saves bandwidth and storage.
					</p><p>
						You can see a list of all images to be downloaded using the following command:
					</p><pre class="programlisting language-terminal">$ oc adm release info &lt;ocp-version&gt;</pre><p>
						The following <code class="literal">ConfigMap</code> example shows how you can exclude images using the <code class="literal">excludePrecachePatterns</code> field.
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-group-upgrade-overrides
data:
  excludePrecachePatterns: |
    azure <span id="CO93-1"><!--Empty--></span><span class="callout">1</span>
    aws
    vsphere
    alibaba</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO93-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								TALM excludes all images with names that include any of the patterns listed here.
							</div></dd></dl></div></section><section class="section" id="talo-precache-start_and_update_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h4 class="title">16.10.8.2. Creating a ClusterGroupUpgrade CR with pre-caching</h4></div></div></div><p>
						For single-node OpenShift, the pre-cache feature allows the required container images to be present on the spoke cluster before the update starts.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							For pre-caching, TALM uses the <code class="literal">spec.remediationStrategy.timeout</code> value from the <code class="literal">ClusterGroupUpgrade</code> CR. You must set a <code class="literal">timeout</code> value that allows sufficient time for the pre-caching job to complete. When you enable the <code class="literal">ClusterGroupUpgrade</code> CR after pre-caching has completed, you can change the <code class="literal">timeout</code> value to a duration that is appropriate for the update.
						</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Install the Topology Aware Lifecycle Manager (TALM).
							</li><li class="listitem">
								Provision one or more managed clusters.
							</li><li class="listitem">
								Log in as a user with <code class="literal">cluster-admin</code> privileges.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Save the contents of the <code class="literal">ClusterGroupUpgrade</code> CR with the <code class="literal">preCaching</code> field set to <code class="literal">true</code> in the <code class="literal">clustergroupupgrades-group-du.yaml</code> file:
							</p><pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: du-upgrade-4918
  namespace: ztp-group-du-sno
spec:
  preCaching: true <span id="CO94-1"><!--Empty--></span><span class="callout">1</span>
  clusters:
  - cnfdb1
  - cnfdb2
  enable: false
  managedPolicies:
  - du-upgrade-platform-upgrade
  remediationStrategy:
    maxConcurrency: 2
    timeout: 240</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO94-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The <code class="literal">preCaching</code> field is set to <code class="literal">true</code>, which enables TALM to pull the container images before starting the update.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								When you want to start pre-caching, apply the <code class="literal">ClusterGroupUpgrade</code> CR by running the following command:
							</p><pre class="programlisting language-terminal">$ oc apply -f clustergroupupgrades-group-du.yaml</pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Check if the <code class="literal">ClusterGroupUpgrade</code> CR exists in the hub cluster by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get cgu -A</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAMESPACE          NAME              AGE   STATE        DETAILS
ztp-group-du-sno   du-upgrade-4918   10s   InProgress   Precaching is required and not done <span id="CO95-1"><!--Empty--></span><span class="callout">1</span></pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO95-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The CR is created.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Check the status of the pre-caching task by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get cgu -n ztp-group-du-sno du-upgrade-4918 -o jsonpath='{.status}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-json">{
  "conditions": [
    {
      "lastTransitionTime": "2022-01-27T19:07:24Z",
      "message": "Precaching is required and not done",
      "reason": "InProgress",
      "status": "False",
      "type": "PrecachingSucceeded"
    },
    {
      "lastTransitionTime": "2022-01-27T19:07:34Z",
      "message": "Pre-caching spec is valid and consistent",
      "reason": "PrecacheSpecIsWellFormed",
      "status": "True",
      "type": "PrecacheSpecValid"
    }
  ],
  "precaching": {
    "clusters": [
      "cnfdb1" <span id="CO96-1"><!--Empty--></span><span class="callout">1</span>
      "cnfdb2"
    ],
    "spec": {
      "platformImage": "image.example.io"},
    "status": {
      "cnfdb1": "Active"
      "cnfdb2": "Succeeded"}
    }
}</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO96-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Displays the list of identified clusters.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Check the status of the pre-caching job by running the following command on the spoke cluster:
							</p><pre class="programlisting language-terminal">$ oc get jobs,pods -n openshift-talo-pre-cache</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                  COMPLETIONS   DURATION   AGE
job.batch/pre-cache   0/1           3m10s      3m10s

NAME                     READY   STATUS    RESTARTS   AGE
pod/pre-cache--1-9bmlr   1/1     Running   0          3m10s</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Check the status of the <code class="literal">ClusterGroupUpgrade</code> CR by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get cgu -n ztp-group-du-sno du-upgrade-4918 -o jsonpath='{.status}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-json">"conditions": [
    {
      "lastTransitionTime": "2022-01-27T19:30:41Z",
      "message": "The ClusterGroupUpgrade CR has all clusters compliant with all the managed policies",
      "reason": "UpgradeCompleted",
      "status": "True",
      "type": "Ready"
    },
    {
      "lastTransitionTime": "2022-01-27T19:28:57Z",
      "message": "Precaching is completed",
      "reason": "PrecachingCompleted",
      "status": "True",
      "type": "PrecachingSucceeded" <span id="CO97-1"><!--Empty--></span><span class="callout">1</span>
    }</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO97-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The pre-cache tasks are done.
									</div></dd></dl></div></li></ol></div></section></section><section class="section" id="talo-troubleshooting_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h3 class="title">16.10.9. Troubleshooting the Topology Aware Lifecycle Manager</h3></div></div></div><p>
					The Topology Aware Lifecycle Manager (TALM) is an OpenShift Container Platform Operator that remediates RHACM policies. When issues occur, use the <code class="literal">oc adm must-gather</code> command to gather details and logs and to take steps in debugging the issues.
				</p><p>
					For more information about related topics, see the following documentation:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/articles/6218901">Red Hat Advanced Cluster Management for Kubernetes 2.4 Support Matrix</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.0/html/troubleshooting/troubleshooting">Red Hat Advanced Cluster Management Troubleshooting</a>
						</li><li class="listitem">
							The "Troubleshooting Operator issues" section
						</li></ul></div><section class="section" id="talo-general-troubleshooting_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h4 class="title">16.10.9.1. General troubleshooting</h4></div></div></div><p>
						You can determine the cause of the problem by reviewing the following questions:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Is the configuration that you are applying supported?
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										Are the RHACM and the OpenShift Container Platform versions compatible?
									</li><li class="listitem">
										Are the TALM and RHACM versions compatible?
									</li></ul></div></li><li class="listitem"><p class="simpara">
								Which of the following components is causing the problem?
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										<a class="xref" href="#talo-troubleshooting-managed-policies_cnf-topology-aware-lifecycle-manager" title="16.10.9.3. Managed policies">Section 16.10.9.3, “Managed policies”</a>
									</li><li class="listitem">
										<a class="xref" href="#talo-troubleshooting-clusters_cnf-topology-aware-lifecycle-manager" title="16.10.9.4. Clusters">Section 16.10.9.4, “Clusters”</a>
									</li><li class="listitem">
										<a class="xref" href="#talo-troubleshooting-remediation-strategy_cnf-topology-aware-lifecycle-manager" title="16.10.9.5. Remediation Strategy">Section 16.10.9.5, “Remediation Strategy”</a>
									</li><li class="listitem">
										<a class="xref" href="#talo-troubleshooting-remediation-talo_cnf-topology-aware-lifecycle-manager" title="16.10.9.6. Topology Aware Lifecycle Manager">Section 16.10.9.6, “Topology Aware Lifecycle Manager”</a>
									</li></ul></div></li></ul></div><p>
						To ensure that the <code class="literal">ClusterGroupUpgrade</code> configuration is functional, you can do the following:
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								Create the <code class="literal">ClusterGroupUpgrade</code> CR with the <code class="literal">spec.enable</code> field set to <code class="literal">false</code>.
							</li><li class="listitem">
								Wait for the status to be updated and go through the troubleshooting questions.
							</li><li class="listitem">
								If everything looks as expected, set the <code class="literal">spec.enable</code> field to <code class="literal">true</code> in the <code class="literal">ClusterGroupUpgrade</code> CR.
							</li></ol></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
							After you set the <code class="literal">spec.enable</code> field to <code class="literal">true</code> in the <code class="literal">ClusterUpgradeGroup</code> CR, the update procedure starts and you cannot edit the CR’s <code class="literal">spec</code> fields anymore.
						</p></div></div></section><section class="section" id="talo-troubleshooting-modify-cgu_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h4 class="title">16.10.9.2. Cannot modify the ClusterUpgradeGroup CR</h4></div></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">Issue</span></dt><dd>
									You cannot edit the <code class="literal">ClusterUpgradeGroup</code> CR after enabling the update.
								</dd><dt><span class="term">Resolution</span></dt><dd><p class="simpara">
									Restart the procedure by performing the following steps:
								</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
											Remove the old <code class="literal">ClusterGroupUpgrade</code> CR by running the following command:
										</p><pre class="programlisting language-terminal">$ oc delete cgu -n &lt;ClusterGroupUpgradeCR_namespace&gt; &lt;ClusterGroupUpgradeCR_name&gt;</pre></li><li class="listitem"><p class="simpara">
											Check and fix the existing issues with the managed clusters and policies.
										</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
													Ensure that all the clusters are managed clusters and available.
												</li><li class="listitem">
													Ensure that all the policies exist and have the <code class="literal">spec.remediationAction</code> field set to <code class="literal">inform</code>.
												</li></ol></div></li><li class="listitem"><p class="simpara">
											Create a new <code class="literal">ClusterGroupUpgrade</code> CR with the correct configurations.
										</p><pre class="programlisting language-terminal">$ oc apply -f &lt;ClusterGroupUpgradeCR_YAML&gt;</pre></li></ol></div></dd></dl></div></section><section class="section" id="talo-troubleshooting-managed-policies_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h4 class="title">16.10.9.3. Managed policies</h4></div></div></div><h5 id="checking-managed-policies-on-the-system">Checking managed policies on the system</h5><div class="variablelist"><dl class="variablelist"><dt><span class="term">Issue</span></dt><dd>
									You want to check if you have the correct managed policies on the system.
								</dd><dt><span class="term">Resolution</span></dt><dd><p class="simpara">
									Run the following command:
								</p><pre class="programlisting language-terminal">$ oc get cgu lab-upgrade -ojsonpath='{.spec.managedPolicies}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-json">["group-du-sno-validator-du-validator-policy", "policy2-common-nto-sub-policy", "policy3-common-ptp-sub-policy"]</pre>

									</p></div></dd></dl></div><h5 id="checking-remediationaction-mode">Checking remediationAction mode</h5><div class="variablelist"><dl class="variablelist"><dt><span class="term">Issue</span></dt><dd>
									You want to check if the <code class="literal">remediationAction</code> field is set to <code class="literal">inform</code> in the <code class="literal">spec</code> of the managed policies.
								</dd><dt><span class="term">Resolution</span></dt><dd><p class="simpara">
									Run the following command:
								</p><pre class="programlisting language-terminal">$ oc get policies --all-namespaces</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAMESPACE   NAME                                                 REMEDIATION ACTION   COMPLIANCE STATE   AGE
default     policy1-common-cluster-version-policy                inform               NonCompliant       5d21h
default     policy2-common-nto-sub-policy                        inform               Compliant          5d21h
default     policy3-common-ptp-sub-policy                        inform               NonCompliant       5d21h
default     policy4-common-sriov-sub-policy                      inform               NonCompliant       5d21h</pre>

									</p></div></dd></dl></div><h5 id="checking-policy-compliance-state">Checking policy compliance state</h5><div class="variablelist"><dl class="variablelist"><dt><span class="term">Issue</span></dt><dd>
									You want to check the compliance state of policies.
								</dd><dt><span class="term">Resolution</span></dt><dd><p class="simpara">
									Run the following command:
								</p><pre class="programlisting language-terminal">$ oc get policies --all-namespaces</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAMESPACE   NAME                                                 REMEDIATION ACTION   COMPLIANCE STATE   AGE
default     policy1-common-cluster-version-policy                inform               NonCompliant       5d21h
default     policy2-common-nto-sub-policy                        inform               Compliant          5d21h
default     policy3-common-ptp-sub-policy                        inform               NonCompliant       5d21h
default     policy4-common-sriov-sub-policy                      inform               NonCompliant       5d21h</pre>

									</p></div></dd></dl></div></section><section class="section" id="talo-troubleshooting-clusters_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h4 class="title">16.10.9.4. Clusters</h4></div></div></div><h6 id="checking-if-managed-clusters-are-present">Checking if managed clusters are present</h6><div class="variablelist"><dl class="variablelist"><dt><span class="term">Issue</span></dt><dd>
									You want to check if the clusters in the <code class="literal">ClusterGroupUpgrade</code> CR are managed clusters.
								</dd><dt><span class="term">Resolution</span></dt><dd><p class="simpara">
									Run the following command:
								</p><pre class="programlisting language-terminal">$ oc get managedclusters</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME            HUB ACCEPTED   MANAGED CLUSTER URLS                    JOINED   AVAILABLE   AGE
local-cluster   true           https://api.hub.example.com:6443        True     Unknown     13d
spoke1          true           https://api.spoke1.example.com:6443     True     True        13d
spoke3          true           https://api.spoke3.example.com:6443     True     True        27h</pre>

									</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
											Alternatively, check the TALM manager logs:
										</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
													Get the name of the TALM manager by running the following command:
												</p><pre class="programlisting language-terminal">$ oc get pod -n openshift-operators</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
														
<pre class="programlisting language-terminal">NAME                                                         READY   STATUS    RESTARTS   AGE
cluster-group-upgrades-controller-manager-75bcc7484d-8k8xp   2/2     Running   0          45m</pre>

													</p></div></li><li class="listitem"><p class="simpara">
													Check the TALM manager logs by running the following command:
												</p><pre class="programlisting language-terminal">$ oc logs -n openshift-operators \
cluster-group-upgrades-controller-manager-75bcc7484d-8k8xp -c manager</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
														
<pre class="programlisting language-terminal">ERROR	controller-runtime.manager.controller.clustergroupupgrade	Reconciler error	{"reconciler group": "ran.openshift.io", "reconciler kind": "ClusterGroupUpgrade", "name": "lab-upgrade", "namespace": "default", "error": "Cluster spoke5555 is not a ManagedCluster"} <span id="CO98-1"><!--Empty--></span><span class="callout">1</span>
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem</pre>

													</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO98-1"><span class="callout">1</span></a> </dt><dd><div class="para">
															The error message shows that the cluster is not a managed cluster.
														</div></dd></dl></div></li></ol></div></li></ol></div></dd></dl></div><h6 id="checking-if-managed-clusters-are-available">Checking if managed clusters are available</h6><div class="variablelist"><dl class="variablelist"><dt><span class="term">Issue</span></dt><dd>
									You want to check if the managed clusters specified in the <code class="literal">ClusterGroupUpgrade</code> CR are available.
								</dd><dt><span class="term">Resolution</span></dt><dd><p class="simpara">
									Run the following command:
								</p><pre class="programlisting language-terminal">$ oc get managedclusters</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME            HUB ACCEPTED   MANAGED CLUSTER URLS                    JOINED   AVAILABLE   AGE
local-cluster   true           https://api.hub.testlab.com:6443        True     Unknown     13d
spoke1          true           https://api.spoke1.testlab.com:6443     True     True        13d <span id="CO99-1"><!--Empty--></span><span class="callout">1</span>
spoke3          true           https://api.spoke3.testlab.com:6443     True     True        27h <span id="CO99-2"><!--Empty--></span><span class="callout">2</span></pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO99-1"><span class="callout">1</span></a> <a href="#CO99-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											The value of the <code class="literal">AVAILABLE</code> field is <code class="literal">True</code> for the managed clusters.
										</div></dd></dl></div></dd></dl></div><h6 id="checking-clusterlabelselector">Checking clusterLabelSelector</h6><div class="variablelist"><dl class="variablelist"><dt><span class="term">Issue</span></dt><dd>
									You want to check if the <code class="literal">clusterLabelSelector</code> field specified in the <code class="literal">ClusterGroupUpgrade</code> CR matches at least one of the managed clusters.
								</dd><dt><span class="term">Resolution</span></dt><dd><p class="simpara">
									Run the following command:
								</p><pre class="programlisting language-terminal">$ oc get managedcluster --selector=upgrade=true <span id="CO100-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO100-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The label for the clusters you want to update is <code class="literal">upgrade:true</code>.
										</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME            HUB ACCEPTED   MANAGED CLUSTER URLS                     JOINED    AVAILABLE   AGE
spoke1          true           https://api.spoke1.testlab.com:6443      True     True        13d
spoke3          true           https://api.spoke3.testlab.com:6443      True     True        27h</pre>

									</p></div></dd></dl></div><h6 id="checking-if-canary-clusters-are-present">Checking if canary clusters are present</h6><div class="variablelist"><dl class="variablelist"><dt><span class="term">Issue</span></dt><dd><p class="simpara">
									You want to check if the canary clusters are present in the list of clusters.
								</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">ClusterGroupUpgrade</code> CR</strong></p><p>
										
<pre class="programlisting language-yaml">spec:
    remediationStrategy:
        canaries:
        - spoke3
        maxConcurrency: 2
        timeout: 240
    clusterLabelSelectors:
      - matchLabels:
          upgrade: true</pre>

									</p></div></dd><dt><span class="term">Resolution</span></dt><dd><p class="simpara">
									Run the following commands:
								</p><pre class="programlisting language-terminal">$ oc get cgu lab-upgrade -ojsonpath='{.spec.clusters}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-json">["spoke1", "spoke3"]</pre>

									</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
											Check if the canary clusters are present in the list of clusters that match <code class="literal">clusterLabelSelector</code> labels by running the following command:
										</p><pre class="programlisting language-terminal">$ oc get managedcluster --selector=upgrade=true</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
												
<pre class="programlisting language-terminal">NAME            HUB ACCEPTED   MANAGED CLUSTER URLS   JOINED    AVAILABLE   AGE
spoke1          true           https://api.spoke1.testlab.com:6443   True     True        13d
spoke3          true           https://api.spoke3.testlab.com:6443   True     True        27h</pre>

											</p></div></li></ol></div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							A cluster can be present in <code class="literal">spec.clusters</code> and also be matched by the <code class="literal">spec.clusterLabelSelector</code> label.
						</p></div></div><h6 id="checking-the-pre-caching-status-on-spoke-clusters">Checking the pre-caching status on spoke clusters</h6><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Check the status of pre-caching by running the following command on the spoke cluster:
							</p><pre class="programlisting language-terminal">$ oc get jobs,pods -n openshift-talo-pre-cache</pre></li></ol></div></section><section class="section" id="talo-troubleshooting-remediation-strategy_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h4 class="title">16.10.9.5. Remediation Strategy</h4></div></div></div><h6 id="checking-if-remediationstrategy-is-present-in-the-clustergroupupgrade-cr">Checking if remediationStrategy is present in the ClusterGroupUpgrade CR</h6><div class="variablelist"><dl class="variablelist"><dt><span class="term">Issue</span></dt><dd>
									You want to check if the <code class="literal">remediationStrategy</code> is present in the <code class="literal">ClusterGroupUpgrade</code> CR.
								</dd><dt><span class="term">Resolution</span></dt><dd><p class="simpara">
									Run the following command:
								</p><pre class="programlisting language-terminal">$ oc get cgu lab-upgrade -ojsonpath='{.spec.remediationStrategy}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-json">{"maxConcurrency":2, "timeout":240}</pre>

									</p></div></dd></dl></div><h6 id="checking-if-maxconcurrency-is-specified-in-the-clustergroupupgrade-cr">Checking if maxConcurrency is specified in the ClusterGroupUpgrade CR</h6><div class="variablelist"><dl class="variablelist"><dt><span class="term">Issue</span></dt><dd>
									You want to check if the <code class="literal">maxConcurrency</code> is specified in the <code class="literal">ClusterGroupUpgrade</code> CR.
								</dd><dt><span class="term">Resolution</span></dt><dd><p class="simpara">
									Run the following command:
								</p><pre class="programlisting language-terminal">$ oc get cgu lab-upgrade -ojsonpath='{.spec.remediationStrategy.maxConcurrency}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">2</pre>

									</p></div></dd></dl></div></section><section class="section" id="talo-troubleshooting-remediation-talo_cnf-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h4 class="title">16.10.9.6. Topology Aware Lifecycle Manager</h4></div></div></div><h6 id="checking-condition-message-and-status-in-the-clustergroupupgrade-cr">Checking condition message and status in the ClusterGroupUpgrade CR</h6><div class="variablelist"><dl class="variablelist"><dt><span class="term">Issue</span></dt><dd>
									You want to check the value of the <code class="literal">status.conditions</code> field in the <code class="literal">ClusterGroupUpgrade</code> CR.
								</dd><dt><span class="term">Resolution</span></dt><dd><p class="simpara">
									Run the following command:
								</p><pre class="programlisting language-terminal">$ oc get cgu lab-upgrade -ojsonpath='{.status.conditions}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-json">{"lastTransitionTime":"2022-02-17T22:25:28Z", "message":"Missing managed policies:[policyList]", "reason":"NotAllManagedPoliciesExist", "status":"False", "type":"Validated"}</pre>

									</p></div></dd></dl></div><h6 id="checking-corresponding-copied-policies">Checking corresponding copied policies</h6><div class="variablelist"><dl class="variablelist"><dt><span class="term">Issue</span></dt><dd>
									You want to check if every policy from <code class="literal">status.managedPoliciesForUpgrade</code> has a corresponding policy in <code class="literal">status.copiedPolicies</code>.
								</dd><dt><span class="term">Resolution</span></dt><dd><p class="simpara">
									Run the following command:
								</p><pre class="programlisting language-terminal">$ oc get cgu lab-upgrade -oyaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">status:
  …
  copiedPolicies:
  - lab-upgrade-policy3-common-ptp-sub-policy
  managedPoliciesForUpgrade:
  - name: policy3-common-ptp-sub-policy
    namespace: default</pre>

									</p></div></dd></dl></div><h6 id="checking-if-status-remediationplan-was-computed">Checking if status.remediationPlan was computed</h6><div class="variablelist"><dl class="variablelist"><dt><span class="term">Issue</span></dt><dd>
									You want to check if <code class="literal">status.remediationPlan</code> is computed.
								</dd><dt><span class="term">Resolution</span></dt><dd><p class="simpara">
									Run the following command:
								</p><pre class="programlisting language-terminal">$ oc get cgu lab-upgrade -ojsonpath='{.status.remediationPlan}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-json">[["spoke2", "spoke3"]]</pre>

									</p></div></dd></dl></div><h6 id="errors-in-the-talm-manager-container">Errors in the TALM manager container</h6><div class="variablelist"><dl class="variablelist"><dt><span class="term">Issue</span></dt><dd>
									You want to check the logs of the manager container of TALM.
								</dd><dt><span class="term">Resolution</span></dt><dd><p class="simpara">
									Run the following command:
								</p><pre class="programlisting language-terminal">$ oc logs -n openshift-operators \
cluster-group-upgrades-controller-manager-75bcc7484d-8k8xp -c manager</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">ERROR	controller-runtime.manager.controller.clustergroupupgrade	Reconciler error	{"reconciler group": "ran.openshift.io", "reconciler kind": "ClusterGroupUpgrade", "name": "lab-upgrade", "namespace": "default", "error": "Cluster spoke5555 is not a ManagedCluster"} <span id="CO101-1"><!--Empty--></span><span class="callout">1</span>
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO101-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Displays the error.
										</div></dd></dl></div></dd></dl></div><h6 id="clusters-are-not-compliant-to-some-policies-after-a-literal-clustergroupupgrade-literal-cr-has-completed">Clusters are not compliant to some policies after a <code class="literal">ClusterGroupUpgrade</code> CR has completed</h6><div class="variablelist"><dl class="variablelist"><dt><span class="term">Issue</span></dt><dd><p class="simpara">
									The policy compliance status that TALM uses to decide if remediation is needed has not yet fully updated for all clusters. This may be because:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											The CGU was run too soon after a policy was created or updated.
										</li><li class="listitem">
											The remediation of a policy affects the compliance of subsequent policies in the <code class="literal">ClusterGroupUpgrade</code> CR.
										</li></ul></div></dd><dt><span class="term">Resolution</span></dt><dd>
									Create and apply a new <code class="literal">ClusterGroupUpdate</code> CR with the same specification.
								</dd></dl></div><h6 id="talo-troubleshooting-auto-create-policies_cnf-topology-aware-lifecycle-manager">Auto-created <code class="literal">ClusterGroupUpgrade</code> CR in the GitOps ZTP workflow has no managed policies</h6><div class="variablelist"><dl class="variablelist"><dt><span class="term">Issue</span></dt><dd>
									If there are no policies for the managed cluster when the cluster becomes <code class="literal">Ready</code>, a <code class="literal">ClusterGroupUpgrade</code> CR with no policies is auto-created. Upon completion of the <code class="literal">ClusterGroupUpgrade</code> CR, the managed cluster is labeled as <code class="literal">ztp-done</code>. If the <code class="literal">PolicyGenTemplate</code> CRs were not pushed to the Git repository within the required time after <code class="literal">SiteConfig</code> resources were pushed, this might result in no policies being available for the target cluster when the cluster became <code class="literal">Ready</code>.
								</dd><dt><span class="term">Resolution</span></dt><dd>
									Verify that the policies you want to apply are available on the hub cluster, then create a <code class="literal">ClusterGroupUpgrade</code> CR with the required policies.
								</dd></dl></div><p>
						You can either manually create the <code class="literal">ClusterGroupUpgrade</code> CR or trigger auto-creation again. To trigger auto-creation of the <code class="literal">ClusterGroupUpgrade</code> CR, remove the <code class="literal">ztp-done</code> label from the cluster and delete the empty <code class="literal">ClusterGroupUpgrade</code> CR that was previously created in the <code class="literal">zip-install</code> namespace.
					</p><h6 id="talo-troubleshooting-pre-cache-failed_cnf-topology-aware-lifecycle-manager">Pre-caching has failed</h6><div class="variablelist"><dl class="variablelist"><dt><span class="term">Issue</span></dt><dd><p class="simpara">
									Pre-caching might fail for one of the following reasons:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											There is not enough free space on the node.
										</li><li class="listitem">
											For a disconnected environment, the pre-cache image has not been properly mirrored.
										</li><li class="listitem">
											There was an issue when creating the pod.
										</li></ul></div></dd><dt><span class="term">Resolution</span></dt><dd><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
											To check if pre-caching has failed due to insufficient space, check the log of the pre-caching pod in the node.
										</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
													Find the name of the pod using the following command:
												</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-talo-pre-cache</pre></li><li class="listitem"><p class="simpara">
													Check the logs to see if the error is related to insufficient space using the following command:
												</p><pre class="programlisting language-terminal">$ oc logs -n openshift-talo-pre-cache &lt;pod name&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
											If there is no log, check the pod status using the following command:
										</p><pre class="programlisting language-terminal">$ oc describe pod -n openshift-talo-pre-cache &lt;pod name&gt;</pre></li><li class="listitem"><p class="simpara">
											If the pod does not exist, check the job status to see why it could not create a pod using the following command:
										</p><pre class="programlisting language-terminal">$ oc describe job -n openshift-talo-pre-cache pre-cache</pre></li></ol></div></dd></dl></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								For information about troubleshooting, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#troubleshooting-operator-issues-1">OpenShift Container Platform Troubleshooting Operator Issues</a>.
							</li><li class="listitem">
								For more information about using Topology Aware Lifecycle Manager in the ZTP workflow, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-topology-aware-lifecycle-manager">Updating managed policies with Topology Aware Lifecycle Manager</a>.
							</li><li class="listitem">
								For more information about the <code class="literal">PolicyGenTemplate</code> CRD, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-the-policygentemplate_ztp-configuring-managed-clusters-policies">About the PolicyGenTemplate CRD</a>
							</li></ul></div></section></section></section><section class="section" id="ztp-topology-aware-lifecycle-manager"><div class="titlepage"><div><div><h2 class="title">16.11. Updating managed clusters in a disconnected environment with the Topology Aware Lifecycle Manager</h2></div></div></div><p>
				You can use the Topology Aware Lifecycle Manager (TALM) to manage the software lifecycle of OpenShift Container Platform managed clusters. TALM uses Red Hat Advanced Cluster Management (RHACM) policies to perform changes on the target clusters.
			</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						For more information about the Topology Aware Lifecycle Manager, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#cnf-about-topology-aware-lifecycle-manager-config_cnf-topology-aware-lifecycle-manager">About the Topology Aware Lifecycle Manager</a>.
					</li></ul></div><section class="section" id="talo-platform-prepare-end-to-end_ztp-talm"><div class="titlepage"><div><div><h3 class="title">16.11.1. Updating clusters in a disconnected environment</h3></div></div></div><p>
					You can upgrade managed clusters and Operators for managed clusters that you have deployed using GitOps Zero Touch Provisioning (ZTP) and Topology Aware Lifecycle Manager (TALM).
				</p><section class="section" id="talo-platform-prepare-for-update-env-setup_ztp-talm"><div class="titlepage"><div><div><h4 class="title">16.11.1.1. Setting up the environment</h4></div></div></div><p>
						TALM can perform both platform and Operator updates.
					</p><p>
						You must mirror both the platform image and Operator images that you want to update to in your mirror registry before you can use TALM to update your disconnected clusters. Complete the following steps to mirror the images:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								For platform updates, you must perform the following steps:
							</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
										Mirror the desired OpenShift Container Platform image repository. Ensure that the desired platform image is mirrored by following the "Mirroring the OpenShift Container Platform image repository" procedure linked in the Additional Resources. Save the contents of the <code class="literal">imageContentSources</code> section in the <code class="literal">imageContentSources.yaml</code> file:
									</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-yaml">imageContentSources:
 - mirrors:
   - mirror-ocp-registry.ibmcloud.io.cpak:5000/openshift-release-dev/openshift4
   source: quay.io/openshift-release-dev/ocp-release
 - mirrors:
   - mirror-ocp-registry.ibmcloud.io.cpak:5000/openshift-release-dev/openshift4
   source: quay.io/openshift-release-dev/ocp-v4.0-art-dev</pre>

										</p></div></li><li class="listitem"><p class="simpara">
										Save the image signature of the desired platform image that was mirrored. You must add the image signature to the <code class="literal">PolicyGenTemplate</code> CR for platform updates. To get the image signature, perform the following steps:
									</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
												Specify the desired OpenShift Container Platform tag by running the following command:
											</p><pre class="programlisting language-terminal">$ OCP_RELEASE_NUMBER=&lt;release_version&gt;</pre></li><li class="listitem"><p class="simpara">
												Specify the architecture of the cluster by running the following command:
											</p><pre class="programlisting language-terminal">$ ARCHITECTURE=&lt;cluster_architecture&gt; <span id="CO102-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO102-1"><span class="callout">1</span></a> </dt><dd><div class="para">
														Specify the architecture of the cluster, such as <code class="literal">x86_64</code>, <code class="literal">aarch64</code>, <code class="literal">s390x</code>, or <code class="literal">ppc64le</code>.
													</div></dd></dl></div></li><li class="listitem"><p class="simpara">
												Get the release image digest from Quay by running the following command
											</p><pre class="programlisting language-terminal">$ DIGEST="$(oc adm release info quay.io/openshift-release-dev/ocp-release:${OCP_RELEASE_NUMBER}-${ARCHITECTURE} | sed -n 's/Pull From: .*@//p')"</pre></li><li class="listitem"><p class="simpara">
												Set the digest algorithm by running the following command:
											</p><pre class="programlisting language-terminal">$ DIGEST_ALGO="${DIGEST%%:*}"</pre></li><li class="listitem"><p class="simpara">
												Set the digest signature by running the following command:
											</p><pre class="programlisting language-terminal">$ DIGEST_ENCODED="${DIGEST#*:}"</pre></li><li class="listitem"><p class="simpara">
												Get the image signature from the <a class="link" href="https://mirror.openshift.com/pub/openshift-v4/signatures/openshift/release/">mirror.openshift.com</a> website by running the following command:
											</p><pre class="programlisting language-terminal">$ SIGNATURE_BASE64=$(curl -s "https://mirror.openshift.com/pub/openshift-v4/signatures/openshift/release/${DIGEST_ALGO}=${DIGEST_ENCODED}/signature-1" | base64 -w0 &amp;&amp; echo)</pre></li><li class="listitem"><p class="simpara">
												Save the image signature to the <code class="literal">checksum-&lt;OCP_RELEASE_NUMBER&gt;.yaml</code> file by running the following commands:
											</p><pre class="programlisting language-terminal">$ cat &gt;checksum-${OCP_RELEASE_NUMBER}.yaml &lt;&lt;EOF
${DIGEST_ALGO}-${DIGEST_ENCODED}: ${SIGNATURE_BASE64}
EOF</pre></li></ol></div></li><li class="listitem"><p class="simpara">
										Prepare the update graph. You have two options to prepare the update graph:
									</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
												Use the OpenShift Update Service.
											</p><p class="simpara">
												For more information about how to set up the graph on the hub cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.4/html/clusters/managing-your-clusters#deploy-the-operator-for-cincinnati">Deploy the operator for OpenShift Update Service</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.4/html/clusters/managing-your-clusters#build-the-graph-data-init-container">Build the graph data init container</a>.
											</p></li><li class="listitem"><p class="simpara">
												Make a local copy of the upstream graph. Host the update graph on an <code class="literal">http</code> or <code class="literal">https</code> server in the disconnected environment that has access to the managed cluster. To download the update graph, use the following command:
											</p><pre class="programlisting language-terminal">$ curl -s https://api.openshift.com/api/upgrades_info/v1/graph?channel=stable-4.13 -o ~/upgrade-graph_stable-4.13</pre></li></ol></div></li></ol></div></li><li class="listitem"><p class="simpara">
								For Operator updates, you must perform the following task:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										Mirror the Operator catalogs. Ensure that the desired operator images are mirrored by following the procedure in the "Mirroring Operator catalogs for use with disconnected clusters" section.
									</li></ul></div></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								For more information about how to update GitOps Zero Touch Provisioning (ZTP), see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-updating-gitops">Upgrading GitOps ZTP</a>.
							</li><li class="listitem">
								For more information about how to mirror an OpenShift Container Platform image repository, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-mirror-repository_installing-mirroring-installation-images">Mirroring the OpenShift Container Platform image repository</a>.
							</li><li class="listitem">
								For more information about how to mirror Operator catalogs for disconnected clusters, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#olm-mirror-catalog_installing-mirroring-installation-images">Mirroring Operator catalogs for use with disconnected clusters</a>.
							</li><li class="listitem">
								For more information about how to prepare the disconnected environment and mirroring the desired image repository, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-preparing-the-hub-cluster">Preparing the disconnected environment</a>.
							</li><li class="listitem">
								For more information about update channels and releases, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#understanding-upgrade-channels-releases">Understanding update channels and releases</a>.
							</li></ul></div></section><section class="section" id="talo-platform-update_ztp-talm"><div class="titlepage"><div><div><h4 class="title">16.11.1.2. Performing a platform update</h4></div></div></div><p>
						You can perform a platform update with the TALM.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Install the Topology Aware Lifecycle Manager (TALM).
							</li><li class="listitem">
								Update GitOps Zero Touch Provisioning (ZTP) to the latest version.
							</li><li class="listitem">
								Provision one or more managed clusters with GitOps ZTP.
							</li><li class="listitem">
								Mirror the desired image repository.
							</li><li class="listitem">
								Log in as a user with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								Create RHACM policies in the hub cluster.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a <code class="literal">PolicyGenTemplate</code> CR for the platform update:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Save the following contents of the <code class="literal">PolicyGenTemplate</code> CR in the <code class="literal">du-upgrade.yaml</code> file.
									</p><div class="formalpara"><p class="title"><strong>Example of <code class="literal">PolicyGenTemplate</code> for platform update</strong></p><p>
											
<pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "du-upgrade"
  namespace: "ztp-group-du-sno"
spec:
  bindingRules:
    group-du-sno: ""
  mcp: "master"
  remediationAction: inform
  sourceFiles:
    - fileName: ImageSignature.yaml <span id="CO103-1"><!--Empty--></span><span class="callout">1</span>
      policyName: "platform-upgrade-prep"
      binaryData:
        ${DIGEST_ALGO}-${DIGEST_ENCODED}: ${SIGNATURE_BASE64} <span id="CO103-2"><!--Empty--></span><span class="callout">2</span>
    - fileName: DisconnectedICSP.yaml
      policyName: "platform-upgrade-prep"
      metadata:
        name: disconnected-internal-icsp-for-ocp
      spec:
        repositoryDigestMirrors: <span id="CO103-3"><!--Empty--></span><span class="callout">3</span>
          - mirrors:
            - quay-intern.example.com/ocp4/openshift-release-dev
            source: quay.io/openshift-release-dev/ocp-release
          - mirrors:
            - quay-intern.example.com/ocp4/openshift-release-dev
            source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
    - fileName: ClusterVersion.yaml <span id="CO103-4"><!--Empty--></span><span class="callout">4</span>
      policyName: "platform-upgrade"
      metadata:
        name: version
      spec:
        channel: "stable-4.13"
        upstream: http://upgrade.example.com/images/upgrade-graph_stable-4.13
        desiredUpdate:
          version: 4.13.4
      status:
        history:
          - version: 4.13.4
            state: "Completed"</pre>

										</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO103-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												The <code class="literal">ConfigMap</code> CR contains the signature of the desired release image to update to.
											</div></dd><dt><a href="#CO103-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												Shows the image signature of the desired OpenShift Container Platform release. Get the signature from the <code class="literal">checksum-${OCP_RELEASE_NUMBER}.yaml</code> file you saved when following the procedures in the "Setting up the environment" section.
											</div></dd><dt><a href="#CO103-3"><span class="callout">3</span></a> </dt><dd><div class="para">
												Shows the mirror repository that contains the desired OpenShift Container Platform image. Get the mirrors from the <code class="literal">imageContentSources.yaml</code> file that you saved when following the procedures in the "Setting up the environment" section.
											</div></dd><dt><a href="#CO103-4"><span class="callout">4</span></a> </dt><dd><div class="para">
												Shows the <code class="literal">ClusterVersion</code> CR to trigger the update. The <code class="literal">channel</code>, <code class="literal">upstream</code>, and <code class="literal">desiredVersion</code> fields are all required for image pre-caching.
											</div></dd></dl></div><p class="simpara">
										The <code class="literal">PolicyGenTemplate</code> CR generates two policies:
									</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												The <code class="literal">du-upgrade-platform-upgrade-prep</code> policy does the preparation work for the platform update. It creates the <code class="literal">ConfigMap</code> CR for the desired release image signature, creates the image content source of the mirrored release image repository, and updates the cluster version with the desired update channel and the update graph reachable by the managed cluster in the disconnected environment.
											</li><li class="listitem">
												The <code class="literal">du-upgrade-platform-upgrade</code> policy is used to perform platform upgrade.
											</li></ul></div></li><li class="listitem"><p class="simpara">
										Add the <code class="literal">du-upgrade.yaml</code> file contents to the <code class="literal">kustomization.yaml</code> file located in the GitOps ZTP Git repository for the <code class="literal">PolicyGenTemplate</code> CRs and push the changes to the Git repository.
									</p><p class="simpara">
										ArgoCD pulls the changes from the Git repository and generates the policies on the hub cluster.
									</p></li><li class="listitem"><p class="simpara">
										Check the created policies by running the following command:
									</p><pre class="programlisting language-terminal">$ oc get policies -A | grep platform-upgrade</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Create the <code class="literal">ClusterGroupUpdate</code> CR for the platform update with the <code class="literal">spec.enable</code> field set to <code class="literal">false</code>.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Save the content of the platform update <code class="literal">ClusterGroupUpdate</code> CR with the <code class="literal">du-upgrade-platform-upgrade-prep</code> and the <code class="literal">du-upgrade-platform-upgrade</code> policies and the target clusters to the <code class="literal">cgu-platform-upgrade.yml</code> file, as shown in the following example:
									</p><pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: cgu-platform-upgrade
  namespace: default
spec:
  managedPolicies:
  - du-upgrade-platform-upgrade-prep
  - du-upgrade-platform-upgrade
  preCaching: false
  clusters:
  - spoke1
  remediationStrategy:
    maxConcurrency: 1
  enable: false</pre></li><li class="listitem"><p class="simpara">
										Apply the <code class="literal">ClusterGroupUpdate</code> CR to the hub cluster by running the following command:
									</p><pre class="programlisting language-terminal">$ oc apply -f cgu-platform-upgrade.yml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Optional: Pre-cache the images for the platform update.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Enable pre-caching in the <code class="literal">ClusterGroupUpdate</code> CR by running the following command:
									</p><pre class="programlisting language-terminal">$ oc --namespace=default patch clustergroupupgrade.ran.openshift.io/cgu-platform-upgrade \
--patch '{"spec":{"preCaching": true}}' --type=merge</pre></li><li class="listitem"><p class="simpara">
										Monitor the update process and wait for the pre-caching to complete. Check the status of pre-caching by running the following command on the hub cluster:
									</p><pre class="programlisting language-terminal">$ oc get cgu cgu-platform-upgrade -o jsonpath='{.status.precaching.status}'</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Start the platform update:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Enable the <code class="literal">cgu-platform-upgrade</code> policy and disable pre-caching by running the following command:
									</p><pre class="programlisting language-terminal">$ oc --namespace=default patch clustergroupupgrade.ran.openshift.io/cgu-platform-upgrade \
--patch '{"spec":{"enable":true, "preCaching": false}}' --type=merge</pre></li><li class="listitem"><p class="simpara">
										Monitor the process. Upon completion, ensure that the policy is compliant by running the following command:
									</p><pre class="programlisting language-terminal">$ oc get policies --all-namespaces</pre></li></ol></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								For more information about mirroring the images in a disconnected environment, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-acm-adding-images-to-mirror-registry_ztp-preparing-the-hub-cluster">Preparing the disconnected environment</a>.
							</li></ul></div></section><section class="section" id="talo-operator-update_ztp-talm"><div class="titlepage"><div><div><h4 class="title">16.11.1.3. Performing an Operator update</h4></div></div></div><p>
						You can perform an Operator update with the TALM.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Install the Topology Aware Lifecycle Manager (TALM).
							</li><li class="listitem">
								Update GitOps Zero Touch Provisioning (ZTP) to the latest version.
							</li><li class="listitem">
								Provision one or more managed clusters with GitOps ZTP.
							</li><li class="listitem">
								Mirror the desired index image, bundle images, and all Operator images referenced in the bundle images.
							</li><li class="listitem">
								Log in as a user with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								Create RHACM policies in the hub cluster.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Update the <code class="literal">PolicyGenTemplate</code> CR for the Operator update.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Update the <code class="literal">du-upgrade</code> <code class="literal">PolicyGenTemplate</code> CR with the following additional contents in the <code class="literal">du-upgrade.yaml</code> file:
									</p><pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "du-upgrade"
  namespace: "ztp-group-du-sno"
spec:
  bindingRules:
    group-du-sno: ""
  mcp: "master"
  remediationAction: inform
  sourceFiles:
    - fileName: DefaultCatsrc.yaml
      remediationAction: inform
      policyName: "operator-catsrc-policy"
      metadata:
        name: redhat-operators
      spec:
        displayName: Red Hat Operators Catalog
        image: registry.example.com:5000/olm/redhat-operators:v4.13 <span id="CO104-1"><!--Empty--></span><span class="callout">1</span>
        updateStrategy: <span id="CO104-2"><!--Empty--></span><span class="callout">2</span>
          registryPoll:
            interval: 1h</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO104-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												The index image URL contains the desired Operator images. If the index images are always pushed to the same image name and tag, this change is not needed.
											</div></dd><dt><a href="#CO104-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												Set how frequently the Operator Lifecycle Manager (OLM) polls the index image for new Operator versions with the <code class="literal">registryPoll.interval</code> field. This change is not needed if a new index image tag is always pushed for y-stream and z-stream Operator updates. The <code class="literal">registryPoll.interval</code> field can be set to a shorter interval to expedite the update, however shorter intervals increase computational load. To counteract this, you can restore <code class="literal">registryPoll.interval</code> to the default value once the update is complete.
											</div></dd></dl></div></li><li class="listitem"><p class="simpara">
										This update generates one policy, <code class="literal">du-upgrade-operator-catsrc-policy</code>, to update the <code class="literal">redhat-operators</code> catalog source with the new index images that contain the desired Operators images.
									</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											If you want to use the image pre-caching for Operators and there are Operators from a different catalog source other than <code class="literal">redhat-operators</code>, you must perform the following tasks:
										</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
													Prepare a separate catalog source policy with the new index image or registry poll interval update for the different catalog source.
												</li><li class="listitem">
													Prepare a separate subscription policy for the desired Operators that are from the different catalog source.
												</li></ul></div></div></div><p class="simpara">
										For example, the desired SRIOV-FEC Operator is available in the <code class="literal">certified-operators</code> catalog source. To update the catalog source and the Operator subscription, add the following contents to generate two policies, <code class="literal">du-upgrade-fec-catsrc-policy</code> and <code class="literal">du-upgrade-subscriptions-fec-policy</code>:
									</p><pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "du-upgrade"
  namespace: "ztp-group-du-sno"
spec:
  bindingRules:
    group-du-sno: ""
  mcp: "master"
  remediationAction: inform
  sourceFiles:
       …
    - fileName: DefaultCatsrc.yaml
      remediationAction: inform
      policyName: "fec-catsrc-policy"
      metadata:
        name: certified-operators
      spec:
        displayName: Intel SRIOV-FEC Operator
        image: registry.example.com:5000/olm/far-edge-sriov-fec:v4.10
        updateStrategy:
          registryPoll:
            interval: 10m
    - fileName: AcceleratorsSubscription.yaml
      policyName: "subscriptions-fec-policy"
      spec:
        channel: "stable"
        source: certified-operators</pre></li><li class="listitem"><p class="simpara">
										Remove the specified subscriptions channels in the common <code class="literal">PolicyGenTemplate</code> CR, if they exist. The default subscriptions channels from the GitOps ZTP image are used for the update.
									</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											The default channel for the Operators applied through GitOps ZTP 4.13 is <code class="literal">stable</code>, except for the <code class="literal">performance-addon-operator</code>. As of OpenShift Container Platform 4.11, the <code class="literal">performance-addon-operator</code> functionality was moved to the <code class="literal">node-tuning-operator</code>. For the 4.10 release, the default channel for PAO is <code class="literal">v4.10</code>. You can also specify the default channels in the common <code class="literal">PolicyGenTemplate</code> CR.
										</p></div></div></li><li class="listitem"><p class="simpara">
										Push the <code class="literal">PolicyGenTemplate</code> CRs updates to the GitOps ZTP Git repository.
									</p><p class="simpara">
										ArgoCD pulls the changes from the Git repository and generates the policies on the hub cluster.
									</p></li><li class="listitem"><p class="simpara">
										Check the created policies by running the following command:
									</p><pre class="programlisting language-terminal">$ oc get policies -A | grep -E "catsrc-policy|subscription"</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Apply the required catalog source updates before starting the Operator update.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Save the content of the <code class="literal">ClusterGroupUpgrade</code> CR named <code class="literal">operator-upgrade-prep</code> with the catalog source policies and the target managed clusters to the <code class="literal">cgu-operator-upgrade-prep.yml</code> file:
									</p><pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: cgu-operator-upgrade-prep
  namespace: default
spec:
  clusters:
  - spoke1
  enable: true
  managedPolicies:
  - du-upgrade-operator-catsrc-policy
  remediationStrategy:
    maxConcurrency: 1</pre></li><li class="listitem"><p class="simpara">
										Apply the policy to the hub cluster by running the following command:
									</p><pre class="programlisting language-terminal">$ oc apply -f cgu-operator-upgrade-prep.yml</pre></li><li class="listitem"><p class="simpara">
										Monitor the update process. Upon completion, ensure that the policy is compliant by running the following command:
									</p><pre class="programlisting language-terminal">$ oc get policies -A | grep -E "catsrc-policy"</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Create the <code class="literal">ClusterGroupUpgrade</code> CR for the Operator update with the <code class="literal">spec.enable</code> field set to <code class="literal">false</code>.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Save the content of the Operator update <code class="literal">ClusterGroupUpgrade</code> CR with the <code class="literal">du-upgrade-operator-catsrc-policy</code> policy and the subscription policies created from the common <code class="literal">PolicyGenTemplate</code> and the target clusters to the <code class="literal">cgu-operator-upgrade.yml</code> file, as shown in the following example:
									</p><pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: cgu-operator-upgrade
  namespace: default
spec:
  managedPolicies:
  - du-upgrade-operator-catsrc-policy <span id="CO105-1"><!--Empty--></span><span class="callout">1</span>
  - common-subscriptions-policy <span id="CO105-2"><!--Empty--></span><span class="callout">2</span>
  preCaching: false
  clusters:
  - spoke1
  remediationStrategy:
    maxConcurrency: 1
  enable: false</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO105-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												The policy is needed by the image pre-caching feature to retrieve the operator images from the catalog source.
											</div></dd><dt><a href="#CO105-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												The policy contains Operator subscriptions. If you have followed the structure and content of the reference <code class="literal">PolicyGenTemplates</code>, all Operator subscriptions are grouped into the <code class="literal">common-subscriptions-policy</code> policy.
											</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											One <code class="literal">ClusterGroupUpgrade</code> CR can only pre-cache the images of the desired Operators defined in the subscription policy from one catalog source included in the <code class="literal">ClusterGroupUpgrade</code> CR. If the desired Operators are from different catalog sources, such as in the example of the SRIOV-FEC Operator, another <code class="literal">ClusterGroupUpgrade</code> CR must be created with <code class="literal">du-upgrade-fec-catsrc-policy</code> and <code class="literal">du-upgrade-subscriptions-fec-policy</code> policies for the SRIOV-FEC Operator images pre-caching and update.
										</p></div></div></li><li class="listitem"><p class="simpara">
										Apply the <code class="literal">ClusterGroupUpgrade</code> CR to the hub cluster by running the following command:
									</p><pre class="programlisting language-terminal">$ oc apply -f cgu-operator-upgrade.yml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Optional: Pre-cache the images for the Operator update.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Before starting image pre-caching, verify the subscription policy is <code class="literal">NonCompliant</code> at this point by running the following command:
									</p><pre class="programlisting language-terminal">$ oc get policy common-subscriptions-policy -n &lt;policy_namespace&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">NAME                          REMEDIATION ACTION   COMPLIANCE STATE     AGE
common-subscriptions-policy   inform               NonCompliant         27d</pre>

										</p></div></li><li class="listitem"><p class="simpara">
										Enable pre-caching in the <code class="literal">ClusterGroupUpgrade</code> CR by running the following command:
									</p><pre class="programlisting language-terminal">$ oc --namespace=default patch clustergroupupgrade.ran.openshift.io/cgu-operator-upgrade \
--patch '{"spec":{"preCaching": true}}' --type=merge</pre></li><li class="listitem"><p class="simpara">
										Monitor the process and wait for the pre-caching to complete. Check the status of pre-caching by running the following command on the managed cluster:
									</p><pre class="programlisting language-terminal">$ oc get cgu cgu-operator-upgrade -o jsonpath='{.status.precaching.status}'</pre></li><li class="listitem"><p class="simpara">
										Check if the pre-caching is completed before starting the update by running the following command:
									</p><pre class="programlisting language-terminal">$ oc get cgu -n default cgu-operator-upgrade -ojsonpath='{.status.conditions}' | jq</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-json">[
    {
      "lastTransitionTime": "2022-03-08T20:49:08.000Z",
      "message": "The ClusterGroupUpgrade CR is not enabled",
      "reason": "UpgradeNotStarted",
      "status": "False",
      "type": "Ready"
    },
    {
      "lastTransitionTime": "2022-03-08T20:55:30.000Z",
      "message": "Precaching is completed",
      "reason": "PrecachingCompleted",
      "status": "True",
      "type": "PrecachingDone"
    }
]</pre>

										</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
								Start the Operator update.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Enable the <code class="literal">cgu-operator-upgrade</code> <code class="literal">ClusterGroupUpgrade</code> CR and disable pre-caching to start the Operator update by running the following command:
									</p><pre class="programlisting language-terminal">$ oc --namespace=default patch clustergroupupgrade.ran.openshift.io/cgu-operator-upgrade \
--patch '{"spec":{"enable":true, "preCaching": false}}' --type=merge</pre></li><li class="listitem"><p class="simpara">
										Monitor the process. Upon completion, ensure that the policy is compliant by running the following command:
									</p><pre class="programlisting language-terminal">$ oc get policies --all-namespaces</pre></li></ol></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								For more information about updating GitOps ZTP, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#ztp-updating-gitops">Upgrading GitOps ZTP</a>.
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#cnf-topology-aware-lifecycle-manager-operator-troubleshooting_ztp-talm">Troubleshooting missed Operator updates due to out-of-date policy compliance states</a>.
							</li></ul></div><section class="section" id="cnf-topology-aware-lifecycle-manager-operator-troubleshooting_ztp-talm"><div class="titlepage"><div><div><h5 class="title">16.11.1.3.1. Troubleshooting missed Operator updates due to out-of-date policy compliance states</h5></div></div></div><p>
							In some scenarios, Topology Aware Lifecycle Manager (TALM) might miss Operator updates due to an out-of-date policy compliance state.
						</p><p>
							After a catalog source update, it takes time for the Operator Lifecycle Manager (OLM) to update the subscription status. The status of the subscription policy might continue to show as compliant while TALM decides whether remediation is needed. As a result, the Operator specified in the subscription policy does not get upgraded.
						</p><p>
							To avoid this scenario, add another catalog source configuration to the <code class="literal">PolicyGenTemplate</code> and specify this configuration in the subscription for any Operators that require an update.
						</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
									Add a catalog source configuration in the <code class="literal">PolicyGenTemplate</code> resource:
								</p><pre class="programlisting language-yaml">- fileName: DefaultCatsrc.yaml
      remediationAction: inform
      policyName: "operator-catsrc-policy"
      metadata:
        name: redhat-operators
      spec:
        displayName: Red Hat Operators Catalog
        image: registry.example.com:5000/olm/redhat-operators:v{product-version}
        updateStrategy:
          registryPoll:
            interval: 1h
      status:
        connectionState:
            lastObservedState: READY
- fileName: DefaultCatsrc.yaml
      remediationAction: inform
      policyName: "operator-catsrc-policy"
      metadata:
        name: redhat-operators-v2 <span id="CO106-1"><!--Empty--></span><span class="callout">1</span>
      spec:
        displayName: Red Hat Operators Catalog v2 <span id="CO106-2"><!--Empty--></span><span class="callout">2</span>
        image: registry.example.com:5000/olredhat-operators:&lt;version&gt; <span id="CO106-3"><!--Empty--></span><span class="callout">3</span>
        updateStrategy:
          registryPoll:
            interval: 1h
      status:
        connectionState:
            lastObservedState: READY</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO106-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Update the name for the new configuration.
										</div></dd><dt><a href="#CO106-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Update the display name for the new configuration.
										</div></dd><dt><a href="#CO106-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Update the index image URL. This <code class="literal">fileName.spec.image</code> field overrides any configuration in the <code class="literal">DefaultCatsrc.yaml</code> file.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Update the <code class="literal">Subscription</code> resource to point to the new configuration for Operators that require an update:
								</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: operator-subscription
  namespace: operator-namspace
# ...
spec:
  source: redhat-operators-v2 <span id="CO107-1"><!--Empty--></span><span class="callout">1</span>
# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO107-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Enter the name of the additional catalog source configuration that you defined in the <code class="literal">PolicyGenTemplate</code> resource.
										</div></dd></dl></div></li></ol></div></section></section><section class="section" id="talo-operator-and-platform-update_ztp-talm"><div class="titlepage"><div><div><h4 class="title">16.11.1.4. Performing a platform and an Operator update together</h4></div></div></div><p>
						You can perform a platform and an Operator update at the same time.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Install the Topology Aware Lifecycle Manager (TALM).
							</li><li class="listitem">
								Update GitOps Zero Touch Provisioning (ZTP) to the latest version.
							</li><li class="listitem">
								Provision one or more managed clusters with GitOps ZTP.
							</li><li class="listitem">
								Log in as a user with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								Create RHACM policies in the hub cluster.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Create the <code class="literal">PolicyGenTemplate</code> CR for the updates by following the steps described in the "Performing a platform update" and "Performing an Operator update" sections.
							</li><li class="listitem"><p class="simpara">
								Apply the prep work for the platform and the Operator update.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Save the content of the <code class="literal">ClusterGroupUpgrade</code> CR with the policies for platform update preparation work, catalog source updates, and target clusters to the <code class="literal">cgu-platform-operator-upgrade-prep.yml</code> file, for example:
									</p><pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: cgu-platform-operator-upgrade-prep
  namespace: default
spec:
  managedPolicies:
  - du-upgrade-platform-upgrade-prep
  - du-upgrade-operator-catsrc-policy
  clusterSelector:
  - group-du-sno
  remediationStrategy:
    maxConcurrency: 10
  enable: true</pre></li><li class="listitem"><p class="simpara">
										Apply the <code class="literal">cgu-platform-operator-upgrade-prep.yml</code> file to the hub cluster by running the following command:
									</p><pre class="programlisting language-terminal">$ oc apply -f cgu-platform-operator-upgrade-prep.yml</pre></li><li class="listitem"><p class="simpara">
										Monitor the process. Upon completion, ensure that the policy is compliant by running the following command:
									</p><pre class="programlisting language-terminal">$ oc get policies --all-namespaces</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Create the <code class="literal">ClusterGroupUpdate</code> CR for the platform and the Operator update with the <code class="literal">spec.enable</code> field set to <code class="literal">false</code>.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Save the contents of the platform and Operator update <code class="literal">ClusterGroupUpdate</code> CR with the policies and the target clusters to the <code class="literal">cgu-platform-operator-upgrade.yml</code> file, as shown in the following example:
									</p><pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: cgu-du-upgrade
  namespace: default
spec:
  managedPolicies:
  - du-upgrade-platform-upgrade <span id="CO108-1"><!--Empty--></span><span class="callout">1</span>
  - du-upgrade-operator-catsrc-policy <span id="CO108-2"><!--Empty--></span><span class="callout">2</span>
  - common-subscriptions-policy <span id="CO108-3"><!--Empty--></span><span class="callout">3</span>
  preCaching: true
  clusterSelector:
  - group-du-sno
  remediationStrategy:
    maxConcurrency: 1
  enable: false</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO108-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												This is the platform update policy.
											</div></dd><dt><a href="#CO108-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												This is the policy containing the catalog source information for the Operators to be updated. It is needed for the pre-caching feature to determine which Operator images to download to the managed cluster.
											</div></dd><dt><a href="#CO108-3"><span class="callout">3</span></a> </dt><dd><div class="para">
												This is the policy to update the Operators.
											</div></dd></dl></div></li><li class="listitem"><p class="simpara">
										Apply the <code class="literal">cgu-platform-operator-upgrade.yml</code> file to the hub cluster by running the following command:
									</p><pre class="programlisting language-terminal">$ oc apply -f cgu-platform-operator-upgrade.yml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Optional: Pre-cache the images for the platform and the Operator update.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Enable pre-caching in the <code class="literal">ClusterGroupUpgrade</code> CR by running the following command:
									</p><pre class="programlisting language-terminal">$ oc --namespace=default patch clustergroupupgrade.ran.openshift.io/cgu-du-upgrade \
--patch '{"spec":{"preCaching": true}}' --type=merge</pre></li><li class="listitem"><p class="simpara">
										Monitor the update process and wait for the pre-caching to complete. Check the status of pre-caching by running the following command on the managed cluster:
									</p><pre class="programlisting language-terminal">$ oc get jobs,pods -n openshift-talm-pre-cache</pre></li><li class="listitem"><p class="simpara">
										Check if the pre-caching is completed before starting the update by running the following command:
									</p><pre class="programlisting language-terminal">$ oc get cgu cgu-du-upgrade -ojsonpath='{.status.conditions}'</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Start the platform and Operator update.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Enable the <code class="literal">cgu-du-upgrade</code> <code class="literal">ClusterGroupUpgrade</code> CR to start the platform and the Operator update by running the following command:
									</p><pre class="programlisting language-terminal">$ oc --namespace=default patch clustergroupupgrade.ran.openshift.io/cgu-du-upgrade \
--patch '{"spec":{"enable":true, "preCaching": false}}' --type=merge</pre></li><li class="listitem"><p class="simpara">
										Monitor the process. Upon completion, ensure that the policy is compliant by running the following command:
									</p><pre class="programlisting language-terminal">$ oc get policies --all-namespaces</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											The CRs for the platform and Operator updates can be created from the beginning by configuring the setting to <code class="literal">spec.enable: true</code>. In this case, the update starts immediately after pre-caching completes and there is no need to manually enable the CR.
										</p><p>
											Both pre-caching and the update create extra resources, such as policies, placement bindings, placement rules, managed cluster actions, and managed cluster view, to help complete the procedures. Setting the <code class="literal">afterCompletion.deleteObjects</code> field to <code class="literal">true</code> deletes all these resources after the updates complete.
										</p></div></div></li></ol></div></li></ol></div></section><section class="section" id="talm-pao-update_ztp-talm"><div class="titlepage"><div><div><h4 class="title">16.11.1.5. Removing Performance Addon Operator subscriptions from deployed clusters</h4></div></div></div><p>
						In earlier versions of OpenShift Container Platform, the Performance Addon Operator provided automatic, low latency performance tuning for applications. In OpenShift Container Platform 4.11 or later, these functions are part of the Node Tuning Operator.
					</p><p>
						Do not install the Performance Addon Operator on clusters running OpenShift Container Platform 4.11 or later. If you upgrade to OpenShift Container Platform 4.11 or later, the Node Tuning Operator automatically removes the Performance Addon Operator.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You need to remove any policies that create Performance Addon Operator subscriptions to prevent a re-installation of the Operator.
						</p></div></div><p>
						The reference DU profile includes the Performance Addon Operator in the <code class="literal">PolicyGenTemplate</code> CR <code class="literal">common-ranGen.yaml</code>. To remove the subscription from deployed managed clusters, you must update <code class="literal">common-ranGen.yaml</code>.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If you install Performance Addon Operator 4.10.3-5 or later on OpenShift Container Platform 4.11 or later, the Performance Addon Operator detects the cluster version and automatically hibernates to avoid interfering with the Node Tuning Operator functions. However, to ensure best performance, remove the Performance Addon Operator from your OpenShift Container Platform 4.11 clusters.
						</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Create a Git repository where you manage your custom site configuration data. The repository must be accessible from the hub cluster and be defined as a source repository for ArgoCD.
							</li><li class="listitem">
								Update to OpenShift Container Platform 4.11 or later.
							</li><li class="listitem">
								Log in as a user with <code class="literal">cluster-admin</code> privileges.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Change the <code class="literal">complianceType</code> to <code class="literal">mustnothave</code> for the Performance Addon Operator namespace, Operator group, and subscription in the <code class="literal">common-ranGen.yaml</code> file.
							</p><pre class="programlisting language-yaml"> -  fileName: PaoSubscriptionNS.yaml
    policyName: "subscriptions-policy"
    complianceType: mustnothave
 -  fileName: PaoSubscriptionOperGroup.yaml
    policyName: "subscriptions-policy"
    complianceType: mustnothave
 -  fileName: PaoSubscription.yaml
    policyName: "subscriptions-policy"
    complianceType: mustnothave</pre></li><li class="listitem">
								Merge the changes with your custom site repository and wait for the ArgoCD application to synchronize the change to the hub cluster. The status of the <code class="literal">common-subscriptions-policy</code> policy changes to <code class="literal">Non-Compliant</code>.
							</li><li class="listitem">
								Apply the change to your target clusters by using the Topology Aware Lifecycle Manager. For more information about rolling out configuration changes, see the "Additional resources" section.
							</li><li class="listitem"><p class="simpara">
								Monitor the process. When the status of the <code class="literal">common-subscriptions-policy</code> policy for a target cluster is <code class="literal">Compliant</code>, the Performance Addon Operator has been removed from the cluster. Get the status of the <code class="literal">common-subscriptions-policy</code> by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get policy -n ztp-common common-subscriptions-policy</pre></li><li class="listitem">
								Delete the Performance Addon Operator namespace, Operator group and subscription CRs from <code class="literal">.spec.sourceFiles</code> in the <code class="literal">common-ranGen.yaml</code> file.
							</li><li class="listitem">
								Merge the changes with your custom site repository and wait for the ArgoCD application to synchronize the change to the hub cluster. The policy remains compliant.
							</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								For more information about the TALM pre-caching workflow, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#talo-precache-feature-concept_cnf-topology-aware-lifecycle-manager">Using the container image pre-cache feature</a>.
							</li></ul></div></section></section><section class="section" id="talo-precache-autocreated-cgu-for-ztp_ztp-talm"><div class="titlepage"><div><div><h3 class="title">16.11.2. About the auto-created ClusterGroupUpgrade CR for GitOps ZTP</h3></div></div></div><p>
					TALM has a controller called <code class="literal">ManagedClusterForCGU</code> that monitors the <code class="literal">Ready</code> state of the <code class="literal">ManagedCluster</code> CRs on the hub cluster and creates the <code class="literal">ClusterGroupUpgrade</code> CRs for GitOps Zero Touch Provisioning (ZTP).
				</p><p>
					For any managed cluster in the <code class="literal">Ready</code> state without a <code class="literal">ztp-done</code> label applied, the <code class="literal">ManagedClusterForCGU</code> controller automatically creates a <code class="literal">ClusterGroupUpgrade</code> CR in the <code class="literal">ztp-install</code> namespace with its associated RHACM policies that are created during the GitOps ZTP process. TALM then remediates the set of configuration policies that are listed in the auto-created <code class="literal">ClusterGroupUpgrade</code> CR to push the configuration CRs to the managed cluster.
				</p><p>
					If there are no policies for the managed cluster at the time when the cluster becomes <code class="literal">Ready</code>, a <code class="literal">ClusterGroupUpgrade</code> CR with no policies is created. Upon completion of the <code class="literal">ClusterGroupUpgrade</code> the managed cluster is labeled as <code class="literal">ztp-done</code>. If there are policies that you want to apply for that managed cluster, manually create a <code class="literal">ClusterGroupUpgrade</code> as a day-2 operation.
				</p><div class="formalpara"><p class="title"><strong>Example of an auto-created <code class="literal">ClusterGroupUpgrade</code> CR for GitOps ZTP</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  generation: 1
  name: spoke1
  namespace: ztp-install
  ownerReferences:
  - apiVersion: cluster.open-cluster-management.io/v1
    blockOwnerDeletion: true
    controller: true
    kind: ManagedCluster
    name: spoke1
    uid: 98fdb9b2-51ee-4ee7-8f57-a84f7f35b9d5
  resourceVersion: "46666836"
  uid: b8be9cd2-764f-4a62-87d6-6b767852c7da
spec:
  actions:
    afterCompletion:
      addClusterLabels:
        ztp-done: "" <span id="CO109-1"><!--Empty--></span><span class="callout">1</span>
      deleteClusterLabels:
        ztp-running: ""
      deleteObjects: true
    beforeEnable:
      addClusterLabels:
        ztp-running: "" <span id="CO109-2"><!--Empty--></span><span class="callout">2</span>
  clusters:
  - spoke1
  enable: true
  managedPolicies:
  - common-spoke1-config-policy
  - common-spoke1-subscriptions-policy
  - group-spoke1-config-policy
  - spoke1-config-policy
  - group-spoke1-validator-du-policy
  preCaching: false
  remediationStrategy:
    maxConcurrency: 1
    timeout: 240</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO109-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Applied to the managed cluster when TALM completes the cluster configuration.
						</div></dd><dt><a href="#CO109-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Applied to the managed cluster when TALM starts deploying the configuration policies.
						</div></dd></dl></div></section></section><section class="section" id="ztp-updating-gitops"><div class="titlepage"><div><div><h2 class="title">16.12. Updating GitOps ZTP</h2></div></div></div><p>
				You can update the GitOps Zero Touch Provisioning (ZTP) infrastructure independently from the hub cluster, Red Hat Advanced Cluster Management (RHACM), and the managed OpenShift Container Platform clusters.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					You can update the Red Hat OpenShift GitOps Operator when new versions become available. When updating the GitOps ZTP plugin, review the updated files in the reference configuration and ensure that the changes meet your requirements.
				</p></div></div><section class="section" id="ztp-updating-gitops-ztp_ztp-updating-gitops"><div class="titlepage"><div><div><h3 class="title">16.12.1. Overview of the GitOps ZTP update process</h3></div></div></div><p>
					You can update GitOps Zero Touch Provisioning (ZTP) for a fully operational hub cluster running an earlier version of the GitOps ZTP infrastructure. The update process avoids impact on managed clusters.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Any changes to policy settings, including adding recommended content, results in updated polices that must be rolled out to the managed clusters and reconciled.
					</p></div></div><p>
					At a high level, the strategy for updating the GitOps ZTP infrastructure is as follows:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Label all existing clusters with the <code class="literal">ztp-done</code> label.
						</li><li class="listitem">
							Stop the ArgoCD applications.
						</li><li class="listitem">
							Install the new GitOps ZTP tools.
						</li><li class="listitem">
							Update required content and optional changes in the Git repository.
						</li><li class="listitem">
							Update and restart the application configuration.
						</li></ol></div></section><section class="section" id="ztp-preparing-for-the-gitops-ztp-upgrade_ztp-updating-gitops"><div class="titlepage"><div><div><h3 class="title">16.12.2. Preparing for the upgrade</h3></div></div></div><p>
					Use the following procedure to prepare your site for the GitOps Zero Touch Provisioning (ZTP) upgrade.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Get the latest version of the GitOps ZTP container that has the custom resources (CRs) used to configure Red Hat OpenShift GitOps for use with GitOps ZTP.
						</li><li class="listitem"><p class="simpara">
							Extract the <code class="literal">argocd/deployment</code> directory by using the following commands:
						</p><pre class="programlisting language-terminal">$ mkdir -p ./update</pre><pre class="programlisting language-terminal">$ podman run --log-driver=none --rm registry.redhat.io/openshift4/ztp-site-generate-rhel8:v4.13 extract /home/ztp --tar | tar x -C ./update</pre><p class="simpara">
							The <code class="literal">/update</code> directory contains the following subdirectories:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">update/extra-manifest</code>: contains the source CR files that the <code class="literal">SiteConfig</code> CR uses to generate the extra manifest <code class="literal">configMap</code>.
								</li><li class="listitem">
									<code class="literal">update/source-crs</code>: contains the source CR files that the <code class="literal">PolicyGenTemplate</code> CR uses to generate the Red Hat Advanced Cluster Management (RHACM) policies.
								</li><li class="listitem">
									<code class="literal">update/argocd/deployment</code>: contains patches and YAML files to apply on the hub cluster for use in the next step of this procedure.
								</li><li class="listitem">
									<code class="literal">update/argocd/example</code>: contains example <code class="literal">SiteConfig</code> and <code class="literal">PolicyGenTemplate</code> files that represent the recommended configuration.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							Update the <code class="literal">clusters-app.yaml</code> and <code class="literal">policies-app.yaml</code> files to reflect the name of your applications and the URL, branch, and path for your Git repository.
						</p><p class="simpara">
							If the upgrade includes changes that results in obsolete policies, the obsolete policies should be removed prior to performing the upgrade.
						</p></li><li class="listitem"><p class="simpara">
							Diff the changes between the configuration and deployment source CRs in the <code class="literal">/update</code> folder and Git repo where you manage your fleet site CRs. Apply and push the required changes to your site repository.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								When you update GitOps ZTP to the latest version, you must apply the changes from the <code class="literal">update/argocd/deployment</code> directory to your site repository. Do not use older versions of the <code class="literal">argocd/deployment/</code> files.
							</p></div></div></li></ol></div></section><section class="section" id="ztp-labeling-the-existing-clusters_ztp-updating-gitops"><div class="titlepage"><div><div><h3 class="title">16.12.3. Labeling the existing clusters</h3></div></div></div><p>
					To ensure that existing clusters remain untouched by the tool updates, label all existing managed clusters with the <code class="literal">ztp-done</code> label.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						This procedure only applies when updating clusters that were not provisioned with Topology Aware Lifecycle Manager (TALM). Clusters that you provision with TALM are automatically labeled with <code class="literal">ztp-done</code>.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Find a label selector that lists the managed clusters that were deployed with GitOps Zero Touch Provisioning (ZTP), such as <code class="literal">local-cluster!=true</code>:
						</p><pre class="programlisting language-terminal">$ oc get managedcluster -l 'local-cluster!=true'</pre></li><li class="listitem"><p class="simpara">
							Ensure that the resulting list contains all the managed clusters that were deployed with GitOps ZTP, and then use that selector to add the <code class="literal">ztp-done</code> label:
						</p><pre class="programlisting language-terminal">$ oc label managedcluster -l 'local-cluster!=true' ztp-done=</pre></li></ol></div></section><section class="section" id="ztp-stopping-the-existing-gitops-ztp-applications_ztp-updating-gitops"><div class="titlepage"><div><div><h3 class="title">16.12.4. Stopping the existing GitOps ZTP applications</h3></div></div></div><p>
					Removing the existing applications ensures that any changes to existing content in the Git repository are not rolled out until the new version of the tools is available.
				</p><p>
					Use the application files from the <code class="literal">deployment</code> directory. If you used custom names for the applications, update the names in these files first.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Perform a non-cascaded delete on the <code class="literal">clusters</code> application to leave all generated resources in place:
						</p><pre class="programlisting language-terminal">$ oc delete -f update/argocd/deployment/clusters-app.yaml</pre></li><li class="listitem"><p class="simpara">
							Perform a cascaded delete on the <code class="literal">policies</code> application to remove all previous policies:
						</p><pre class="programlisting language-terminal">$ oc patch -f policies-app.yaml -p '{"metadata": {"finalizers": ["resources-finalizer.argocd.argoproj.io"]}}' --type merge</pre><pre class="programlisting language-terminal">$ oc delete -f update/argocd/deployment/policies-app.yaml</pre></li></ol></div></section><section class="section" id="ztp-required-changes-to-the-git-repository_ztp-updating-gitops"><div class="titlepage"><div><div><h3 class="title">16.12.5. Required changes to the Git repository</h3></div></div></div><p>
					When upgrading the <code class="literal">ztp-site-generate</code> container from an earlier release of GitOps Zero Touch Provisioning (ZTP) to 4.10 or later, there are additional requirements for the contents of the Git repository. Existing content in the repository must be updated to reflect these changes.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Make required changes to <code class="literal">PolicyGenTemplate</code> files:
						</p><p class="simpara">
							All <code class="literal">PolicyGenTemplate</code> files must be created in a <code class="literal">Namespace</code> prefixed with <code class="literal">ztp</code>. This ensures that the GitOps ZTP application is able to manage the policy CRs generated by GitOps ZTP without conflicting with the way Red Hat Advanced Cluster Management (RHACM) manages the policies internally.
						</p></li><li class="listitem"><p class="simpara">
							Add the <code class="literal">kustomization.yaml</code> file to the repository:
						</p><p class="simpara">
							All <code class="literal">SiteConfig</code> and <code class="literal">PolicyGenTemplate</code> CRs must be included in a <code class="literal">kustomization.yaml</code> file under their respective directory trees. For example:
						</p><pre class="programlisting language-terminal">├── policygentemplates
│   ├── site1-ns.yaml
│   ├── site1.yaml
│   ├── site2-ns.yaml
│   ├── site2.yaml
│   ├── common-ns.yaml
│   ├── common-ranGen.yaml
│   ├── group-du-sno-ranGen-ns.yaml
│   ├── group-du-sno-ranGen.yaml
│   └── kustomization.yaml
└── siteconfig
    ├── site1.yaml
    ├── site2.yaml
    └── kustomization.yaml</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The files listed in the <code class="literal">generator</code> sections must contain either <code class="literal">SiteConfig</code> or <code class="literal">PolicyGenTemplate</code> CRs only. If your existing YAML files contain other CRs, for example, <code class="literal">Namespace</code>, these other CRs must be pulled out into separate files and listed in the <code class="literal">resources</code> section.
							</p></div></div><p class="simpara">
							The <code class="literal">PolicyGenTemplate</code> kustomization file must contain all <code class="literal">PolicyGenTemplate</code> YAML files in the <code class="literal">generator</code> section and <code class="literal">Namespace</code> CRs in the <code class="literal">resources</code> section. For example:
						</p><pre class="programlisting language-yaml">apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

generators:
- common-ranGen.yaml
- group-du-sno-ranGen.yaml
- site1.yaml
- site2.yaml

resources:
- common-ns.yaml
- group-du-sno-ranGen-ns.yaml
- site1-ns.yaml
- site2-ns.yaml</pre><p class="simpara">
							The <code class="literal">SiteConfig</code> kustomization file must contain all <code class="literal">SiteConfig</code> YAML files in the <code class="literal">generator</code> section and any other CRs in the resources:
						</p><pre class="programlisting language-terminal">apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

generators:
- site1.yaml
- site2.yaml</pre></li><li class="listitem"><p class="simpara">
							Remove the <code class="literal">pre-sync.yaml</code> and <code class="literal">post-sync.yaml</code> files.
						</p><p class="simpara">
							In OpenShift Container Platform 4.10 and later, the <code class="literal">pre-sync.yaml</code> and <code class="literal">post-sync.yaml</code> files are no longer required. The <code class="literal">update/deployment/kustomization.yaml</code> CR manages the policies deployment on the hub cluster.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								There is a set of <code class="literal">pre-sync.yaml</code> and <code class="literal">post-sync.yaml</code> files under both the <code class="literal">SiteConfig</code> and <code class="literal">PolicyGenTemplate</code> trees.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Review and incorporate recommended changes
						</p><p class="simpara">
							Each release may include additional recommended changes to the configuration applied to deployed clusters. Typically these changes result in lower CPU use by the OpenShift platform, additional features, or improved tuning of the platform.
						</p><p class="simpara">
							Review the reference <code class="literal">SiteConfig</code> and <code class="literal">PolicyGenTemplate</code> CRs applicable to the types of cluster in your network. These examples can be found in the <code class="literal">argocd/example</code> directory extracted from the GitOps ZTP container.
						</p></li></ul></div></section><section class="section" id="ztp-installing-the-new-gitops-ztp-applications_ztp-updating-gitops"><div class="titlepage"><div><div><h3 class="title">16.12.6. Installing the new GitOps ZTP applications</h3></div></div></div><p>
					Using the extracted <code class="literal">argocd/deployment</code> directory, and after ensuring that the applications point to your site Git repository, apply the full contents of the deployment directory. Applying the full contents of the directory ensures that all necessary resources for the applications are correctly configured.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To patch the ArgoCD instance in the hub cluster by using the patch file that you previously extracted into the <code class="literal">update/argocd/deployment/</code> directory, enter the following command:
						</p><pre class="programlisting language-terminal">$ oc patch argocd openshift-gitops \
-n openshift-gitops --type=merge \
--patch-file update/argocd/deployment/argocd-openshift-gitops-patch.json</pre></li><li class="listitem"><p class="simpara">
							To apply the contents of the <code class="literal">argocd/deployment</code> directory, enter the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -k update/argocd/deployment</pre></li></ol></div></section><section class="section" id="ztp-roll-out-the-configuration-changes_ztp-updating-gitops"><div class="titlepage"><div><div><h3 class="title">16.12.7. Rolling out the GitOps ZTP configuration changes</h3></div></div></div><p>
					If any configuration changes were included in the upgrade due to implementing recommended changes, the upgrade process results in a set of policy CRs on the hub cluster in the <code class="literal">Non-Compliant</code> state. With the GitOps Zero Touch Provisioning (ZTP) version 4.10 and later <code class="literal">ztp-site-generate</code> container, these policies are set to <code class="literal">inform</code> mode and are not pushed to the managed clusters without an additional step by the user. This ensures that potentially disruptive changes to the clusters can be managed in terms of when the changes are made, for example, during a maintenance window, and how many clusters are updated concurrently.
				</p><p>
					To roll out the changes, create one or more <code class="literal">ClusterGroupUpgrade</code> CRs as detailed in the TALM documentation. The CR must contain the list of <code class="literal">Non-Compliant</code> policies that you want to push out to the managed clusters as well as a list or selector of which clusters should be included in the update.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For information about the Topology Aware Lifecycle Manager (TALM), see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#cnf-about-topology-aware-lifecycle-manager-config_cnf-topology-aware-lifecycle-manager">About the Topology Aware Lifecycle Manager configuration</a>.
						</li><li class="listitem">
							For information about creating <code class="literal">ClusterGroupUpgrade</code> CRs, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#talo-precache-autocreated-cgu-for-ztp_ztp-talm">About the auto-created ClusterGroupUpgrade CR for ZTP</a>.
						</li></ul></div></section></section><section class="section" id="ztp-sno-additional-worker-node"><div class="titlepage"><div><div><h2 class="title">16.13. Expanding single-node OpenShift clusters with GitOps ZTP</h2></div></div></div><p>
				You can expand single-node OpenShift clusters with GitOps Zero Touch Provisioning (ZTP). When you add worker nodes to single-node OpenShift clusters, the original single-node OpenShift cluster retains the control plane node role. Adding worker nodes does not require any downtime for the existing single-node OpenShift cluster.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Although there is no specified limit on the number of worker nodes that you can add to a single-node OpenShift cluster, you must revaluate the reserved CPU allocation on the control plane node for the additional worker nodes.
				</p></div></div><p>
				If you require workload partitioning on the worker node, you must deploy and remediate the managed cluster policies on the hub cluster before installing the node. This way, the workload partitioning <code class="literal">MachineConfig</code> objects are rendered and associated with the <code class="literal">worker</code> machine config pool before the GitOps ZTP workflow applies the <code class="literal">MachineConfig</code> ignition file to the worker node.
			</p><p>
				It is recommended that you first remediate the policies, and then install the worker node. If you create the workload partitioning manifests after installing the worker node, you must drain the node manually and delete all the pods managed by daemon sets. When the managing daemon sets create the new pods, the new pods undergo the workload partitioning process.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Adding worker nodes to single-node OpenShift clusters with GitOps ZTP is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
				</p><p>
					For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
				</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						For more information about single-node OpenShift clusters tuned for vDU application deployments, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#sno-configure-for-vdu">Reference configuration for deploying vDUs on single-node OpenShift</a>.
					</li><li class="listitem">
						For more information about worker nodes, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#adding-worker-nodes-to-single-node-openshift-clusters">Adding worker nodes to single-node OpenShift clusters</a>.
					</li></ul></div><section class="section" id="ztp-additional-worker-apply-du-profile_sno-additional-worker"><div class="titlepage"><div><div><h3 class="title">16.13.1. Applying profiles to the worker node</h3></div></div></div><p>
					You can configure the additional worker node with a DU profile.
				</p><p>
					You can apply a RAN distributed unit (DU) profile to the worker node cluster using the GitOps Zero Touch Provisioning (ZTP) common, group, and site-specific <code class="literal">PolicyGenTemplate</code> resources. The GitOps ZTP pipeline that is linked to the ArgoCD <code class="literal">policies</code> application includes the following CRs that you can find in the <code class="literal">out/argocd/example/policygentemplates</code> folder when you extract the <code class="literal">ztp-site-generate</code> container:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">common-ranGen.yaml</code>
						</li><li class="listitem">
							<code class="literal">group-du-sno-ranGen.yaml</code>
						</li><li class="listitem">
							<code class="literal">example-sno-site.yaml</code>
						</li><li class="listitem">
							<code class="literal">ns.yaml</code>
						</li><li class="listitem">
							<code class="literal">kustomization.yaml</code>
						</li></ul></div><p>
					Configuring the DU profile on the worker node is considered an upgrade. To initiate the upgrade flow, you must update the existing policies or create additional ones. Then, you must create a <code class="literal">ClusterGroupUpgrade</code> CR to reconcile the policies in the group of clusters.
				</p></section><section class="section" id="ztp-additional-worker-daemon-selector-comp_sno-additional-worker"><div class="titlepage"><div><div><h3 class="title">16.13.2. (Optional) Ensuring PTP and SR-IOV daemon selector compatibility</h3></div></div></div><p>
					If the DU profile was deployed using the GitOps Zero Touch Provisioning (ZTP) plugin version 4.11 or earlier, the PTP and SR-IOV Operators might be configured to place the daemons only on nodes labelled as <code class="literal">master</code>. This configuration prevents the PTP and SR-IOV daemons from operating on the worker node. If the PTP and SR-IOV daemon node selectors are incorrectly configured on your system, you must change the daemons before proceeding with the worker DU profile configuration.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check the daemon node selector settings of the PTP Operator on one of the spoke clusters:
						</p><pre class="programlisting language-terminal">$ oc get ptpoperatorconfig/default -n openshift-ptp -ojsonpath='{.spec}' | jq</pre><div class="formalpara"><p class="title"><strong>Example output for PTP Operator</strong></p><p>
								
<pre class="programlisting language-json">{"daemonNodeSelector":{"node-role.kubernetes.io/master":""}} <span id="CO110-1"><!--Empty--></span><span class="callout">1</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO110-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									If the node selector is set to <code class="literal">master</code>, the spoke was deployed with the version of the GitOps ZTP plugin that requires changes.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Check the daemon node selector settings of the SR-IOV Operator on one of the spoke clusters:
						</p><pre class="programlisting language-terminal">$  oc get sriovoperatorconfig/default -n \
openshift-sriov-network-operator -ojsonpath='{.spec}' | jq</pre><div class="formalpara"><p class="title"><strong>Example output for SR-IOV Operator</strong></p><p>
								
<pre class="programlisting language-json">{"configDaemonNodeSelector":{"node-role.kubernetes.io/worker":""},"disableDrain":false,"enableInjector":true,"enableOperatorWebhook":true} <span id="CO111-1"><!--Empty--></span><span class="callout">1</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO111-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									If the node selector is set to <code class="literal">master</code>, the spoke was deployed with the version of the GitOps ZTP plugin that requires changes.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							In the group policy, add the following <code class="literal">complianceType</code> and <code class="literal">spec</code> entries:
						</p><pre class="programlisting language-yaml">spec:
    - fileName: PtpOperatorConfig.yaml
      policyName: "config-policy"
      complianceType: mustonlyhave
      spec:
        daemonNodeSelector:
          node-role.kubernetes.io/worker: ""
    - fileName: SriovOperatorConfig.yaml
      policyName: "config-policy"
      complianceType: mustonlyhave
      spec:
        configDaemonNodeSelector:
          node-role.kubernetes.io/worker: ""</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Changing the <code class="literal">daemonNodeSelector</code> field causes temporary PTP synchronization loss and SR-IOV connectivity loss.
							</p></div></div></li><li class="listitem">
							Commit the changes in Git, and then push to the Git repository being monitored by the GitOps ZTP ArgoCD application.
						</li></ol></div></section><section class="section" id="ztp-additional-worker-node-selector-comp_sno-additional-worker"><div class="titlepage"><div><div><h3 class="title">16.13.3. PTP and SR-IOV node selector compatibility</h3></div></div></div><p>
					The PTP configuration resources and SR-IOV network node policies use <code class="literal">node-role.kubernetes.io/master: ""</code> as the node selector. If the additional worker nodes have the same NIC configuration as the control plane node, the policies used to configure the control plane node can be reused for the worker nodes. However, the node selector must be changed to select both node types, for example with the <code class="literal">"node-role.kubernetes.io/worker"</code> label.
				</p></section><section class="section" id="ztp-additional-worker-policies_sno-additional-worker"><div class="titlepage"><div><div><h3 class="title">16.13.4. Using PolicyGenTemplate CRs to apply worker node policies to worker nodes</h3></div></div></div><p>
					You can create policies for worker nodes.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the following policy template:
						</p><pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "example-sno-workers"
  namespace: "example-sno"
spec:
  bindingRules:
    sites: "example-sno" <span id="CO112-1"><!--Empty--></span><span class="callout">1</span>
  mcp: "worker" <span id="CO112-2"><!--Empty--></span><span class="callout">2</span>
  sourceFiles:
    - fileName: MachineConfigGeneric.yaml <span id="CO112-3"><!--Empty--></span><span class="callout">3</span>
      policyName: "config-policy"
      metadata:
        labels:
          machineconfiguration.openshift.io/role: worker
        name: enable-workload-partitioning
      spec:
        config:
          storage:
            files:
            - contents:
                source: data:text/plain;charset=utf-8;base64,W2NyaW8ucnVudGltZS53b3JrbG9hZHMubWFuYWdlbWVudF0KYWN0aXZhdGlvbl9hbm5vdGF0aW9uID0gInRhcmdldC53b3JrbG9hZC5vcGVuc2hpZnQuaW8vbWFuYWdlbWVudCIKYW5ub3RhdGlvbl9wcmVmaXggPSAicmVzb3VyY2VzLndvcmtsb2FkLm9wZW5zaGlmdC5pbyIKcmVzb3VyY2VzID0geyAiY3B1c2hhcmVzIiA9IDAsICJjcHVzZXQiID0gIjAtMyIgfQo=
              mode: 420
              overwrite: true
              path: /etc/crio/crio.conf.d/01-workload-partitioning
              user:
                name: root
            - contents:
                source: data:text/plain;charset=utf-8;base64,ewogICJtYW5hZ2VtZW50IjogewogICAgImNwdXNldCI6ICIwLTMiCiAgfQp9Cg==
              mode: 420
              overwrite: true
              path: /etc/kubernetes/openshift-workload-pinning
              user:
                name: root
    - fileName: PerformanceProfile.yaml
      policyName: "config-policy"
      metadata:
        name: openshift-worker-node-performance-profile
      spec:
        cpu: <span id="CO112-4"><!--Empty--></span><span class="callout">4</span>
          isolated: "4-47"
          reserved: "0-3"
        hugepages:
          defaultHugepagesSize: 1G
          pages:
            - size: 1G
              count: 32
        realTimeKernel:
          enabled: true
    - fileName: TunedPerformancePatch.yaml
      policyName: "config-policy"
      metadata:
        name: performance-patch-worker
      spec:
        profile:
          - name: performance-patch-worker
            data: |
              [main]
              summary=Configuration changes profile inherited from performance created tuned
              include=openshift-node-performance-openshift-worker-node-performance-profile
              [bootloader]
              cmdline_crash=nohz_full=4-47 <span id="CO112-5"><!--Empty--></span><span class="callout">5</span>
              [sysctl]
              kernel.timer_migration=1
              [scheduler]
              group.ice-ptp=0:f:10:*:ice-ptp.*
              [service]
              service.stalld=start,enable
              service.chronyd=stop,disable
        recommend:
        - profile: performance-patch-worker</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO112-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The policies are applied to all clusters with this label.
								</div></dd><dt><a href="#CO112-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The <code class="literal">MCP</code> field must be set to <code class="literal">worker</code>.
								</div></dd><dt><a href="#CO112-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									This generic <code class="literal">MachineConfig</code> CR is used to configure workload partitioning on the worker node.
								</div></dd><dt><a href="#CO112-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The <code class="literal">cpu.isolated</code> and <code class="literal">cpu.reserved</code> fields must be configured for each particular hardware platform.
								</div></dd><dt><a href="#CO112-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									The <code class="literal">cmdline_crash</code> CPU set must match the <code class="literal">cpu.isolated</code> set in the <code class="literal">PerformanceProfile</code> section.
								</div></dd></dl></div><p class="simpara">
							A generic <code class="literal">MachineConfig</code> CR is used to configure workload partitioning on the worker node. You can generate the content of <code class="literal">crio</code> and <code class="literal">kubelet</code> configuration files.
						</p></li><li class="listitem">
							Add the created policy template to the Git repository monitored by the ArgoCD <code class="literal">policies</code> application.
						</li><li class="listitem">
							Add the policy in the <code class="literal">kustomization.yaml</code> file.
						</li><li class="listitem">
							Commit the changes in Git, and then push to the Git repository being monitored by the GitOps ZTP ArgoCD application.
						</li><li class="listitem"><p class="simpara">
							To remediate the new policies to your spoke cluster, create a TALM custom resource:
						</p><pre class="programlisting language-terminal">$ cat &lt;&lt;EOF | oc apply -f -
apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: example-sno-worker-policies
  namespace: default
spec:
  backup: false
  clusters:
  - example-sno
  enable: true
  managedPolicies:
  - group-du-sno-config-policy
  - example-sno-workers-config-policy
  - example-sno-config-policy
  preCaching: false
  remediationStrategy:
    maxConcurrency: 1
EOF</pre></li></ol></div></section><section class="section" id="ztp-additional-worker-sno-proc_sno-additional-worker"><div class="titlepage"><div><div><h3 class="title">16.13.5. Adding worker nodes to single-node OpenShift clusters with GitOps ZTP</h3></div></div></div><p>
					You can add one or more worker nodes to existing single-node OpenShift clusters to increase available CPU resources in the cluster.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install and configure RHACM 2.6 or later in an OpenShift Container Platform 4.11 or later bare-metal hub cluster
						</li><li class="listitem">
							Install Topology Aware Lifecycle Manager in the hub cluster
						</li><li class="listitem">
							Install Red Hat OpenShift GitOps in the hub cluster
						</li><li class="listitem">
							Use the GitOps ZTP <code class="literal">ztp-site-generate</code> container image version 4.12 or later
						</li><li class="listitem">
							Deploy a managed single-node OpenShift cluster with GitOps ZTP
						</li><li class="listitem">
							Configure the Central Infrastructure Management as described in the RHACM documentation
						</li><li class="listitem">
							Configure the DNS serving the cluster to resolve the internal API endpoint <code class="literal">api-int.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							If you deployed your cluster by using the <code class="literal">example-sno.yaml</code> <code class="literal">SiteConfig</code> manifest, add your new worker node to the <code class="literal">spec.clusters['example-sno'].nodes</code> list:
						</p><pre class="programlisting language-yaml">nodes:
- hostName: "example-node2.example.com"
  role: "worker"
  bmcAddress: "idrac-virtualmedia+https://[1111:2222:3333:4444::bbbb:1]/redfish/v1/Systems/System.Embedded.1"
  bmcCredentialsName:
    name: "example-node2-bmh-secret"
  bootMACAddress: "AA:BB:CC:DD:EE:11"
  bootMode: "UEFI"
  nodeNetwork:
    interfaces:
      - name: eno1
        macAddress: "AA:BB:CC:DD:EE:11"
    config:
      interfaces:
        - name: eno1
          type: ethernet
          state: up
          macAddress: "AA:BB:CC:DD:EE:11"
          ipv4:
            enabled: false
          ipv6:
            enabled: true
            address:
            - ip: 1111:2222:3333:4444::1
              prefix-length: 64
      dns-resolver:
        config:
          search:
          - example.com
          server:
          - 1111:2222:3333:4444::2
      routes:
        config:
        - destination: ::/0
          next-hop-interface: eno1
          next-hop-address: 1111:2222:3333:4444::1
          table-id: 254</pre></li><li class="listitem"><p class="simpara">
							Create a BMC authentication secret for the new host, as referenced by the <code class="literal">bmcCredentialsName</code> field in the <code class="literal">spec.nodes</code> section of your <code class="literal">SiteConfig</code> file:
						</p><pre class="programlisting language-yaml">apiVersion: v1
data:
  password: "password"
  username: "username"
kind: Secret
metadata:
  name: "example-node2-bmh-secret"
  namespace: example-sno
type: Opaque</pre></li><li class="listitem"><p class="simpara">
							Commit the changes in Git, and then push to the Git repository that is being monitored by the GitOps ZTP ArgoCD application.
						</p><p class="simpara">
							When the ArgoCD <code class="literal">cluster</code> application synchronizes, two new manifests appear on the hub cluster generated by the GitOps ZTP plugin:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">BareMetalHost</code>
								</li><li class="listitem"><p class="simpara">
									<code class="literal">NMStateConfig</code>
								</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
										The <code class="literal">cpuset</code> field should not be configured for the worker node. Workload partitioning for worker nodes is added through management policies after the node installation is complete.
									</p></div></div></li></ul></div></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						You can monitor the installation process in several ways.
					</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Check if the preprovisioning images are created by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get ppimg -n example-sno</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAMESPACE       NAME            READY   REASON
example-sno     example-sno     True    ImageCreated
example-sno     example-node2   True    ImageCreated</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check the state of the bare-metal hosts:
						</p><pre class="programlisting language-terminal">$ oc get bmh -n example-sno</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME            STATE          CONSUMER   ONLINE   ERROR   AGE
example-sno     provisioned               true             69m
example-node2   provisioning              true             4m50s <span id="CO113-1"><!--Empty--></span><span class="callout">1</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO113-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal">provisioning</code> state indicates that node booting from the installation media is in progress.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Continuously monitor the installation process:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Watch the agent install process by running the following command:
								</p><pre class="programlisting language-terminal">$ oc get agent -n example-sno --watch</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                   CLUSTER   APPROVED   ROLE     STAGE
671bc05d-5358-8940-ec12-d9ad22804faa   example-sno   true       master   Done
[...]
14fd821b-a35d-9cba-7978-00ddf535ff37   example-sno   true       worker   Starting installation
14fd821b-a35d-9cba-7978-00ddf535ff37   example-sno   true       worker   Installing
14fd821b-a35d-9cba-7978-00ddf535ff37   example-sno   true       worker   Writing image to disk
[...]
14fd821b-a35d-9cba-7978-00ddf535ff37   example-sno   true       worker   Waiting for control plane
[...]
14fd821b-a35d-9cba-7978-00ddf535ff37   example-sno   true       worker   Rebooting
14fd821b-a35d-9cba-7978-00ddf535ff37   example-sno   true       worker   Done</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									When the worker node installation is finished, the worker node certificates are approved automatically. At this point, the worker appears in the <code class="literal">ManagedClusterInfo</code> status. Run the following command to see the status:
								</p><pre class="programlisting language-terminal">$ oc get managedclusterinfo/example-sno -n example-sno -o \
jsonpath='{range .status.nodeList[*]}{.name}{"\t"}{.conditions}{"\t"}{.labels}{"\n"}{end}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">example-sno	[{"status":"True","type":"Ready"}]	{"node-role.kubernetes.io/master":"","node-role.kubernetes.io/worker":""}
example-node2	[{"status":"True","type":"Ready"}]	{"node-role.kubernetes.io/worker":""}</pre>

									</p></div></li></ol></div></li></ul></div></section></section><section class="section" id="ztp-pre-staging-tool"><div class="titlepage"><div><div><h2 class="title">16.14. Pre-caching images for single-node OpenShift deployments</h2></div></div></div><p>
				In environments with limited bandwidth where you use the GitOps Zero Touch Provisioning (ZTP) solution to deploy a large number of clusters, you want to avoid downloading all the images that are required for bootstrapping and installing OpenShift Container Platform. The limited bandwidth at remote single-node OpenShift sites can cause long deployment times. The factory-precaching-cli tool allows you to pre-stage servers before shipping them to the remote site for ZTP provisioning.
			</p><p>
				The factory-precaching-cli tool does the following:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Downloads the RHCOS rootfs image that is required by the minimal ISO to boot.
					</li><li class="listitem">
						Creates a partition from the installation disk labelled as <code class="literal">data</code>.
					</li><li class="listitem">
						Formats the disk in xfs.
					</li><li class="listitem">
						Creates a GUID Partition Table (GPT) data partition at the end of the disk, where the size of the partition is configurable by the tool.
					</li><li class="listitem">
						Copies the container images required to install OpenShift Container Platform.
					</li><li class="listitem">
						Copies the container images required by ZTP to install OpenShift Container Platform.
					</li><li class="listitem">
						Optional: Copies Day-2 Operators to the partition.
					</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					The factory-precaching-cli tool is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
				</p><p>
					For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
				</p></div></div><section class="section" id="ztp-getting-tool_pre-caching"><div class="titlepage"><div><div><h3 class="title">16.14.1. Getting the factory-precaching-cli tool</h3></div></div></div><p>
					The factory-precaching-cli tool Go binary is publicly available in <a class="link" href="https://quay.io/openshift-kni/telco-ran-tools:latest">the Telco RAN tools container image</a>. The factory-precaching-cli tool Go binary in the container image is executed on the server running an RHCOS live image using <code class="literal">podman</code>. If you are working in a disconnected environment or have a private registry, you need to copy the image there so you can download the image to the server.
				</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Pull the factory-precaching-cli tool image by running the following command:
						</p><pre class="programlisting language-terminal"># podman pull quay.io/openshift-kni/telco-ran-tools:latest</pre></li></ul></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To check that the tool is available, query the current version of the factory-precaching-cli tool Go binary:
						</p><pre class="programlisting language-terminal"># podman run quay.io/openshift-kni/telco-ran-tools:latest -- factory-precaching-cli -v</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">factory-precaching-cli version 20221018.120852+main.feecf17</pre>

							</p></div></li></ul></div></section><section class="section" id="ztp-booting-from-live-os_pre-caching"><div class="titlepage"><div><div><h3 class="title">16.14.2. Booting from a live operating system image</h3></div></div></div><p>
					You can use the factory-precaching-cli tool with to boot servers where only one disk is available and external disk drive cannot be attached to the server.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						RHCOS requires the disk to not be in use when the disk is about to be written with an RHCOS image.
					</p></div></div><p>
					Depending on the server hardware, you can mount the RHCOS live ISO on the blank server using one of the following methods:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Using the Dell RACADM tool on a Dell server.
						</li><li class="listitem">
							Using the HPONCFG tool on a HP server.
						</li><li class="listitem">
							Using the Redfish BMC API.
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						It is recommended to automate the mounting procedure. To automate the procedure, you need to pull the required images and host them on a local HTTP server.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You powered up the host.
						</li><li class="listitem">
							You have network connectivity to the host.
						</li></ul></div><div class="admonition note"><div class="admonition_header">Procedure</div><div><p>
						This example procedure uses the Redfish BMC API to mount the RHCOS live ISO.
					</p></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Mount the RHCOS live ISO:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Check virtual media status:
								</p><pre class="programlisting language-terminal">$ curl --globoff -H "Content-Type: application/json" -H \
"Accept: application/json" -k -X GET --user ${username_password} \
https://$BMC_ADDRESS/redfish/v1/Managers/Self/VirtualMedia/1 | python -m json.tool</pre></li><li class="listitem"><p class="simpara">
									Mount the ISO file as a virtual media:
								</p><pre class="programlisting language-terminal">$ curl --globoff -L -w "%{http_code} %{url_effective}\\n" -ku ${username_password} -H "Content-Type: application/json" -H "Accept: application/json" -d '{"Image": "http://[$HTTPd_IP]/RHCOS-live.iso"}' -X POST https://$BMC_ADDRESS/redfish/v1/Managers/Self/VirtualMedia/1/Actions/VirtualMedia.InsertMedia</pre></li><li class="listitem"><p class="simpara">
									Set the boot order to boot from the virtual media once:
								</p><pre class="programlisting language-terminal">$ curl --globoff  -L -w "%{http_code} %{url_effective}\\n"  -ku ${username_password}  -H "Content-Type: application/json" -H "Accept: application/json" -d '{"Boot":{ "BootSourceOverrideEnabled": "Once", "BootSourceOverrideTarget": "Cd", "BootSourceOverrideMode": "UEFI"}}' -X PATCH https://$BMC_ADDRESS/redfish/v1/Systems/Self</pre></li></ol></div></li><li class="listitem">
							Reboot and ensure that the server is booting from virtual media.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For more information about the <code class="literal">butane</code> utility, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-special-config-butane-about_installing-customizing">About Butane</a>.
						</li><li class="listitem">
							For more information about creating a custom live RHCOS ISO, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#create-custom-live-rhcos-iso_install-sno-installing-sno-with-the-assisted-installer">Creating a custom live RHCOS ISO for remote server access</a>.
						</li><li class="listitem">
							For more information about using the Dell RACADM tool, see <a class="link" href="https://www.dell.com/support/manuals/en-ie/poweredge-r440/idrac9_6.xx_racadm_pub/supported-racadm-interfaces?guid=guid-a5747353-fc88-4438-b617-c50ca260448e&amp;lang=en-us">Integrated Dell Remote Access Controller 9 RACADM CLI Guide</a>.
						</li><li class="listitem">
							For more information about using the HP HPONCFG tool, see <a class="link" href="https://support.hpe.com/hpesc/public/docDisplay?docId=emr_na-a00007610en_us">Using HPONCFG</a>.
						</li><li class="listitem">
							For more information about using the Redfish BMC API, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#install-booting-from-an-iso-over-http-redfish_install-sno-installing-sno-with-the-assisted-installer">Booting from an HTTP-hosted ISO image using the Redfish API</a>.
						</li></ul></div></section><section class="section" id="ztp-partitioning_pre-caching"><div class="titlepage"><div><div><h3 class="title">16.14.3. Partitioning the disk</h3></div></div></div><p>
					To run the full pre-caching process, you have to boot from a live ISO and use the factory-precaching-cli tool from a container image to partition and pre-cache all the artifacts required.
				</p><p>
					A live ISO or RHCOS live ISO is required because the disk must not be in use when the operating system (RHCOS) is written to the device during the provisioning. Single-disk servers can also be enabled with this procedure.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have a disk that is not partitioned.
						</li><li class="listitem">
							You have access to the <code class="literal">quay.io/openshift-kni/telco-ran-tools:latest</code> image.
						</li><li class="listitem">
							You have enough storage to install OpenShift Container Platform and pre-cache the required images.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Verify that the disk is cleared:
						</p><pre class="programlisting language-terminal"># lsblk</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
loop0     7:0    0  93.8G  0 loop /run/ephemeral
loop1     7:1    0 897.3M  1 loop /sysroot
sr0      11:0    1   999M  0 rom  /run/media/iso
nvme0n1 259:1    0   1.5T  0 disk</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Erase any file system, RAID or partition table signatures from the device:
						</p><pre class="programlisting language-terminal"># wipefs -a /dev/nvme0n1</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">/dev/nvme0n1: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54
/dev/nvme0n1: 8 bytes were erased at offset 0x1749a955e00 (gpt): 45 46 49 20 50 41 52 54
/dev/nvme0n1: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa</pre>

							</p></div></li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The tool fails if the disk is not empty because it uses partition number 1 of the device for pre-caching the artifacts.
					</p></div></div><section class="section" id="ztp-create-partition_pre-caching"><div class="titlepage"><div><div><h4 class="title">16.14.3.1. Creating the partition</h4></div></div></div><p>
						Once the device is ready, you create a single partition and a GPT partition table. The partition is automatically labelled as <code class="literal">data</code> and created at the end of the device. Otherwise, the partition will be overridden by the <code class="literal">coreos-installer</code>.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							The <code class="literal">coreos-installer</code> requires the partition to be created at the end of the device and to be labelled as <code class="literal">data</code>. Both requirements are necessary to save the partition when writing the RHCOS image to the disk.
						</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								The container must run as <code class="literal">privileged</code> due to formatting host devices.
							</li><li class="listitem">
								You have to mount the <code class="literal">/dev</code> folder so that the process can be executed inside the container.
							</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							In the following example, the size of the partition is 250 GiB due to allow pre-caching the DU profile for Day 2 Operators.
						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Run the container as <code class="literal">privileged</code> and partition the disk:
							</p><pre class="programlisting language-terminal"># podman run -v /dev:/dev --privileged \
--rm quay.io/openshift-kni/telco-ran-tools:latest -- \
factory-precaching-cli partition \ <span id="CO114-1"><!--Empty--></span><span class="callout">1</span>
-d /dev/nvme0n1 \ <span id="CO114-2"><!--Empty--></span><span class="callout">2</span>
-s 250 <span id="CO114-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO114-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specifies the partitioning function of the factory-precaching-cli tool.
									</div></dd><dt><a href="#CO114-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Defines the root directory on the disk.
									</div></dd><dt><a href="#CO114-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Defines the size of the disk in GB.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Check the storage information:
							</p><pre class="programlisting language-terminal"># lsblk</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
loop0         7:0    0  93.8G  0 loop /run/ephemeral
loop1         7:1    0 897.3M  1 loop /sysroot
sr0          11:0    1   999M  0 rom  /run/media/iso
nvme0n1     259:1    0   1.5T  0 disk
└─nvme0n1p1 259:3    0   250G  0 part</pre>

								</p></div></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
							You must verify that the following requirements are met:
						</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								The device has a GPT partition table
							</li><li class="listitem">
								The partition uses the latest sectors of the device.
							</li><li class="listitem">
								The partition is correctly labeled as <code class="literal">data</code>.
							</li></ul></div><p>
						Query the disk status to verify that the disk is partitioned as expected:
					</p><pre class="programlisting language-terminal"># gdisk -l /dev/nvme0n1</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">GPT fdisk (gdisk) version 1.0.3

Partition table scan:
  MBR: protective
  BSD: not present
  APM: not present
  GPT: present

Found valid GPT with protective MBR; using GPT.
Disk /dev/nvme0n1: 3125627568 sectors, 1.5 TiB
Model: Dell Express Flash PM1725b 1.6TB SFF
Sector size (logical/physical): 512/512 bytes
Disk identifier (GUID): CB5A9D44-9B3C-4174-A5C1-C64957910B61
Partition table holds up to 128 entries
Main partition table begins at sector 2 and ends at sector 33
First usable sector is 34, last usable sector is 3125627534
Partitions will be aligned on 2048-sector boundaries
Total free space is 2601338846 sectors (1.2 TiB)

Number  Start (sector)    End (sector)  Size       Code  Name
   1      2601338880      3125627534   250.0 GiB   8300  data</pre>

						</p></div></section><section class="section" id="ztp-mount-partition_pre-caching"><div class="titlepage"><div><div><h4 class="title">16.14.3.2. Mounting the partition</h4></div></div></div><p>
						After verifying that the disk is partitioned correctly, you can mount the device into <code class="literal">/mnt</code>.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							It is recommended to mount the device into <code class="literal">/mnt</code> because that mounting point is used during GitOps ZTP preparation.
						</p></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Verify that the partition is formatted as <code class="literal">xfs</code>:
							</p><pre class="programlisting language-terminal"># lsblk -f /dev/nvme0n1</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME        FSTYPE LABEL UUID                                 MOUNTPOINT
nvme0n1
└─nvme0n1p1 xfs          1bee8ea4-d6cf-4339-b690-a76594794071</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Mount the partition:
							</p><pre class="programlisting language-terminal"># mount /dev/nvme0n1p1 /mnt/</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Check that the partition is mounted:
							</p><pre class="programlisting language-terminal"># lsblk</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
loop0         7:0    0  93.8G  0 loop /run/ephemeral
loop1         7:1    0 897.3M  1 loop /sysroot
sr0          11:0    1   999M  0 rom  /run/media/iso
nvme0n1     259:1    0   1.5T  0 disk
└─nvme0n1p1 259:2    0   250G  0 part /var/mnt <span id="CO115-1"><!--Empty--></span><span class="callout">1</span></pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO115-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The mount point is <code class="literal">/var/mnt</code> because the <code class="literal">/mnt</code> folder in RHCOS is a link to <code class="literal">/var/mnt</code>.
									</div></dd></dl></div></li></ul></div></section></section><section class="section" id="ztp-downloading-images_pre-caching"><div class="titlepage"><div><div><h3 class="title">16.14.4. Downloading the images</h3></div></div></div><p>
					The factory-precaching-cli tool allows you to download the following images to your partitioned server:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							OpenShift Container Platform images
						</li><li class="listitem">
							Operator images that are included in the distributed unit (DU) profile for 5G RAN sites
						</li><li class="listitem">
							Operator images from disconnected registries
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The list of available Operator images can vary in different OpenShift Container Platform releases.
					</p></div></div><section class="section" id="ztp-downloading-images-parallel-workers_pre-caching"><div class="titlepage"><div><div><h4 class="title">16.14.4.1. Downloading with parallel workers</h4></div></div></div><p>
						The factory-precaching-cli tool uses parallel workers to download multiple images simultaneously. You can configure the number of workers with the <code class="literal">--parallel</code> or <code class="literal">-p</code> option. The default number is set to 80% of the available CPUs to the server.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Your login shell may be restricted to a subset of CPUs, which reduces the CPUs available to the container. To remove this restriction, you can precede your commands with <code class="literal">taskset 0xffffffff</code>, for example:
						</p><pre class="programlisting language-terminal"># taskset 0xffffffff podman run --rm quay.io/openshift-kni/telco-ran-tools:latest factory-precaching-cli download --help</pre></div></div></section><section class="section" id="ztp-preparing-ocp-images_pre-caching"><div class="titlepage"><div><div><h4 class="title">16.14.4.2. Preparing to download the OpenShift Container Platform images</h4></div></div></div><p>
						To download OpenShift Container Platform container images, you need to know the multicluster engine (MCE) version. When you use the <code class="literal">--du-profile</code> flag, you also need to specify the Red Hat Advanced Cluster Management (RHACM) version running in the hub cluster that is going to provision the single-node OpenShift.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have RHACM and MCE installed.
							</li><li class="listitem">
								You partitioned the storage device.
							</li><li class="listitem">
								You have enough space for the images on the partitioned device.
							</li><li class="listitem">
								You connected the bare-metal server to the Internet.
							</li><li class="listitem">
								You have a valid pull secret.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Check the RHACM and MCE version by running the following commands in the hub cluster:
							</p><pre class="programlisting language-terminal">$ oc get csv -A | grep -i advanced-cluster-management</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">open-cluster-management                            advanced-cluster-management.v2.6.3           Advanced Cluster Management for Kubernetes   2.6.3                 advanced-cluster-management.v2.6.3                Succeeded</pre>

								</p></div><pre class="programlisting language-terminal">$ oc get csv -A | grep -i multicluster-engine</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">multicluster-engine                                cluster-group-upgrades-operator.v0.0.3       cluster-group-upgrades-operator              0.0.3                                                                   Pending
multicluster-engine                                multicluster-engine.v2.1.4                   multicluster engine for Kubernetes           2.1.4                 multicluster-engine.v2.0.3                        Succeeded
multicluster-engine                                openshift-gitops-operator.v1.5.7             Red Hat OpenShift GitOps                     1.5.7                 openshift-gitops-operator.v1.5.6-0.1664915551.p   Succeeded
multicluster-engine                                openshift-pipelines-operator-rh.v1.6.4       Red Hat OpenShift Pipelines                  1.6.4                 openshift-pipelines-operator-rh.v1.6.3            Succeeded</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								To access the container registry, copy a valid pull secret on the server to be installed:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Create the <code class="literal">.docker</code> folder:
									</p><pre class="programlisting language-terminal">$ mkdir /root/.docker</pre></li><li class="listitem"><p class="simpara">
										Copy the valid pull in the <code class="literal">config.json</code> file to the previously created <code class="literal">.docker/</code> folder:
									</p><pre class="programlisting language-terminal">$ cp config.json /root/.docker/config.json <span id="CO116-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO116-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												<code class="literal">/root/.docker/config.json</code> is the default path where <code class="literal">podman</code> checks for the login credentials for the registry.
											</div></dd></dl></div></li></ol></div></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If you use a different registry to pull the required artifacts, you need to copy the proper pull secret. If the local registry uses TLS, you need to include the certificates from the registry as well.
						</p></div></div></section><section class="section" id="ztp-downloading-ocp-images_pre-caching"><div class="titlepage"><div><div><h4 class="title">16.14.4.3. Downloading the OpenShift Container Platform images</h4></div></div></div><p>
						The factory-precaching-cli tool allows you to pre-cache all the container images required to provision a specific OpenShift Container Platform release.
					</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Pre-cache the release by running the following command:
							</p><pre class="programlisting language-terminal"># podman run -v /mnt:/mnt -v /root/.docker:/root/.docker --privileged --rm quay.io/openshift-kni/telco-ran-tools -- \
   factory-precaching-cli download \ <span id="CO117-1"><!--Empty--></span><span class="callout">1</span>
   -r 4.13.0 \ <span id="CO117-2"><!--Empty--></span><span class="callout">2</span>
   --acm-version 2.6.3 \ <span id="CO117-3"><!--Empty--></span><span class="callout">3</span>
   --mce-version 2.1.4 \ <span id="CO117-4"><!--Empty--></span><span class="callout">4</span>
   -f /mnt \ <span id="CO117-5"><!--Empty--></span><span class="callout">5</span>
   --img quay.io/custom/repository <span id="CO117-6"><!--Empty--></span><span class="callout">6</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO117-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specifies the downloading function of the factory-precaching-cli tool.
									</div></dd><dt><a href="#CO117-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Defines the OpenShift Container Platform release version.
									</div></dd><dt><a href="#CO117-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Defines the RHACM version.
									</div></dd><dt><a href="#CO117-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										Defines the MCE version.
									</div></dd><dt><a href="#CO117-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										Defines the folder where you want to download the images on the disk.
									</div></dd><dt><a href="#CO117-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										Optional. Defines the repository where you store your additional images. These images are downloaded and pre-cached on the disk.
									</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Generated /mnt/imageset.yaml
Generating list of pre-cached artifacts...
Processing artifact [1/176]: ocp-v4.0-art-dev@sha256_6ac2b96bf4899c01a87366fd0feae9f57b1b61878e3b5823da0c3f34f707fbf5
Processing artifact [2/176]: ocp-v4.0-art-dev@sha256_f48b68d5960ba903a0d018a10544ae08db5802e21c2fa5615a14fc58b1c1657c
Processing artifact [3/176]: ocp-v4.0-art-dev@sha256_a480390e91b1c07e10091c3da2257180654f6b2a735a4ad4c3b69dbdb77bbc06
Processing artifact [4/176]: ocp-v4.0-art-dev@sha256_ecc5d8dbd77e326dba6594ff8c2d091eefbc4d90c963a9a85b0b2f0e6155f995
Processing artifact [5/176]: ocp-v4.0-art-dev@sha256_274b6d561558a2f54db08ea96df9892315bb773fc203b1dbcea418d20f4c7ad1
Processing artifact [6/176]: ocp-v4.0-art-dev@sha256_e142bf5020f5ca0d1bdda0026bf97f89b72d21a97c9cc2dc71bf85050e822bbf
...
Processing artifact [175/176]: ocp-v4.0-art-dev@sha256_16cd7eda26f0fb0fc965a589e1e96ff8577e560fcd14f06b5fda1643036ed6c8
Processing artifact [176/176]: ocp-v4.0-art-dev@sha256_cf4d862b4a4170d4f611b39d06c31c97658e309724f9788e155999ae51e7188f
...
Summary:

Release:                            4.13.0
Hub Version:                        2.6.3
ACM Version:                        2.6.3
MCE Version:                        2.1.4
Include DU Profile:                 No
Workers:                            83</pre>

								</p></div></li></ul></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Check that all the images are compressed in the target folder of server:
							</p><pre class="programlisting language-terminal">$ ls -l /mnt <span id="CO118-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO118-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										It is recommended that you pre-cache the images in the <code class="literal">/mnt</code> folder.
									</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">-rw-r--r--. 1 root root  136352323 Oct 31 15:19 ocp-v4.0-art-dev@sha256_edec37e7cd8b1611d0031d45e7958361c65e2005f145b471a8108f1b54316c07.tgz
-rw-r--r--. 1 root root  156092894 Oct 31 15:33 ocp-v4.0-art-dev@sha256_ee51b062b9c3c9f4fe77bd5b3cc9a3b12355d040119a1434425a824f137c61a9.tgz
-rw-r--r--. 1 root root  172297800 Oct 31 15:29 ocp-v4.0-art-dev@sha256_ef23d9057c367a36e4a5c4877d23ee097a731e1186ed28a26c8d21501cd82718.tgz
-rw-r--r--. 1 root root  171539614 Oct 31 15:23 ocp-v4.0-art-dev@sha256_f0497bb63ef6834a619d4208be9da459510df697596b891c0c633da144dbb025.tgz
-rw-r--r--. 1 root root  160399150 Oct 31 15:20 ocp-v4.0-art-dev@sha256_f0c339da117cde44c9aae8d0bd054bceb6f19fdb191928f6912a703182330ac2.tgz
-rw-r--r--. 1 root root  175962005 Oct 31 15:17 ocp-v4.0-art-dev@sha256_f19dd2e80fb41ef31d62bb8c08b339c50d193fdb10fc39cc15b353cbbfeb9b24.tgz
-rw-r--r--. 1 root root  174942008 Oct 31 15:33 ocp-v4.0-art-dev@sha256_f1dbb81fa1aa724e96dd2b296b855ff52a565fbef003d08030d63590ae6454df.tgz
-rw-r--r--. 1 root root  246693315 Oct 31 15:31 ocp-v4.0-art-dev@sha256_f44dcf2c94e4fd843cbbf9b11128df2ba856cd813786e42e3da1fdfb0f6ddd01.tgz
-rw-r--r--. 1 root root  170148293 Oct 31 15:00 ocp-v4.0-art-dev@sha256_f48b68d5960ba903a0d018a10544ae08db5802e21c2fa5615a14fc58b1c1657c.tgz
-rw-r--r--. 1 root root  168899617 Oct 31 15:16 ocp-v4.0-art-dev@sha256_f5099b0989120a8d08a963601214b5c5cb23417a707a8624b7eb52ab788a7f75.tgz
-rw-r--r--. 1 root root  176592362 Oct 31 15:05 ocp-v4.0-art-dev@sha256_f68c0e6f5e17b0b0f7ab2d4c39559ea89f900751e64b97cb42311a478338d9c3.tgz
-rw-r--r--. 1 root root  157937478 Oct 31 15:37 ocp-v4.0-art-dev@sha256_f7ba33a6a9db9cfc4b0ab0f368569e19b9fa08f4c01a0d5f6a243d61ab781bd8.tgz
-rw-r--r--. 1 root root  145535253 Oct 31 15:26 ocp-v4.0-art-dev@sha256_f8f098911d670287826e9499806553f7a1dd3e2b5332abbec740008c36e84de5.tgz
-rw-r--r--. 1 root root  158048761 Oct 31 15:40 ocp-v4.0-art-dev@sha256_f914228ddbb99120986262168a705903a9f49724ffa958bb4bf12b2ec1d7fb47.tgz
-rw-r--r--. 1 root root  167914526 Oct 31 15:37 ocp-v4.0-art-dev@sha256_fa3ca9401c7a9efda0502240aeb8d3ae2d239d38890454f17fe5158b62305010.tgz
-rw-r--r--. 1 root root  164432422 Oct 31 15:24 ocp-v4.0-art-dev@sha256_fc4783b446c70df30b3120685254b40ce13ba6a2b0bf8fb1645f116cf6a392f1.tgz
-rw-r--r--. 1 root root  306643814 Oct 31 15:11 troubleshoot@sha256_b86b8aea29a818a9c22944fd18243fa0347c7a2bf1ad8864113ff2bb2d8e0726.tgz</pre>

								</p></div></li></ul></div></section><section class="section" id="ztp-downloading-operator-images_pre-caching"><div class="titlepage"><div><div><h4 class="title">16.14.4.4. Downloading the Operator images</h4></div></div></div><p>
						You can also pre-cache Day-2 Operators used in the 5G Radio Access Network (RAN) Distributed Unit (DU) cluster configuration. The Day-2 Operators depend on the installed OpenShift Container Platform version.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							You need to include the RHACM hub and MCE Operator versions by using the <code class="literal">--acm-version</code> and <code class="literal">--mce-version</code> flags so the factory-precaching-cli tool can pre-cache the appropriate containers images for the RHACM and MCE Operators.
						</p></div></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Pre-cache the Operator images:
							</p><pre class="programlisting language-terminal"># podman run -v /mnt:/mnt -v /root/.docker:/root/.docker --privileged --rm quay.io/openshift-kni/telco-ran-tools:latest -- factory-precaching-cli download \ <span id="CO119-1"><!--Empty--></span><span class="callout">1</span>
   -r 4.13.0 \ <span id="CO119-2"><!--Empty--></span><span class="callout">2</span>
   --acm-version 2.6.3 \ <span id="CO119-3"><!--Empty--></span><span class="callout">3</span>
   --mce-version 2.1.4 \ <span id="CO119-4"><!--Empty--></span><span class="callout">4</span>
   -f /mnt \ <span id="CO119-5"><!--Empty--></span><span class="callout">5</span>
   --img quay.io/custom/repository <span id="CO119-6"><!--Empty--></span><span class="callout">6</span>
   --du-profile -s <span id="CO119-7"><!--Empty--></span><span class="callout">7</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO119-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specifies the downloading function of the factory-precaching-cli tool.
									</div></dd><dt><a href="#CO119-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Defines the OpenShift Container Platform release version.
									</div></dd><dt><a href="#CO119-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Defines the RHACM version.
									</div></dd><dt><a href="#CO119-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										Defines the MCE version.
									</div></dd><dt><a href="#CO119-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										Defines the folder where you want to download the images on the disk.
									</div></dd><dt><a href="#CO119-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										Optional. Defines the repository where you store your additional images. These images are downloaded and pre-cached on the disk.
									</div></dd><dt><a href="#CO119-7"><span class="callout">7</span></a> </dt><dd><div class="para">
										Specifies pre-caching the Operators included in the DU configuration.
									</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Generated /mnt/imageset.yaml
Generating list of pre-cached artifacts...
Processing artifact [1/379]: ocp-v4.0-art-dev@sha256_7753a8d9dd5974be8c90649aadd7c914a3d8a1f1e016774c7ac7c9422e9f9958
Processing artifact [2/379]: ose-kube-rbac-proxy@sha256_c27a7c01e5968aff16b6bb6670423f992d1a1de1a16e7e260d12908d3322431c
Processing artifact [3/379]: ocp-v4.0-art-dev@sha256_370e47a14c798ca3f8707a38b28cfc28114f492bb35fe1112e55d1eb51022c99
...
Processing artifact [378/379]: ose-local-storage-operator@sha256_0c81c2b79f79307305e51ce9d3837657cf9ba5866194e464b4d1b299f85034d0
Processing artifact [379/379]: multicluster-operators-channel-rhel8@sha256_c10f6bbb84fe36e05816e873a72188018856ad6aac6cc16271a1b3966f73ceb3
...
Summary:

Release:                            4.13.0
Hub Version:                        2.6.3
ACM Version:                        2.6.3
MCE Version:                        2.1.4
Include DU Profile:                 Yes
Workers:                            83</pre>

								</p></div></li></ul></div></section><section class="section" id="ztp-custom-pre-caching-in-disconnected-environment_pre-caching"><div class="titlepage"><div><div><h4 class="title">16.14.4.5. Pre-caching custom images in disconnected environments</h4></div></div></div><p>
						The <code class="literal">--generate-imageset</code> argument stops the factory-precaching-cli tool after the <code class="literal">ImageSetConfiguration</code> custom resource (CR) is generated. This allows you to customize the <code class="literal">ImageSetConfiguration</code> CR before downloading any images. After you customized the CR, you can use the <code class="literal">--skip-imageset</code> argument to download the images that you specified in the <code class="literal">ImageSetConfiguration</code> CR.
					</p><p>
						You can customize the <code class="literal">ImageSetConfiguration</code> CR in the following ways:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Add Operators and additional images
							</li><li class="listitem">
								Remove Operators and additional images
							</li><li class="listitem">
								Change Operator and catalog sources to local or disconnected registries
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Pre-cache the images:
							</p><pre class="programlisting language-terminal"># podman run -v /mnt:/mnt -v /root/.docker:/root/.docker --privileged --rm quay.io/openshift-kni/telco-ran-tools:latest -- factory-precaching-cli download \ <span id="CO120-1"><!--Empty--></span><span class="callout">1</span>
   -r 4.13.0 \ <span id="CO120-2"><!--Empty--></span><span class="callout">2</span>
   --acm-version 2.6.3 \ <span id="CO120-3"><!--Empty--></span><span class="callout">3</span>
   --mce-version 2.1.4 \ <span id="CO120-4"><!--Empty--></span><span class="callout">4</span>
   -f /mnt \ <span id="CO120-5"><!--Empty--></span><span class="callout">5</span>
   --img quay.io/custom/repository <span id="CO120-6"><!--Empty--></span><span class="callout">6</span>
   --du-profile -s \ <span id="CO120-7"><!--Empty--></span><span class="callout">7</span>
   --generate-imageset <span id="CO120-8"><!--Empty--></span><span class="callout">8</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO120-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specifies the downloading function of the factory-precaching-cli tool.
									</div></dd><dt><a href="#CO120-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Defines the OpenShift Container Platform release version.
									</div></dd><dt><a href="#CO120-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Defines the RHACM version.
									</div></dd><dt><a href="#CO120-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										Defines the MCE version.
									</div></dd><dt><a href="#CO120-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										Defines the folder where you want to download the images on the disk.
									</div></dd><dt><a href="#CO120-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										Optional. Defines the repository where you store your additional images. These images are downloaded and pre-cached on the disk.
									</div></dd><dt><a href="#CO120-7"><span class="callout">7</span></a> </dt><dd><div class="para">
										Specifies pre-caching the Operators included in the DU configuration.
									</div></dd><dt><a href="#CO120-8"><span class="callout">8</span></a> </dt><dd><div class="para">
										The <code class="literal">--generate-imageset</code> argument generates the <code class="literal">ImageSetConfiguration</code> CR only, which allows you to customize the CR.
									</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Generated /mnt/imageset.yaml</pre>

								</p></div><div class="formalpara"><p class="title"><strong>Example ImageSetConfiguration CR</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: mirror.openshift.io/v1alpha2
kind: ImageSetConfiguration
mirror:
  platform:
    channels:
    - name: stable-4.13
      minVersion: 4.13.0 <span id="CO121-1"><!--Empty--></span><span class="callout">1</span>
      maxVersion: 4.13.0
  additionalImages:
    - name: quay.io/custom/repository
  operators:
    - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.13
      packages:
        - name: advanced-cluster-management <span id="CO121-2"><!--Empty--></span><span class="callout">2</span>
          channels:
             - name: 'release-2.6'
               minVersion: 2.6.3
               maxVersion: 2.6.3
        - name: multicluster-engine <span id="CO121-3"><!--Empty--></span><span class="callout">3</span>
          channels:
             - name: 'stable-2.1'
               minVersion: 2.1.4
               maxVersion: 2.1.4
        - name: local-storage-operator <span id="CO121-4"><!--Empty--></span><span class="callout">4</span>
          channels:
            - name: 'stable'
        - name: ptp-operator <span id="CO121-5"><!--Empty--></span><span class="callout">5</span>
          channels:
            - name: 'stable'
        - name: sriov-network-operator <span id="CO121-6"><!--Empty--></span><span class="callout">6</span>
          channels:
            - name: 'stable'
        - name: cluster-logging <span id="CO121-7"><!--Empty--></span><span class="callout">7</span>
          channels:
            - name: 'stable'
        - name: lvms-operator <span id="CO121-8"><!--Empty--></span><span class="callout">8</span>
          channels:
            - name: 'stable-4.13'
        - name: amq7-interconnect-operator <span id="CO121-9"><!--Empty--></span><span class="callout">9</span>
          channels:
            - name: '1.10.x'
        - name: bare-metal-event-relay <span id="CO121-10"><!--Empty--></span><span class="callout">10</span>
          channels:
            - name: 'stable'
    - catalog: registry.redhat.io/redhat/certified-operator-index:v4.13
      packages:
        - name: sriov-fec <span id="CO121-11"><!--Empty--></span><span class="callout">11</span>
          channels:
            - name: 'stable'</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO121-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The platform versions match the versions passed to the tool.
									</div></dd><dt><a href="#CO121-2"><span class="callout">2</span></a> <a href="#CO121-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The versions of RHACM and MCE Operators match the versions passed to the tool.
									</div></dd><dt><a href="#CO121-4"><span class="callout">4</span></a> <a href="#CO121-5"><span class="callout">5</span></a> <a href="#CO121-6"><span class="callout">6</span></a> <a href="#CO121-7"><span class="callout">7</span></a> <a href="#CO121-8"><span class="callout">8</span></a> <a href="#CO121-9"><span class="callout">9</span></a> <a href="#CO121-10"><span class="callout">10</span></a> <a href="#CO121-11"><span class="callout">11</span></a> </dt><dd><div class="para">
										The CR contains all the specified DU Operators.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Customize the catalog resource in the CR:
							</p><pre class="programlisting language-yaml">apiVersion: mirror.openshift.io/v1alpha2
kind: ImageSetConfiguration
mirror:
  platform:
[...]
  operators:
    - catalog: eko4.cloud.lab.eng.bos.redhat.com:8443/redhat/certified-operator-index:v4.13
      packages:
        - name: sriov-fec
          channels:
            - name: 'stable'</pre><p class="simpara">
								When you download images by using a local or disconnected registry, you have to first add certificates for the registries that you want to pull the content from.
							</p></li><li class="listitem"><p class="simpara">
								To avoid any errors, copy the registry certificate into your server:
							</p><pre class="programlisting language-terminal"># cp /tmp/eko4-ca.crt /etc/pki/ca-trust/source/anchors/.</pre></li><li class="listitem"><p class="simpara">
								Then, update the certificates trust store:
							</p><pre class="programlisting language-terminal"># update-ca-trust</pre></li><li class="listitem"><p class="simpara">
								Mount the host <code class="literal">/etc/pki</code> folder into the factory-cli image:
							</p><pre class="programlisting language-terminal"># podman run -v /mnt:/mnt -v /root/.docker:/root/.docker -v /etc/pki:/etc/pki --privileged --rm quay.io/openshift-kni/telco-ran-tools:latest -- \
factory-precaching-cli download \ <span id="CO122-1"><!--Empty--></span><span class="callout">1</span>
   -r 4.13.0 \ <span id="CO122-2"><!--Empty--></span><span class="callout">2</span>
   --acm-version 2.6.3 \ <span id="CO122-3"><!--Empty--></span><span class="callout">3</span>
   --mce-version 2.1.4 \ <span id="CO122-4"><!--Empty--></span><span class="callout">4</span>
   -f /mnt \ <span id="CO122-5"><!--Empty--></span><span class="callout">5</span>
   --img quay.io/custom/repository <span id="CO122-6"><!--Empty--></span><span class="callout">6</span>
   --du-profile -s \ <span id="CO122-7"><!--Empty--></span><span class="callout">7</span>
   --skip-imageset <span id="CO122-8"><!--Empty--></span><span class="callout">8</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO122-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specifies the downloading function of the factory-precaching-cli tool.
									</div></dd><dt><a href="#CO122-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Defines the OpenShift Container Platform release version.
									</div></dd><dt><a href="#CO122-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Defines the RHACM version.
									</div></dd><dt><a href="#CO122-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										Defines the MCE version.
									</div></dd><dt><a href="#CO122-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										Defines the folder where you want to download the images on the disk.
									</div></dd><dt><a href="#CO122-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										Optional. Defines the repository where you store your additional images. These images are downloaded and pre-cached on the disk.
									</div></dd><dt><a href="#CO122-7"><span class="callout">7</span></a> </dt><dd><div class="para">
										Specifies pre-caching the Operators included in the DU configuration.
									</div></dd><dt><a href="#CO122-8"><span class="callout">8</span></a> </dt><dd><div class="para">
										The <code class="literal">--skip-imageset</code> argument allows you to download the images that you specified in your customized <code class="literal">ImageSetConfiguration</code> CR.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Download the images without generating a new <code class="literal">imageSetConfiguration</code> CR:
							</p><pre class="programlisting language-terminal"># podman run -v /mnt:/mnt -v /root/.docker:/root/.docker --privileged --rm quay.io/openshift-kni/telco-ran-tools:latest -- factory-precaching-cli download -r 4.13.0 \
--acm-version 2.6.3 --mce-version 2.1.4 -f /mnt \
--img quay.io/custom/repository \
--du-profile -s \
--skip-imageset</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								To access the online Red Hat registries, see <a class="link" href="https://console.redhat.com/openshift/downloads#tool-pull-secret">OpenShift installation customization tools</a>.
							</li><li class="listitem">
								For more information about using the multicluster engine, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.8/html/clusters/cluster_mce_overview#mce-intro">About cluster lifecycle with the multicluster engine operator</a>.
							</li></ul></div></section></section><section class="section" id="ztp-pre-caching-config-con_pre-caching"><div class="titlepage"><div><div><h3 class="title">16.14.5. Pre-caching images in GitOps ZTP</h3></div></div></div><p>
					The <code class="literal">SiteConfig</code> manifest defines how an OpenShift cluster is to be installed and configured. In the GitOps Zero Touch Provisioning (ZTP) provisioning workflow, the factory-precaching-cli tool requires the following additional fields in the <code class="literal">SiteConfig</code> manifest:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">clusters.ignitionConfigOverride</code>
						</li><li class="listitem">
							<code class="literal">nodes.installerArgs</code>
						</li><li class="listitem">
							<code class="literal">nodes.ignitionConfigOverride</code>
						</li></ul></div><div class="formalpara"><p class="title"><strong>Example SiteConfig with additional fields</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: ran.openshift.io/v1
kind: SiteConfig
metadata:
  name: "example-5g-lab"
  namespace: "example-5g-lab"
spec:
  baseDomain: "example.domain.redhat.com"
  pullSecretRef:
    name: "assisted-deployment-pull-secret"
  clusterImageSetNameRef: "img4.9.10-x86-64-appsub"
  sshPublicKey: "ssh-rsa ..."
  clusters:
  - clusterName: "sno-worker-0"
    clusterImageSetNameRef: "eko4-img4.11.5-x86-64-appsub"
    clusterLabels:
      group-du-sno: ""
      common-411: true
      sites : "example-5g-lab"
      vendor: "OpenShift"
    clusterNetwork:
      - cidr: 10.128.0.0/14
        hostPrefix: 23
    machineNetwork:
      - cidr: 10.19.32.192/26
    serviceNetwork:
      - 172.30.0.0/16
    networkType: "OVNKubernetes"
    additionalNTPSources:
      - clock.corp.redhat.com
    ignitionConfigOverride: '{"ignition":{"version":"3.1.0"},"systemd":{"units":[{"name":"var-mnt.mount","enabled":true,"contents":"[Unit]\nDescription=Mount partition with artifacts\nBefore=precache-images.service\nBindsTo=precache-images.service\nStopWhenUnneeded=true\n\n[Mount]\nWhat=/dev/disk/by-partlabel/data\nWhere=/var/mnt\nType=xfs\nTimeoutSec=30\n\n[Install]\nRequiredBy=precache-images.service"},{"name":"precache-images.service","enabled":true,"contents":"[Unit]\nDescription=Extracts the precached images in discovery stage\nAfter=var-mnt.mount\nBefore=agent.service\n\n[Service]\nType=oneshot\nUser=root\nWorkingDirectory=/var/mnt\nExecStart=bash /usr/local/bin/extract-ai.sh\n#TimeoutStopSec=30\n\n[Install]\nWantedBy=multi-user.target default.target\nWantedBy=agent.service"}]},"storage":{"files":[{"overwrite":true,"path":"/usr/local/bin/extract-ai.sh","mode":755,"user":{"name":"root"},"contents":{"source":"data:,%23%21%2Fbin%2Fbash%0A%0AFOLDER%3D%22%24%7BFOLDER%3A-%24%28pwd%29%7D%22%0AOCP_RELEASE_LIST%3D%22%24%7BOCP_RELEASE_LIST%3A-ai-images.txt%7D%22%0ABINARY_FOLDER%3D%2Fvar%2Fmnt%0A%0Apushd%20%24FOLDER%0A%0Atotal_copies%3D%24%28sort%20-u%20%24BINARY_FOLDER%2F%24OCP_RELEASE_LIST%20%7C%20wc%20-l%29%20%20%23%20Required%20to%20keep%20track%20of%20the%20pull%20task%20vs%20total%0Acurrent_copy%3D1%0A%0Awhile%20read%20-r%20line%3B%0Ado%0A%20%20uri%3D%24%28echo%20%22%24line%22%20%7C%20awk%20%27%7Bprint%241%7D%27%29%0A%20%20%23tar%3D%24%28echo%20%22%24line%22%20%7C%20awk%20%27%7Bprint%242%7D%27%29%0A%20%20podman%20image%20exists%20%24uri%0A%20%20if%20%5B%5B%20%24%3F%20-eq%200%20%5D%5D%3B%20then%0A%20%20%20%20%20%20echo%20%22Skipping%20existing%20image%20%24tar%22%0A%20%20%20%20%20%20echo%20%22Copying%20%24%7Buri%7D%20%5B%24%7Bcurrent_copy%7D%2F%24%7Btotal_copies%7D%5D%22%0A%20%20%20%20%20%20current_copy%3D%24%28%28current_copy%20%2B%201%29%29%0A%20%20%20%20%20%20continue%0A%20%20fi%0A%20%20tar%3D%24%28echo%20%22%24uri%22%20%7C%20%20rev%20%7C%20cut%20-d%20%22%2F%22%20-f1%20%7C%20rev%20%7C%20tr%20%22%3A%22%20%22_%22%29%0A%20%20tar%20zxvf%20%24%7Btar%7D.tgz%0A%20%20if%20%5B%20%24%3F%20-eq%200%20%5D%3B%20then%20rm%20-f%20%24%7Btar%7D.gz%3B%20fi%0A%20%20echo%20%22Copying%20%24%7Buri%7D%20%5B%24%7Bcurrent_copy%7D%2F%24%7Btotal_copies%7D%5D%22%0A%20%20skopeo%20copy%20dir%3A%2F%2F%24%28pwd%29%2F%24%7Btar%7D%20containers-storage%3A%24%7Buri%7D%0A%20%20if%20%5B%20%24%3F%20-eq%200%20%5D%3B%20then%20rm%20-rf%20%24%7Btar%7D%3B%20current_copy%3D%24%28%28current_copy%20%2B%201%29%29%3B%20fi%0Adone%20%3C%20%24%7BBINARY_FOLDER%7D%2F%24%7BOCP_RELEASE_LIST%7D%0A%0A%23%20workaround%20while%20https%3A%2F%2Fgithub.com%2Fopenshift%2Fassisted-service%2Fpull%2F3546%0A%23cp%20%2Fvar%2Fmnt%2Fmodified-rhcos-4.10.3-x86_64-metal.x86_64.raw.gz%20%2Fvar%2Ftmp%2F.%0A%0Aexit%200"}},{"overwrite":true,"path":"/usr/local/bin/agent-fix-bz1964591","mode":755,"user":{"name":"root"},"contents":{"source":"data:,%23%21%2Fusr%2Fbin%2Fsh%0A%0A%23%20This%20script%20is%20a%20workaround%20for%20bugzilla%201964591%20where%20symlinks%20inside%20%2Fvar%2Flib%2Fcontainers%2F%20get%0A%23%20corrupted%20under%20some%20circumstances.%0A%23%0A%23%20In%20order%20to%20let%20agent.service%20start%20correctly%20we%20are%20checking%20here%20whether%20the%20requested%0A%23%20container%20image%20exists%20and%20in%20case%20%22podman%20images%22%20returns%20an%20error%20we%20try%20removing%20the%20faulty%0A%23%20image.%0A%23%0A%23%20In%20such%20a%20scenario%20agent.service%20will%20detect%20the%20image%20is%20not%20present%20and%20pull%20it%20again.%20In%20case%0A%23%20the%20image%20is%20present%20and%20can%20be%20detected%20correctly%2C%20no%20any%20action%20is%20required.%0A%0AIMAGE%3D%24%28echo%20%241%20%7C%20sed%20%27s%2F%3A.%2A%2F%2F%27%29%0Apodman%20image%20exists%20%24IMAGE%20%7C%7C%20echo%20%22already%20loaded%22%20%7C%7C%20echo%20%22need%20to%20be%20pulled%22%0A%23podman%20images%20%7C%20grep%20%24IMAGE%20%7C%7C%20podman%20rmi%20--force%20%241%20%7C%7C%20true"}}]}}'
    nodes:
      - hostName: "snonode.sno-worker-0.example.domain.redhat.com"
        role: "master"
        bmcAddress: "idrac-virtualmedia+https://10.19.28.53/redfish/v1/Systems/System.Embedded.1"
        bmcCredentialsName:
          name: "worker0-bmh-secret"
        bootMACAddress: "e4:43:4b:bd:90:46"
        bootMode: "UEFI"
        rootDeviceHints:
          deviceName: /dev/nvme0n1
        cpuset: "0-1,40-41"
        installerArgs: '["--save-partlabel", "data"]'
        ignitionConfigOverride: '{"ignition":{"version":"3.1.0"},"systemd":{"units":[{"name":"var-mnt.mount","enabled":true,"contents":"[Unit]\nDescription=Mount partition with artifacts\nBefore=precache-ocp-images.service\nBindsTo=precache-ocp-images.service\nStopWhenUnneeded=true\n\n[Mount]\nWhat=/dev/disk/by-partlabel/data\nWhere=/var/mnt\nType=xfs\nTimeoutSec=30\n\n[Install]\nRequiredBy=precache-ocp-images.service"},{"name":"precache-ocp-images.service","enabled":true,"contents":"[Unit]\nDescription=Extracts the precached OCP images into containers storage\nAfter=var-mnt.mount\nBefore=machine-config-daemon-pull.service nodeip-configuration.service\n\n[Service]\nType=oneshot\nUser=root\nWorkingDirectory=/var/mnt\nExecStart=bash /usr/local/bin/extract-ocp.sh\nTimeoutStopSec=60\n\n[Install]\nWantedBy=multi-user.target"}]},"storage":{"files":[{"overwrite":true,"path":"/usr/local/bin/extract-ocp.sh","mode":755,"user":{"name":"root"},"contents":{"source":"data:,%23%21%2Fbin%2Fbash%0A%0AFOLDER%3D%22%24%7BFOLDER%3A-%24%28pwd%29%7D%22%0AOCP_RELEASE_LIST%3D%22%24%7BOCP_RELEASE_LIST%3A-ocp-images.txt%7D%22%0ABINARY_FOLDER%3D%2Fvar%2Fmnt%0A%0Apushd%20%24FOLDER%0A%0Atotal_copies%3D%24%28sort%20-u%20%24BINARY_FOLDER%2F%24OCP_RELEASE_LIST%20%7C%20wc%20-l%29%20%20%23%20Required%20to%20keep%20track%20of%20the%20pull%20task%20vs%20total%0Acurrent_copy%3D1%0A%0Awhile%20read%20-r%20line%3B%0Ado%0A%20%20uri%3D%24%28echo%20%22%24line%22%20%7C%20awk%20%27%7Bprint%241%7D%27%29%0A%20%20%23tar%3D%24%28echo%20%22%24line%22%20%7C%20awk%20%27%7Bprint%242%7D%27%29%0A%20%20podman%20image%20exists%20%24uri%0A%20%20if%20%5B%5B%20%24%3F%20-eq%200%20%5D%5D%3B%20then%0A%20%20%20%20%20%20echo%20%22Skipping%20existing%20image%20%24tar%22%0A%20%20%20%20%20%20echo%20%22Copying%20%24%7Buri%7D%20%5B%24%7Bcurrent_copy%7D%2F%24%7Btotal_copies%7D%5D%22%0A%20%20%20%20%20%20current_copy%3D%24%28%28current_copy%20%2B%201%29%29%0A%20%20%20%20%20%20continue%0A%20%20fi%0A%20%20tar%3D%24%28echo%20%22%24uri%22%20%7C%20%20rev%20%7C%20cut%20-d%20%22%2F%22%20-f1%20%7C%20rev%20%7C%20tr%20%22%3A%22%20%22_%22%29%0A%20%20tar%20zxvf%20%24%7Btar%7D.tgz%0A%20%20if%20%5B%20%24%3F%20-eq%200%20%5D%3B%20then%20rm%20-f%20%24%7Btar%7D.gz%3B%20fi%0A%20%20echo%20%22Copying%20%24%7Buri%7D%20%5B%24%7Bcurrent_copy%7D%2F%24%7Btotal_copies%7D%5D%22%0A%20%20skopeo%20copy%20dir%3A%2F%2F%24%28pwd%29%2F%24%7Btar%7D%20containers-storage%3A%24%7Buri%7D%0A%20%20if%20%5B%20%24%3F%20-eq%200%20%5D%3B%20then%20rm%20-rf%20%24%7Btar%7D%3B%20current_copy%3D%24%28%28current_copy%20%2B%201%29%29%3B%20fi%0Adone%20%3C%20%24%7BBINARY_FOLDER%7D%2F%24%7BOCP_RELEASE_LIST%7D%0A%0Aexit%200"}}]}}'
        nodeNetwork:
          config:
            interfaces:
              - name: ens1f0
                type: ethernet
                state: up
                macAddress: "AA:BB:CC:11:22:33"
                ipv4:
                  enabled: true
                  dhcp: true
                ipv6:
                  enabled: false
          interfaces:
            - name: "ens1f0"
              macAddress: "AA:BB:CC:11:22:33"</pre>

					</p></div><section class="section" id="ztp-pre-caching-config-clusters-ignitionconfigoverride_pre-caching"><div class="titlepage"><div><div><h4 class="title">16.14.5.1. Understanding the clusters.ignitionConfigOverride field</h4></div></div></div><p>
						The <code class="literal">clusters.ignitionConfigOverride</code> field adds a configuration in Ignition format during the GitOps ZTP discovery stage. The configuration includes <code class="literal">systemd</code> services in the ISO mounted in virtual media. This way, the scripts are part of the discovery RHCOS live ISO and they can be used to load the Assisted Installer (AI) images.
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">systemd</code> services</span></dt><dd>
									The <code class="literal">systemd</code> services are <code class="literal">var-mnt.mount</code> and <code class="literal">precache-images.services</code>. The <code class="literal">precache-images.service</code> depends on the disk partition to be mounted in <code class="literal">/var/mnt</code> by the <code class="literal">var-mnt.mount</code> unit. The service calls a script called <code class="literal">extract-ai.sh</code>.
								</dd><dt><span class="term"><code class="literal">extract-ai.sh</code></span></dt><dd>
									The <code class="literal">extract-ai.sh</code> script extracts and loads the required images from the disk partition to the local container storage. When the script finishes successfully, you can use the images locally.
								</dd><dt><span class="term"><code class="literal">agent-fix-bz1964591</code></span></dt><dd>
									The <code class="literal">agent-fix-bz1964591</code> script is a workaround for an AI issue. To prevent AI from removing the images, which can force the <code class="literal">agent.service</code> to pull the images again from the registry, the <code class="literal">agent-fix-bz1964591</code> script checks if the requested container images exist.
								</dd></dl></div></section><section class="section" id="ztp-pre-caching-config-nodes-installerargs_pre-caching"><div class="titlepage"><div><div><h4 class="title">16.14.5.2. Understanding the nodes.installerArgs field</h4></div></div></div><p>
						The <code class="literal">nodes.installerArgs</code> field allows you to configure how the <code class="literal">coreos-installer</code> utility writes the RHCOS live ISO to disk. You need to indicate to save the disk partition labeled as <code class="literal">data</code> because the artifacts saved in the <code class="literal">data</code> partition are needed during the OpenShift Container Platform installation stage.
					</p><p>
						The extra parameters are passed directly to the <code class="literal">coreos-installer</code> utility that writes the live RHCOS to disk. On the next reboot, the operating system starts from the disk.
					</p><p>
						You can pass several options to the <code class="literal">coreos-installer</code> utility:
					</p><pre class="programlisting language-terminal">OPTIONS:
...
    -u, --image-url &lt;URL&gt;
            Manually specify the image URL

    -f, --image-file &lt;path&gt;
            Manually specify a local image file

    -i, --ignition-file &lt;path&gt;
            Embed an Ignition config from a file

    -I, --ignition-url &lt;URL&gt;
            Embed an Ignition config from a URL
...
        --save-partlabel &lt;lx&gt;...
            Save partitions with this label glob

        --save-partindex &lt;id&gt;...
            Save partitions with this number or range
...
        --insecure-ignition
            Allow Ignition URL without HTTPS or hash</pre></section><section class="section" id="ztp-pre-caching-config-nodes-ignitionconfigoverride_pre-caching"><div class="titlepage"><div><div><h4 class="title">16.14.5.3. Understanding the nodes.ignitionConfigOverride field</h4></div></div></div><p>
						Similarly to <code class="literal">clusters.ignitionConfigOverride</code>, the <code class="literal">nodes.ignitionConfigOverride</code> field allows the addtion of configurations in Ignition format to the <code class="literal">coreos-installer</code> utility, but at the OpenShift Container Platform installation stage. When the RHCOS is written to disk, the extra configuration included in the GitOps ZTP discovery ISO is no longer available. During the discovery stage, the extra configuration is stored in the memory of the live OS.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							At this stage, the number of container images extracted and loaded is bigger than in the discovery stage. Depending on the OpenShift Container Platform release and whether you install the Day-2 Operators, the installation time can vary.
						</p></div></div><p>
						At the installation stage, the <code class="literal">var-mnt.mount</code> and <code class="literal">precache-ocp.services</code> <code class="literal">systemd</code> services are used.
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">precache-ocp.service</code></span></dt><dd><p class="simpara">
									The <code class="literal">precache-ocp.service</code> depends on the disk partition to be mounted in <code class="literal">/var/mnt</code> by the <code class="literal">var-mnt.mount</code> unit. The <code class="literal">precache-ocp.service</code> service calls a script called <code class="literal">extract-ocp.sh</code>.
								</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
										To extract all the images before the OpenShift Container Platform installation, you must execute <code class="literal">precache-ocp.service</code> before executing the <code class="literal">machine-config-daemon-pull.service</code> and <code class="literal">nodeip-configuration.service</code> services.
									</p></div></div></dd><dt><span class="term"><code class="literal">extract-ocp.sh</code></span></dt><dd>
									The <code class="literal">extract-ocp.sh</code> script extracts and loads the required images from the disk partition to the local container storage. When the script finishes successfully, you can use the images locally.
								</dd></dl></div><p>
						When you upload the <code class="literal">SiteConfig</code> and the optional <code class="literal">PolicyGenTemplates</code> custom resources (CRs) to the Git repo, which Argo CD is monitoring, you can start the GitOps ZTP workflow by syncing the CRs with the hub cluster.
					</p></section></section><section class="section" id="ztp-pre-staging-troubleshooting_pre-caching"><div class="titlepage"><div><div><h3 class="title">16.14.6. Troubleshooting</h3></div></div></div><section class="section" id="rendered-catalog-is-invalid"><div class="titlepage"><div><div><h4 class="title">16.14.6.1. Rendered catalog is invalid</h4></div></div></div><p>
						When you download images by using a local or disconnected registry, you might see the <code class="literal">The rendered catalog is invalid</code> error. This means that you are missing certificates of the new registry you want to pull content from.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The factory-precaching-cli tool image is built on a UBI RHEL image. Certificate paths and locations are the same on RHCOS.
						</p></div></div><div class="formalpara"><p class="title"><strong>Example error</strong></p><p>
							
<pre class="programlisting language-terminal">Generating list of pre-cached artifacts...
error: unable to run command oc-mirror -c /mnt/imageset.yaml file:///tmp/fp-cli-3218002584/mirror --ignore-history --dry-run: Creating directory: /tmp/fp-cli-3218002584/mirror/oc-mirror-workspace/src/publish
Creating directory: /tmp/fp-cli-3218002584/mirror/oc-mirror-workspace/src/v2
Creating directory: /tmp/fp-cli-3218002584/mirror/oc-mirror-workspace/src/charts
Creating directory: /tmp/fp-cli-3218002584/mirror/oc-mirror-workspace/src/release-signatures
backend is not configured in /mnt/imageset.yaml, using stateless mode
backend is not configured in /mnt/imageset.yaml, using stateless mode
No metadata detected, creating new workspace
level=info msg=trying next host error=failed to do request: Head "https://eko4.cloud.lab.eng.bos.redhat.com:8443/v2/redhat/redhat-operator-index/manifests/v4.11": x509: certificate signed by unknown authority host=eko4.cloud.lab.eng.bos.redhat.com:8443

The rendered catalog is invalid.

Run "oc-mirror list operators --catalog CATALOG-NAME --package PACKAGE-NAME" for more information.

error: error rendering new refs: render reference "eko4.cloud.lab.eng.bos.redhat.com:8443/redhat/redhat-operator-index:v4.11": error resolving name : failed to do request: Head "https://eko4.cloud.lab.eng.bos.redhat.com:8443/v2/redhat/redhat-operator-index/manifests/v4.11": x509: certificate signed by unknown authority</pre>

						</p></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Copy the registry certificate into your server:
							</p><pre class="programlisting language-terminal"># cp /tmp/eko4-ca.crt /etc/pki/ca-trust/source/anchors/.</pre></li><li class="listitem"><p class="simpara">
								Update the certificates truststore:
							</p><pre class="programlisting language-terminal"># update-ca-trust</pre></li><li class="listitem"><p class="simpara">
								Mount the host <code class="literal">/etc/pki</code> folder into the factory-cli image:
							</p><pre class="programlisting language-terminal"># podman run -v /mnt:/mnt -v /root/.docker:/root/.docker -v /etc/pki:/etc/pki --privileged -it --rm quay.io/openshift-kni/telco-ran-tools:latest -- \
factory-precaching-cli download -r 4.13.0 --acm-version 2.5.4 \
   --mce-version 2.0.4 -f /mnt \--img quay.io/custom/repository
   --du-profile -s --skip-imageset</pre></li></ol></div></section></section></section></section><div><div xml:lang="en-US" class="legalnotice" id="idm139735351482992"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"><!--Empty--></span>© 2023 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div>


  <nav class="pvof-doc__book-nav">
  <ol class="book-nav__list">
              </ol>
</nav>


          </div>
              </div>
              <div id="comments-footer" class="book-comments">
          

  

        </div>
          </div>
  </article>
<meta itemscope="" itemref="md1">



    </div>
      <!-- CP_PRIMER_FOOTER -->            </div>
        </main>
    </div>
    <!--googleoff: all-->
    <div id="to-top"><a class="btn_slideto" href="#masthead" aria-label="Back to Top"><span class="web-icon-upload"></span></a></div>
    <footer class="footer-main">
        <div class="footer-top">
            <div class="container">

              <div class="brand">
                <a href="https://redhat.com">
                  <svg class="rh-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 613 145">
                    <defs>
                      <style>
                        .rh-logo-hat {
                          fill: #e00;
                        }
                        .rh-logo-type {
                          fill: #fff;
                        }
                      </style>
                    </defs>
                    <title>Red Hat</title>
                    <path
                      class="rh-logo-hat"
                      d="M127.47,83.49c12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42l-7.45-32.36c-1.72-7.12-3.23-10.35-15.73-16.6C124.89,8.69,103.76.5,97.51.5,91.69.5,90,8,83.06,8c-6.68,0-11.64-5.6-17.89-5.6-6,0-9.91,4.09-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33C22.27,52,.5,55,.5,74.22c0,31.48,74.59,70.28,133.65,70.28,45.28,0,56.7-20.48,56.7-36.65,0-12.72-11-27.16-30.83-35.78"/>
                      <path class="rh-logo-band"
                      d="M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33l3.66-9.06A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45,12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42Z"/>
                      <path
                      class="rh-logo-type"
                      d="M579.74,92.8c0,11.89,7.15,17.67,20.19,17.67a52.11,52.11,0,0,0,11.89-1.68V95a24.84,24.84,0,0,1-7.68,1.16c-5.37,0-7.36-1.68-7.36-6.73V68.3h15.56V54.1H596.78v-18l-17,3.68V54.1H568.49V68.3h11.25Zm-53,.32c0-3.68,3.69-5.47,9.26-5.47a43.12,43.12,0,0,1,10.1,1.26v7.15a21.51,21.51,0,0,1-10.63,2.63c-5.46,0-8.73-2.1-8.73-5.57m5.2,17.56c6,0,10.84-1.26,15.36-4.31v3.37h16.82V74.08c0-13.56-9.14-21-24.39-21-8.52,0-16.94,2-26,6.1l6.1,12.52c6.52-2.74,12-4.42,16.83-4.42,7,0,10.62,2.73,10.62,8.31v2.73a49.53,49.53,0,0,0-12.62-1.58c-14.31,0-22.93,6-22.93,16.73,0,9.78,7.78,17.24,20.19,17.24m-92.44-.94h18.09V80.92h30.29v28.82H506V36.12H487.93V64.41H457.64V36.12H439.55ZM370.62,81.87c0-8,6.31-14.1,14.62-14.1A17.22,17.22,0,0,1,397,72.09V91.54A16.36,16.36,0,0,1,385.24,96c-8.2,0-14.62-6.1-14.62-14.09m26.61,27.87h16.83V32.44l-17,3.68V57.05a28.3,28.3,0,0,0-14.2-3.68c-16.19,0-28.92,12.51-28.92,28.5a28.25,28.25,0,0,0,28.4,28.6,25.12,25.12,0,0,0,14.93-4.83ZM320,67c5.36,0,9.88,3.47,11.67,8.83H308.47C310.15,70.3,314.36,67,320,67M291.33,82c0,16.2,13.25,28.82,30.28,28.82,9.36,0,16.2-2.53,23.25-8.42l-11.26-10c-2.63,2.74-6.52,4.21-11.14,4.21a14.39,14.39,0,0,1-13.68-8.83h39.65V83.55c0-17.67-11.88-30.39-28.08-30.39a28.57,28.57,0,0,0-29,28.81M262,51.58c6,0,9.36,3.78,9.36,8.31S268,68.2,262,68.2H244.11V51.58Zm-36,58.16h18.09V82.92h13.77l13.89,26.82H292l-16.2-29.45a22.27,22.27,0,0,0,13.88-20.72c0-13.25-10.41-23.45-26-23.45H226Z"/>
                  </svg>
                </a>
              </div>

              <div role="navigation" aria-label="quick">
                  <h3>Quick Links</h3>
                  <ul>
                      <li><a class="download-software" href="https://access.redhat.com/downloads/">Downloads</a></li>
                      <li><a class="manage-subscriptions" href="https://access.redhat.com/management">Subscriptions</a></li>
                      <li><a class="support-cases" href="https://access.redhat.com/support">Support Cases</a></li>
                      <li><a class="customer-service" href="https://access.redhat.com/support/customer-service">Customer Service</a></li>
                      <li><a class="quick-docs" href="https://access.redhat.com/documentation">Product Documentation</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="help">
                  <h3>Help</h3>
                  <ul>
                      <li><a class="contact-us" href="https://access.redhat.com/support/contact/">Contact Us</a></li>
                      <li><a class="cp-faqs" href="https://access.redhat.com/articles/33844">Customer Portal FAQ</a></li>
                      <li><a class="login-problems" href="https://access.redhat.com/help/login_assistance">Log-in Assistance</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="site">
                  <h3>Site Info</h3>
                  <ul>
                      <li><a class="trust-red-hat" href="https://www.redhat.com/en/trust">Trust Red Hat</a></li>
                      <li><a class="browser-support-policy" href="https://www.redhat.com/en/about/browser-support">Browser Support Policy</a></li>
                      <li><a class="accessibility" href="https://www.redhat.com/en/about/digital-accessibility">Accessibility</a></li>
                      <li><a class="recognition" href="https://access.redhat.com/recognition/">Awards and Recognition</a></li>
                      <li><a class="colophon" href="https://access.redhat.com/help/colophon/">Colophon</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="other">
                  <h3>Related Sites</h3>
                  <ul>
                      <li><a href="https://www.redhat.com/" class="red-hat-com">redhat.com</a></li>
                      <li><a href="http://developers.redhat.com/" class="red-hat-developers">developers.redhat.com</a></li>
                      <li><a href="https://connect.redhat.com/" class="partner-connect">connect.redhat.com</a></li>
                      <li><a href="https://cloud.redhat.com/" class="cloud-com">cloud.redhat.com</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="about">
                  <h3>About</h3>
                  <ul>
                      <li><a href="https://access.redhat.com/subscription-value" class="subscription-value">Red Hat Subscription Value</a></li>
                      <li><a href="https://www.redhat.com/about/" class="about-red-hat">About Red Hat</a></li>
                      <li><a href="http://jobs.redhat.com" class="about-jobs">Red Hat Jobs</a></li>
                  </ul>
              </div>

            </div>
        </div>

        <div class="anchor">
            <div class="container">
                <div class="status-legal">
                    <a hidden href="https://status.redhat.com" class="status-page-widget">
                        <span class="status-description"></span>
                        <span class="status-dot shape-circle"></span>
                    </a>
                    <div class="legal-copyright">
                        <div class="copyright">Copyright © 2023 Red Hat, Inc.</div>
                        <div role="navigation" aria-label="legal" class="legal">
                            <ul>
                                <li><a href="http://www.redhat.com/en/about/privacy-policy" class="privacy-policy">Privacy Statement</a></li>
                                <li><a href="https://www.redhat.com/en/about/terms-use" class="terms-of-use">Terms of Use</a></li>
                                <li><a href="http://www.redhat.com/en/about/all-policies-guidelines" class="all-policies">All Policies and Guidelines</a></li>
                                <li><a id="teconsent"></a></li>
                            </ul>
                            <div id="privacy_policy">We've updated our <a href='http://www.redhat.com/en/about/privacy-policy' class='privacy-policy'>Privacy Statement</a> effective September 15, 2023.
                            </div>
                          </div>
                        </div>
                </div>
                <div class="social">
                    <a href="http://www.redhat.com/summit/" class="summit">
                        <img src="https://access.redhat.com/chrome_themes/nimbus/img/rh-summit-red-a.svg" alt="Red Hat Summit" /> <span class="offscreen">Red Hat Summit</span>
                    </a>

                    <div class="social-media">
                        <a href="https://twitter.com/RedHat" class="sm-icon twitter"><span class="nicon-twitter"></span><span class="offscreen">Twitter</span></a>                        
                    </div>
                </div>
            </div>
        </div>
    </footer>
    <!-- TrustArc -->
    <div id="consent_blackbar"></div> 
    <!--googleon: all-->
</div>
<!-- /CP_PRIMER_FOOTER -->


  </div>

    
  </body>
</html>
