NVIDIA GPU architecture overview

NVIDIA supports the use of graphics processing unit (GPU) resources on "Red Hat OpenShift Container Platform". "Red Hat OpenShift Container Platform" is a security-focused and hardened Kubernetes platform developed and supported by Red Hat for deploying and managing Kubernetes clusters at scale. "Red Hat OpenShift Container Platform" includes enhancements to Kubernetes so that users can easily configure and use NVIDIA GPU resources to accelerate workloads.

The NVIDIA GPU Operator leverages the Operator framework within "Red Hat OpenShift Container Platform" to manage the full lifecycle of NVIDIA software components required to run GPU-accelerated workloads.

These components include the NVIDIA drivers (to enable CUDA), the Kubernetes device plugin for GPUs, the NVIDIA Container Toolkit, automatic node tagging using GPU feature discovery (GFD), DCGM-based monitoring, and others.

The NVIDIA GPU Operator is only supported by NVIDIA. For more information about obtaining support from NVIDIA, see Obtaining Support from NVIDIA.
NVIDIA GPU prerequisites
A working OpenShift cluster with at least one GPU worker node.

Access to the OpenShift cluster as a cluster-admin to perform the required steps.

OpenShift CLI (oc) is installed.

The node feature discovery (NFD) Operator is installed and a nodefeaturediscovery instance is created.
NVIDIA GPU enablement
The following diagram shows how the GPU architecture is enabled for OpenShift:


GPUs and bare metal
You can deploy "Red Hat OpenShift Container Platform" on an NVIDIA-certified bare metal server but with some limitations:

Control plane nodes can be CPU nodes.

Worker nodes must be GPU nodes, provided that AI/ML workloads are executed on these worker nodes.

When using OpenShift, note that one or three or more servers are required. Clusters with two servers are not supported. The single server deployment is called single node openShift (SNO) and using this configuration results in a non-high availability OpenShift environment.


You can choose one of the following methods to access the containerized GPUs:

GPU passthrough

Multi-Instance GPU (MIG)


Red Hat OpenShift on Bare Metal Stack
GPUs and virtualization
Many developers and enterprises are moving to containerized applications and serverless infrastructures, but there is still a lot of interest in developing and maintaining applications that run on virtual machines (VMs). Red Hat OpenShift Virtualization provides this capability, enabling enterprises to incorporate VMs into containerized workflows within clusters.

You can choose one of the following methods to connect the worker nodes to the GPUs:

GPU passthrough to access and use GPU hardware within a virtual machine (VM).

GPU (vGPU) time-slicing, when GPU compute capacity is not saturated by workloads.


NVIDIA GPU Operator with OpenShift Virtualization
GPUs and vSphere
You can deploy "Red Hat OpenShift Container Platform" on an NVIDIA-certified VMware vSphere server that can host different GPU types.

An NVIDIA GPU driver must be installed in the hypervisor in case vGPU instances are used by the VMs. For VMware vSphere, this host driver is provided in the form of a VIB file.

The maximum number of vGPUS that can be allocated to worker node VMs depends on the version of vSphere:

vSphere 7.0: maximum 4 vGPU per VM

vSphere 8.0: maximum 8 vGPU per VM


You can choose one of the following methods to attach the worker nodes to the GPUs:

GPU passthrough for accessing and using GPU hardware within a virtual machine (VM)

GPU (vGPU) time-slicing, when not all of the GPU is needed


Similar to bare metal deployments, one or three or more servers are required. Clusters with two servers are not supported.

OpenShift Container Platform on VMware vSphere with NVIDIA vGPUs
GPUs and Red Hat KVM
You can use "Red Hat OpenShift Container Platform" on an NVIDIA-certified kernel-based virtual machine (KVM) server.

Similar to bare-metal deployments, one or three or more servers are required. Clusters with two servers are not supported.

However, unlike bare-metal deployments, you can use different types of GPUs in the server. This is because you can assign these GPUs to different VMs that act as Kubernetes nodes. The only limitation is that a Kubernetes node must have the same set of GPU types at its own level.

You can choose one of the following methods to access the containerized GPUs:

GPU passthrough for accessing and using GPU hardware within a virtual machine (VM)

GPU (vGPU) time-slicing when not all of the GPU is needed


To enable the vGPU capability, a special driver must be installed at the host level. This driver is delivered as a RPM package. This host driver is not required at all for GPU passthrough allocation.

How To Deploy OpenShift Container Platform 4.13 on KVM
GPUs and CSPs
You can deploy "Red Hat OpenShift Container Platform" to one of the major cloud service providers (CSPs): Amazon Web Services (AWS), Google Cloud Platform (GCP), or Microsoft Azure.

Two modes of operation are available: a fully managed deployment and a self-managed deployment.

In a fully managed deployment, everything is automated by Red Hat in collaboration with CSP. You can request an OpenShift instance through the CSP web console, and the cluster is automatically created and fully managed by Red Hat. You do not have to worry about node failures or errors in the environment. Red Hat is fully responsible for maintaining the uptime of the cluster. The fully managed services are available on AWS and Azure. For AWS, the OpenShift service is called ROSA (Red Hat OpenShift Service on AWS). For Azure, the service is called Azure Red Hat OpenShift.

In a self-managed deployment, you are responsible for instantiating and maintaining the OpenShift cluster. Red Hat provides the OpenShift-install utility to support the deployment of the OpenShift cluster in this case. The self-managed services are available globally to all CSPs.


It is important that this compute instance is a GPU-accelerated compute instance and that the GPU type matches the list of supported GPUs from NVIDIA AI Enterprise. For example, T4, V100, and A100 are part of this list.

You can choose one of the following methods to access the containerized GPUs:

GPU passthrough to access and use GPU hardware within a virtual machine (VM).

GPU (vGPU) time slicing when the entire GPU is not required.


Red Hat Openshift in the Cloud
GPUs and Red Hat Device Edge
Red Hat Device Edge provides access to MicroShift. MicroShift provides the simplicity of a single-node deployment with the functionality and services you need for resource-constrained (edge) computing. Red Hat Device Edge meets the needs of bare-metal, virtual, containerized, or Kubernetes workloads deployed in resource-constrained environments.

You can enable NVIDIA GPUs on containers in a Red Hat Device Edge environment.

You use GPU passthrough to access the containerized GPUs.

How to accelerate workloads with NVIDIA GPUs on Red Hat Device Edge
NVIDIA GPU features for "Red Hat OpenShift Container Platform"

NVIDIA Container Toolkit
NVIDIA Container Toolkit enables you to create and run GPU-accelerated containers. The toolkit includes a container runtime library and utilities to automatically configure containers to use NVIDIA GPUs.
NVIDIA AI Enterprise
NVIDIA AI Enterprise is an end-to-end, cloud-native suite of AI and data analytics software optimized, certified, and supported with NVIDIA-Certified systems.
Multi-Instance GPU (MIG) Support in "Red Hat OpenShift Container Platform"
MIG is useful whenever you have an application that does not require the full power of an entire GPU. The MIG feature of the new NVIDIA Ampere architecture enables you to split your hardware resources into multiple GPU instances, each of which is available to the operating system as an independent CUDA-enabled GPU. The NVIDIA GPU Operator version 1.7.0 and higher provides MIG support for the A100 and A30 Ampere cards. These GPU instances are designed to support multiple independent CUDA applications (up to 7) so that they operate completely isolated from each other with dedicated hardware resources.
Time-slicing NVIDIA GPUs in OpenShift
GPU time-slicing enables workloads scheduled on overloaded GPUs to be interleaved.
GPU Feature Discovery
NVIDIA GPU Feature Discovery for Kubernetes is a software component that enables you to automatically generate labels for the GPUs available on a node. GPU Feature Discovery uses node feature discovery (NFD) to perform this labeling.
NVIDIA GPU Operator with OpenShift Virtualization
Up until this point, the GPU Operator only provisioned worker nodes to run GPU-accelerated containers. Now, the GPU Operator can also be used to provision worker nodes for running GPU-accelerated virtual machines (VMs).
GPU Monitoring dashboard
You can install a monitoring dashboard to display GPU usage information on the cluster Observe page in the "Red Hat OpenShift Container Platform" web console. GPU utilization information includes the number of available GPUs, power consumption (in watts), temperature (in degrees Celsius), utilization (in percent), and other metrics for each GPU.


NVIDIA-Certified Systems

NVIDIA AI Enterprise with OpenShift

NVIDIA Container Toolkit

Enabling the GPU Monitoring Dashboard

MIG Support in OpenShift Container Platform

Time-slicing NVIDIA GPUs in OpenShift

Deploy GPU Operators in a disconnected or airgapped environment

Installing the Node Feature Discovery (NFD) Operator

"Red Hat OpenShift Container Platform" Installing the Node Feature Discovery Operator