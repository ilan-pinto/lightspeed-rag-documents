Installing a cluster on OpenStack with Kuryr

Kuryr is a deprecated feature. Deprecated functionality is still included in "Red Hat OpenShift Container Platform" and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments.

For the most recent list of major functionality that has been deprecated or removed within "Red Hat OpenShift Container Platform", refer to the Deprecated and removed features section of the "Red Hat OpenShift Container Platform" release notes.
In "Red Hat OpenShift Container Platform" version {product-version}, you can install a customized cluster on Red Hat OpenStack Platform (RHOSP) that uses Kuryr SDN. To customize the installation, modify parameters in the install-config.yaml before you install the cluster.
Prerequisites
You reviewed details about the "Red Hat OpenShift Container Platform" installation and update processes.

You read the documentation on selecting a cluster installation method and preparing it for users.

You verified that "Red Hat OpenShift Container Platform" {product-version} is compatible with your RHOSP version by using the Supported platforms for OpenShift clusters section. You can also compare platform support across different versions by viewing the "Red Hat OpenShift Container Platform" on RHOSP support matrix.

You have a storage service installed in RHOSP, such as block storage (Cinder) or object storage (Swift). Object storage is the recommended storage technology for "Red Hat OpenShift Container Platform" registry cluster deployment. For more information, see Optimizing storage.

You understand performance and scalability practices for cluster scaling, control plane sizing, and etcd. For more information, see Recommended practices for scaling the cluster.
About Kuryr SDN
Kuryr is a deprecated feature. Deprecated functionality is still included in "Red Hat OpenShift Container Platform" and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments.

For the most recent list of major functionality that has been deprecated or removed within "Red Hat OpenShift Container Platform", refer to the Deprecated and removed features section of the "Red Hat OpenShift Container Platform" release notes.
Kuryr is a container network interface (CNI) plugin solution that uses the Neutron and Octavia Red Hat OpenStack Platform (RHOSP) services to provide networking for pods and Services.

Kuryr and "Red Hat OpenShift Container Platform" integration is primarily designed for "Red Hat OpenShift Container Platform" clusters running on RHOSP VMs. Kuryr improves the network performance by plugging "Red Hat OpenShift Container Platform" pods into RHOSP SDN. In addition, it provides interconnectivity between pods and RHOSP virtual instances.

Kuryr components are installed as pods in "Red Hat OpenShift Container Platform" using the openshift-kuryr namespace:

kuryr-controller - a single service instance installed on a master node.
This is modeled in "Red Hat OpenShift Container Platform" as a Deployment object.

kuryr-cni - a container installing and configuring Kuryr as a CNI driver on
each "Red Hat OpenShift Container Platform" node. This is modeled in "Red Hat OpenShift Container Platform" as a DaemonSet object.


The Kuryr controller watches the "Red Hat OpenShift Container Platform" API server for pod, service, and namespace create, update, and delete events. It maps the "Red Hat OpenShift Container Platform" API calls to corresponding objects in Neutron and Octavia. This means that every network solution that implements the Neutron trunk port functionality can be used to back "Red Hat OpenShift Container Platform" via Kuryr. This includes open source solutions such as Open vSwitch (OVS) and Open Virtual Network (OVN) as well as Neutron-compatible commercial SDNs.

Kuryr is recommended for "Red Hat OpenShift Container Platform" deployments on encapsulated RHOSP tenant networks to avoid double encapsulation, such as running an encapsulated "Red Hat OpenShift Container Platform" SDN over an RHOSP network.

If you use provider networks or tenant VLANs, you do not need to use Kuryr to avoid double encapsulation. The performance benefit is negligible. Depending on your configuration, though, using Kuryr to avoid having two overlays might still be beneficial.

Kuryr is not recommended in deployments where all of the following criteria are true:

The RHOSP version is less than 16.

The deployment uses UDP services, or a large number of TCP services on few hypervisors.


or

The ovn-octavia Octavia driver is disabled.

The deployment uses a large number of TCP services on few hypervisors.
Resource guidelines for installing "Red Hat OpenShift Container Platform" on RHOSP with Kuryr
When using Kuryr SDN, the pods, services, namespaces, and network policies are using resources from the RHOSP quota; this increases the minimum requirements. Kuryr also has some additional requirements on top of what a default install requires.

Use the following quota to satisfy a default cluster's minimum requirements:


A cluster might function with fewer than recommended resources, but its performance is not guaranteed.

If RHOSP object storage (Swift) is available and operated by a user account with the swiftoperator role, it is used as the default backend for the "Red Hat OpenShift Container Platform" image registry. In this case, the volume storage requirement is 175 GB. Swift space requirements vary depending on the size of the image registry.
If you are using Red Hat OpenStack Platform (RHOSP) version 16 with the Amphora driver rather than the OVN Octavia driver, security groups are associated with service accounts instead of user projects.
Take the following notes into consideration when setting resources:

The number of ports that are required is larger than the number of pods. Kuryr
uses ports pools to have pre-created ports ready to be used by pods and speed up
the pods' booting time.

Each network policy is mapped into an RHOSP security group, and
depending on the NetworkPolicy spec, one or more rules are added to the
security group.

Each service is mapped to an RHOSP load balancer. Consider this requirement
when estimating the number of security groups required for the quota.

The quota does not account for load balancer resources (such as VM
resources), but you must consider these resources when you decide the
RHOSP deployment's size. The default installation will have more than
50 load balancers; the clusters must be able to accommodate them.


An "Red Hat OpenShift Container Platform" deployment comprises control plane machines, compute machines, and a bootstrap machine.

To enable Kuryr SDN, your environment must meet the following requirements:

Run RHOSP 13+.

Have Overcloud with Octavia.

Use Neutron Trunk ports extension.

Use openvswitch firewall driver if ML2/OVS Neutron driver is used instead
of ovs-hybrid.


Increasing quota
When using Kuryr SDN, you must increase quotas to satisfy the Red Hat OpenStack Platform (RHOSP) resources used by pods, services, namespaces, and network policies.

Increase the quotas for a project by running the following command:
Configuring Neutron
Kuryr CNI leverages the Neutron Trunks extension to plug containers into the Red Hat OpenStack Platform (RHOSP) SDN, so you must use the trunks extension for Kuryr to properly work.

In addition, if you leverage the default ML2/OVS Neutron driver, the firewall must be set to openvswitch instead of ovs_hybrid so that security groups are enforced on trunk subports and Kuryr can properly handle network policies.
Configuring Octavia
Kuryr SDN uses Red Hat OpenStack Platform (RHOSP)'s Octavia LBaaS to implement "Red Hat OpenShift Container Platform" services. Thus, you must install and configure Octavia components in RHOSP to use Kuryr SDN.

To enable Octavia, you must include the Octavia service during the installation of the RHOSP Overcloud, or upgrade the Octavia service if the Overcloud already exists. The following steps for enabling Octavia apply to both a clean install of the Overcloud or an Overcloud update.

The following steps only capture the key pieces required during the deployment of RHOSP when dealing with Octavia. It is also important to note that registry methods vary.

This example uses the local registry method.
If you are using the local registry, create a template to upload the images to
the registry. For example:

Verify that the local_registry_images.yaml file contains the Octavia images.
For example:

Pull the container images from registry.redhat.io to the Undercloud node:

Install or update your Overcloud environment with Octavia:


The Octavia OVN Driver
Octavia supports multiple provider drivers through the Octavia API.

To see all available Octavia provider drivers, on a command line, enter:

$ openstack loadbalancer provider list
+---------+-------------------------------------------------+
| name    | description                                     |
+---------+-------------------------------------------------+
| amphora | The Octavia Amphora driver.                     |
| octavia | Deprecated alias of the Octavia Amphora driver. |
| ovn     | Octavia OVN driver.                             |
+---------+-------------------------------------------------+
Beginning with RHOSP version 16, the Octavia OVN provider driver (ovn) is supported on "Red Hat OpenShift Container Platform" on RHOSP deployments.

ovn is an integration driver for the load balancing that Octavia and OVN provide. It supports basic load balancing capabilities, and is based on OpenFlow rules. The driver is automatically enabled in Octavia by Director on deployments that use OVN Neutron ML2.

The Amphora provider driver is the default driver. If ovn is enabled, however, Kuryr uses it.

If Kuryr uses ovn instead of Amphora, it offers the following benefits:

Decreased resource requirements. Kuryr does not require a load balancer VM for each service.

Reduced network latency.

Increased service creation speed by using OpenFlow rules instead of a VM for each service.

Distributed load balancing actions across all nodes instead of centralized on Amphora VMs.


You can configure your cluster to use the Octavia OVN driver after your RHOSP cloud is upgraded from version 13 to version 16.
Known limitations of installing with Kuryr
Using "Red Hat OpenShift Container Platform" with Kuryr SDN has several known limitations.


Using "Red Hat OpenShift Container Platform" with Kuryr SDN has several limitations that apply to all versions and environments:

Service objects with the NodePort type are not supported.

Clusters that use the OVN Octavia provider driver support Service objects for which the .spec.selector property is unspecified only if the .subsets.addresses property of the Endpoints object includes the subnet of the nodes or pods.

If the subnet on which machines are created is not connected to a router, or if the subnet is connected, but the router has no external gateway set, Kuryr cannot create floating IPs for Service objects with type LoadBalancer.

Configuring the sessionAffinity=ClientIP property on Service objects does not have an effect. Kuryr does not support this setting.



Using "Red Hat OpenShift Container Platform" with Kuryr SDN has several limitations that depend on the RHOSP version.

RHOSP versions before 16 use
the default Octavia load balancer driver (Amphora). This driver requires that one
Amphora load balancer VM is deployed per "Red Hat OpenShift Container Platform" service. Creating too many
services can cause you to run out of resources.

Kuryr SDN does not support automatic unidling by a service.



As a result of the RHOSP upgrade process, the Octavia API might be changed, and upgrades to the Amphora images that are used for load balancers might be required.

You can address API changes on an individual basis.

If the Amphora image is upgraded, the RHOSP operator can handle existing load balancer VMs in two ways:

Upgrade each VM by triggering a load balancer failover.

Leave responsibility for upgrading the VMs to users.


If the operator takes the first option, there might be short downtimes during failovers.

If the operator takes the second option, the existing load balancers will not support upgraded Octavia API features, like UDP listeners. In this case, users must recreate their Services to use these features.
Control plane machines
By default, the "Red Hat OpenShift Container Platform" installation process creates three control plane machines.

Each machine requires:

An instance from the RHOSP quota

A port from the RHOSP quota

A flavor with at least 16 GB memory and 4 vCPUs

At least 100 GB storage space from the RHOSP quota
Compute machines
By default, the "Red Hat OpenShift Container Platform" installation process creates three compute machines.

Each machine requires:

An instance from the RHOSP quota

A port from the RHOSP quota

A flavor with at least 8 GB memory and 2 vCPUs

At least 100 GB storage space from the RHOSP quota


Compute machines host the applications that you run on "Red Hat OpenShift Container Platform"; aim to run as many as you can.
Bootstrap machine
During installation, a bootstrap machine is temporarily provisioned to stand up the control plane. After the production control plane is ready, the bootstrap machine is deprovisioned.

The bootstrap machine requires:

An instance from the RHOSP quota

A port from the RHOSP quota

A flavor with at least 16 GB memory and 4 vCPUs

At least 100 GB storage space from the RHOSP quota
Load balancing requirements for user-provisioned infrastructure
Before you install "Red Hat OpenShift Container Platform", you can provision your own API and application ingress load balancing infrastructure to use in place of the default, internal load balancing solution. In production scenarios, you can deploy the API and application Ingress load balancers separately so that you can scale the load balancer infrastructure for each in isolation.

If you want to deploy the API and application Ingress load balancers with a Red Hat Enterprise Linux (RHEL) instance, you must purchase the RHEL subscription separately.
The load balancing infrastructure must meet the following requirements:

API load balancer: Provides a common endpoint for users, both human and machine, to interact with and configure the platform. Configure the following conditions:

Application Ingress load balancer: Provides an ingress point for application traffic flowing in from outside the cluster. A working configuration for the Ingress router is required for an "Red Hat OpenShift Container Platform" cluster.


Example load balancer configuration for clusters that are deployed with user-managed load balancers
This section provides an example API and application Ingress load balancer configuration that meets the load balancing requirements for clusters that are deployed with user-managed load balancers. The sample is an /etc/haproxy/haproxy.cfg configuration for an HAProxy load balancer. The example is not meant to provide advice for choosing one load balancing solution over another.

In the example, the same load balancer is used for the Kubernetes API and application ingress traffic. In production scenarios, you can deploy the API and application ingress load balancers separately so that you can scale the load balancer infrastructure for each in isolation.

If you are using HAProxy as a load balancer and SELinux is set to enforcing, you must ensure that the HAProxy service can bind to the configured TCP port by running setsebool -P haproxy_connect_any=1.
global
  log         127.0.0.1 local2
  pidfile     /var/run/haproxy.pid
  maxconn     4000
  daemon
defaults
  mode                    http
  log                     global
  option                  dontlognull
  option http-server-close
  option                  redispatch
  retries                 3
  timeout http-request    10s
  timeout queue           1m
  timeout connect         10s
  timeout client          1m
  timeout server          1m
  timeout http-keep-alive 10s
  timeout check           10s
  maxconn                 3000
listen api-server-6443 1
  bind *:6443
  mode tcp
  server bootstrap bootstrap.ocp4.example.com:6443 check inter 1s backup 2
  server master0 master0.ocp4.example.com:6443 check inter 1s
  server master1 master1.ocp4.example.com:6443 check inter 1s
  server master2 master2.ocp4.example.com:6443 check inter 1s
listen machine-config-server-22623 3
  bind *:22623
  mode tcp
  server bootstrap bootstrap.ocp4.example.com:22623 check inter 1s backup 2
  server master0 master0.ocp4.example.com:22623 check inter 1s
  server master1 master1.ocp4.example.com:22623 check inter 1s
  server master2 master2.ocp4.example.com:22623 check inter 1s
listen ingress-router-443 4
  bind *:443
  mode tcp
  balance source
  server worker0 worker0.ocp4.example.com:443 check inter 1s
  server worker1 worker1.ocp4.example.com:443 check inter 1s
listen ingress-router-80 5
  bind *:80
  mode tcp
  balance source
  server worker0 worker0.ocp4.example.com:80 check inter 1s
  server worker1 worker1.ocp4.example.com:80 check inter 1s
Port 6443 handles the Kubernetes API traffic and points to the control plane machines.

The bootstrap entries must be in place before the "Red Hat OpenShift Container Platform" cluster installation and they must be removed after the bootstrap process is complete.

Port 22623 handles the machine config server traffic and points to the control plane machines.

Port 443 handles the HTTPS traffic and points to the machines that run the Ingress Controller pods. The Ingress Controller pods run on the compute machines by default.

Port 80 handles the HTTP traffic and points to the machines that run the Ingress Controller pods. The Ingress Controller pods run on the compute machines by default.
If you are using HAProxy as a load balancer, you can check that the haproxy process is listening on ports 6443, 22623, 443, and 80 by running netstat -nltupe on the HAProxy node.
Internet access for "Red Hat OpenShift Container Platform"
In "Red Hat OpenShift Container Platform" {product-version}, you require access to the internet to install your cluster.

You must have internet access to:

Access OpenShift Cluster Manager Hybrid Cloud Console to download the installation program and perform subscription management. If the cluster has internet access and you do not disable Telemetry, that service automatically entitles your cluster.

Access Quay.io to obtain the packages that are required to install your cluster.

Obtain the packages that are required to perform cluster updates.
Enabling Swift on RHOSP
Swift is operated by a user account with the swiftoperator role. Add the role to an account before you run the installation program.

If the Red Hat OpenStack Platform (RHOSP) object storage service, commonly known as Swift, is available, "Red Hat OpenShift Container Platform" uses it as the image registry storage. If it is unavailable, the installation program relies on the RHOSP block storage service, commonly known as Cinder.

If Swift is present and you want to use it, you must enable access to it. If it is not present, or if you do not want to use it, skip this section.
RHOSP 17 sets the rgw_max_attr_size parameter of Ceph RGW to 256 characters. This setting causes issues with uploading container images to the "Red Hat OpenShift Container Platform" registry. You must set the value of rgw_max_attr_size to at least 1024 characters.

Before installation, check if your RHOSP deployment is affected by this problem. If it is, reconfigure Ceph RGW.
You have a RHOSP administrator account on the target environment.

The Swift service is installed.

On Ceph RGW, the account in url option is enabled.


To enable Swift on RHOSP:

As an administrator in the RHOSP CLI, add the swiftoperator role to the account that will access Swift:


Your RHOSP deployment can now use Swift for the image registry.
Verifying external network access
The "Red Hat OpenShift Container Platform" installation process requires external network access. You must provide an external network value to it, or deployment fails. Before you begin the process, verify that a network with the external router type exists in Red Hat OpenStack Platform (RHOSP).

On RHOSP, the NeutronDhcpAgentDnsmasqDnsServers parameter must be configured to allow DHCP agents to forward instances' DNS queries. One way to set this parameter is to:


Using the RHOSP CLI, verify the name and ID of the 'External' network:


A network with an external router type appears in the network list. If at least one does not, see Creating a default floating IP network and Creating a default provider network.

If the external network's CIDR range overlaps one of the default network ranges, you must change the matching network ranges in the install-config.yaml file before you start the installation process.

The default network ranges are:
If the installation program finds multiple networks with the same name, it sets one of them at random. To avoid this behavior, create unique names for resources in RHOSP.
If the Neutron trunk service plugin is enabled, a trunk port is created by default. For more information, see Neutron trunk port.
Defining parameters for the installation program
The "Red Hat OpenShift Container Platform" installation program relies on a file that is called clouds.yaml. The file describes Red Hat OpenStack Platform (RHOSP) configuration parameters, including the project name, log in information, and authorization service URLs.

Create the clouds.yaml file:

If your RHOSP installation uses self-signed certificate authority (CA) certificates for endpoint authentication:

Place the clouds.yaml file in one of the following locations:
Setting OpenStack Cloud Controller Manager options
Optionally, you can edit the OpenStack Cloud Controller Manager (CCM) configuration for your cluster. This configuration controls how "Red Hat OpenShift Container Platform" interacts with Red Hat OpenStack Platform (RHOSP).

For a complete list of configuration parameters, see the "OpenStack Cloud Controller Manager reference guide" page in the "Installing on OpenStack" documentation.

If you have not already generated manifest files for your cluster, generate them by running the following command:

In a text editor, open the cloud-provider configuration manifest file. For example:

Modify the options according to the CCM reference guide.

Save the changes to the file and proceed with installation.
Obtaining the installation program
Before you install "Red Hat OpenShift Container Platform", download the installation file on  the host you are using for installation.

You have a computer that runs Linux or macOS, with 500 MB of local disk space.


Access the Infrastructure Provider page on the OpenShift Cluster Manager site. If you have a Red Hat account, log in with your credentials. If you do not, create an account.

Select your infrastructure provider.

Navigate to the page for your installation type, download the installation program that corresponds with your host operating system and architecture, and place the file in the directory where you will store the installation configuration files.

Extract the installation program. For example, on a computer that uses a Linux
operating system, run the following command:

Download your installation pull secret from the Red Hat OpenShift Cluster Manager. This pull secret allows you to authenticate with the services that are provided by the included authorities, including Quay.io, which serves the container images for "Red Hat OpenShift Container Platform" components.
Creating the installation configuration file
You can customize the "Red Hat OpenShift Container Platform" cluster you install on Red Hat OpenStack Platform (RHOSP).

You have the "Red Hat OpenShift Container Platform" installation program and the pull secret for your cluster.


Create the install-config.yaml file.

Modify the install-config.yaml file. You can find more information about the available parameters in the "Installation configuration parameters" section.

Back up the install-config.yaml file so that you can use
it to install multiple clusters.


Installation configuration parameters for OpenStack


Configuring the cluster-wide proxy during installation
Production environments can deny direct access to the internet and instead have an HTTP or HTTPS proxy available. You can configure a new "Red Hat OpenShift Container Platform" cluster to use a proxy by configuring the proxy settings in the install-config.yaml file.

Kuryr installations default to HTTP proxies.
For Kuryr installations on restricted networks that use the Proxy object, the proxy must be able to reply to the router that the cluster uses. To add a static route for the proxy configuration, from a command line as the root user, enter:

The restricted subnet must have a gateway that is defined and available to be linked to the Router resource that Kuryr creates.

You have an existing install-config.yaml file.

You reviewed the sites that your cluster requires access to and determined whether any of them need to bypass the proxy. By default, all cluster egress traffic is proxied, including calls to hosting cloud provider APIs. You added sites to the Proxy object's spec.noProxy field to bypass the proxy if necessary.


Edit your install-config.yaml file and add the proxy settings. For example:

Save the file and reference it when installing "Red Hat OpenShift Container Platform".


The installation program creates a cluster-wide proxy that is named cluster that uses the proxy settings in the provided install-config.yaml file. If no proxy settings are provided, a cluster Proxy object is still created, but it will have a nil spec.

Only the Proxy object named cluster is supported, and no additional proxies can be created.
Custom subnets in RHOSP deployments
Optionally, you can deploy a cluster on a Red Hat OpenStack Platform (RHOSP) subnet of your choice. The subnet's GUID is passed as the value of platform.openstack.machinesSubnet in the install-config.yaml file.

This subnet is used as the cluster's primary subnet. By default, nodes and ports are created on it. You can create nodes and ports on a different RHOSP subnet by setting the value of the platform.openstack.machinesSubnet property to the subnet's UUID.

Before you run the "Red Hat OpenShift Container Platform" installer with a custom subnet, verify that your configuration meets the following requirements:

The subnet that is used by platform.openstack.machinesSubnet has DHCP enabled.

The CIDR of platform.openstack.machinesSubnet matches the CIDR of networking.machineNetwork.

The installation program user has permission to create ports on this network, including ports with fixed IP addresses.


Clusters that use custom subnets have the following limitations:

If you plan to install a cluster that uses floating IP addresses, the platform.openstack.machinesSubnet subnet must be attached to a router that is connected to the externalNetwork network.

If the platform.openstack.machinesSubnet value is set in the install-config.yaml file, the installation program does not create a private network or subnet for your RHOSP machines.

You cannot use the platform.openstack.externalDNS property at the same time as a custom subnet. To add DNS to a cluster that uses a custom subnet, configure DNS on the RHOSP network.


By default, the API VIP takes x.x.x.5 and the Ingress VIP takes x.x.x.7 from your network's CIDR block. To override these default values, set values for platform.openstack.apiVIPs and platform.openstack.ingressVIPs that are outside of the DHCP allocation pool.
The CIDR ranges for networks are not adjustable after cluster installation. Red Hat does not provide direct guidance on determining the range during cluster installation because it requires careful consideration of the number of created pods per namespace.
Sample customized install-config.yaml file for RHOSP with Kuryr
To deploy with Kuryr SDN instead of the default OVN-Kubernetes network plugin, you must modify the install-config.yaml file to include Kuryr as the desired networking.networkType. This sample install-config.yaml demonstrates all of the possible Red Hat OpenStack Platform (RHOSP) customization options.

This sample file is provided for reference only. You must obtain your install-config.yaml file by using the installation program.
apiVersion: v1
baseDomain: example.com
controlPlane:
  name: master
  platform: {}
  replicas: 3
compute:
- name: worker
  platform:
    openstack:
      type: ml.large
  replicas: 3
metadata:
  name: example
networking:
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  machineNetwork:
  - cidr: 10.0.0.0/16
  serviceNetwork:
  - 172.30.0.0/16 1
  networkType: Kuryr 2
platform:
  openstack:
    cloud: mycloud
    externalNetwork: external
    computeFlavor: m1.xlarge
    apiFloatingIP: 128.0.0.1
    trunkSupport: true 3
    octaviaSupport: true 3
pullSecret: '{"auths": ...}'
sshKey: ssh-ed25519 AAAA...
The Amphora Octavia driver creates two ports per load balancer. As a
result, the service subnet that the installer creates is twice the size of the
CIDR that is specified as the value of the serviceNetwork property. The larger range is
required to prevent IP address conflicts.

The cluster network plugin to install. The supported values are Kuryr, OVNKubernetes, and OpenShiftSDN. The default value is OVNKubernetes.

Both trunkSupport and octaviaSupport are automatically discovered by the
installer, so there is no need to set them. But if your environment does not
meet both requirements, Kuryr SDN will not properly work. Trunks are needed
to connect the pods to the RHOSP network and Octavia is required to create the
"Red Hat OpenShift Container Platform" services.
Installation configuration for a cluster on OpenStack with a user-managed load balancer
The following example install-config.yaml file demonstrates how to configure a cluster that uses an external, user-managed load balancer rather than the default internal load balancer.

apiVersion: v1
baseDomain: mydomain.test
compute:
- name: worker
  platform:
    openstack:
      type: m1.xlarge
  replicas: 3
controlPlane:
  name: master
  platform:
    openstack:
      type: m1.xlarge
  replicas: 3
metadata:
  name: mycluster
networking:
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  machineNetwork:
  - cidr: 192.168.10.0/24
platform:
  openstack:
    cloud: mycloud
    machinesSubnet: 8586bf1a-cc3c-4d40-bdf6-c243decc603a 1
    apiVIPs:
    - 192.168.10.5
    ingressVIPs:
    - 192.168.10.7
    loadBalancer:
      type: UserManaged 2
Regardless of which load balancer you use, the load balancer is deployed to this subnet.

The UserManaged value indicates that you are using an user-managed load balancer.
Cluster deployment on RHOSP provider networks
You can deploy your "Red Hat OpenShift Container Platform" clusters on Red Hat OpenStack Platform (RHOSP) with a primary network interface on a provider network. Provider networks are commonly used to give projects direct access to a public network that can be used to reach the internet. You can also share provider networks among projects as part of the network creation process.

RHOSP provider networks map directly to an existing physical network in the data center. A RHOSP administrator must create them.

In the following example, "Red Hat OpenShift Container Platform" workloads are connected to a data center by using a provider network:


"Red Hat OpenShift Container Platform" clusters that are installed on provider networks do not require tenant networks or floating IP addresses. The installer does not create these resources during installation.

Example provider network types include flat (untagged) and VLAN (802.1Q tagged).

A cluster can support as many provider network connections as the network type allows. For example, VLAN networks typically support up to 4096 connections.
You can learn more about provider and tenant networks in the RHOSP documentation.

RHOSP provider network requirements for cluster installation
Before you install an "Red Hat OpenShift Container Platform" cluster, your Red Hat OpenStack Platform (RHOSP) deployment and provider network must meet a number of conditions:

The RHOSP networking service (Neutron) is enabled and accessible through the RHOSP networking API.

The RHOSP networking service has the port security and allowed address pairs extensions enabled.

The provider network can be shared with other tenants.

The RHOSP project that you use to install the cluster must own the provider network, as well as an appropriate subnet.

Verify that the provider network can reach the RHOSP metadata service IP address, which is 169.254.169.254 by default.

Optional: To secure the network, create role-based access control (RBAC) rules that limit network access to a single project.
Deploying a cluster that has a primary interface on a provider network
You can deploy an "Red Hat OpenShift Container Platform" cluster that has its primary network interface on an Red Hat OpenStack Platform (RHOSP) provider network.

Your Red Hat OpenStack Platform (RHOSP) deployment is configured as described by "RHOSP provider network requirements for cluster installation".


In a text editor, open the install-config.yaml file.

Set the value of the platform.openstack.apiVIPs property to the IP address for the API VIP.

Set the value of the platform.openstack.ingressVIPs property to the IP address for the Ingress VIP.

Set the value of the platform.openstack.machinesSubnet property to the UUID of the provider network subnet.

Set the value of the networking.machineNetwork.cidr property to the CIDR block of the provider network subnet.


The platform.openstack.apiVIPs and platform.openstack.ingressVIPs properties must both be unassigned IP addresses from the networking.machineNetwork.cidr block.
        ...
        platform:
          openstack:
            apiVIPs: 1
              - 192.0.2.13
            ingressVIPs: 1
              - 192.0.2.23
            machinesSubnet: fa806b2f-ac49-4bce-b9db-124bc64209bf
            # ...
        networking:
          machineNetwork:
          - cidr: 192.0.2.0/24
In "Red Hat OpenShift Container Platform" 4.12 and later, the apiVIP and ingressVIP configuration settings are deprecated. Instead, use a list format to enter values in the apiVIPs and ingressVIPs configuration settings.


You cannot set the platform.openstack.externalNetwork or platform.openstack.externalDNS parameters while using a provider network for the primary network interface.
When you deploy the cluster, the installer uses the install-config.yaml file to deploy the cluster on the provider network.

You can add additional networks, including provider networks, to the platform.openstack.additionalNetworkIDs list.

After you deploy your cluster, you can attach pods to additional networks. For more information, see Understanding multiple networks.
Kuryr ports pools
A Kuryr ports pool maintains a number of ports on standby for pod creation.

Keeping ports on standby minimizes pod creation time. Without ports pools, Kuryr must explicitly request port creation or deletion whenever a pod is created or deleted.

The Neutron ports that Kuryr uses are created in subnets that are tied to namespaces. These pod ports are also added as subports to the primary port of "Red Hat OpenShift Container Platform" cluster nodes.

Because Kuryr keeps each namespace in a separate subnet, a separate ports pool is maintained for each namespace-worker pair.

Prior to installing a cluster, you can set the following parameters in the cluster-network-03-config.yml manifest file to configure ports pool behavior:

The enablePortPoolsPrepopulation parameter controls pool prepopulation, which forces Kuryr to add Neutron ports to the pools when the first pod that is configured to use the dedicated network for pods is created in a namespace. The default value is false.

The poolMinPorts parameter is the minimum number of free ports that are kept in the pool. The default value is 1.

The poolMaxPorts parameter is the maximum number of free ports that are kept in the pool. A value of 0 disables that upper bound. This is the default setting.

The poolBatchPorts parameter defines the maximum number of Neutron ports that can be created at once. The default value is 3.
Adjusting Kuryr ports pools during installation
During installation, you can configure how Kuryr manages Red Hat OpenStack Platform (RHOSP) Neutron ports to control the speed and efficiency of pod creation.

Create and modify the install-config.yaml file.


From a command line, create the manifest files:

Create a file that is named cluster-network-03-config.yml in the
<installation_directory>/manifests/ directory:

Open the cluster-network-03-config.yml file in an editor,  and enter a custom resource (CR) that describes the Cluster Network Operator configuration that you want:

Edit the settings to meet your requirements. The following file is provided as an example:

Save the cluster-network-03-config.yml file, and exit the text editor.

Optional: Back up the manifests/cluster-network-03-config.yml file. The installation program deletes the manifests/ directory while creating the cluster.
Generating a key pair for cluster node SSH access
During an "Red Hat OpenShift Container Platform" installation, you can provide an SSH public key to the installation program. The key is passed to the Red Hat Enterprise Linux CoreOS (RHCOS) nodes through their Ignition config files and is used to authenticate SSH access to the nodes. The key is added to the ~/.ssh/authorized_keys list for the core user on each node, which enables password-less authentication.

After the key is passed to the nodes, you can use the key pair to SSH in to the RHCOS nodes as the user core. To access the nodes through SSH, the private key identity must be managed by SSH for your local user.

If you want to SSH in to your cluster nodes to perform installation debugging or disaster recovery, you must provide the SSH public key during the installation process. The ./openshift-install gather command also requires the SSH public key to be in place on the cluster nodes.

Do not skip this procedure in production environments, where disaster recovery and debugging is required.
If you do not have an existing SSH key pair on your local machine to use for authentication onto your cluster nodes, create one. For example, on a computer that uses a Linux operating system, run the following command:

View the public SSH key:

Add the SSH private key identity to the SSH agent for your local user, if it has not already been added. SSH agent management of the key is required for password-less SSH authentication onto your cluster nodes, or if you want to use the ./openshift-install gather command.

Add your SSH private key to the ssh-agent:


When you install "Red Hat OpenShift Container Platform", provide the SSH public key to the installation program.
Enabling access to the environment
At deployment, all "Red Hat OpenShift Container Platform" machines are created in a Red Hat OpenStack Platform (RHOSP)-tenant network. Therefore, they are not accessible directly in most RHOSP deployments.

You can configure "Red Hat OpenShift Container Platform" API and application access by using floating IP addresses (FIPs) during installation. You can also complete an installation without configuring FIPs, but the installer will not configure a way to reach the API or applications externally.

Enabling access with floating IP addresses
Create floating IP (FIP) addresses for external access to the "Red Hat OpenShift Container Platform" API and cluster applications.

Using the Red Hat OpenStack Platform (RHOSP) CLI, create the API FIP:

Using the Red Hat OpenStack Platform (RHOSP) CLI, create the apps, or Ingress, FIP:

Add records that follow these patterns to your DNS server for the API and Ingress FIPs:

Add the FIPs to the
install-config.yaml
file as the values of the following
parameters:


If you use these values, you must also enter an external network as the value of the platform.openstack.externalNetwork parameter in the install-config.yaml file.

You can make "Red Hat OpenShift Container Platform" resources available outside of the cluster by assigning a floating IP address and updating your firewall configuration.
Completing installation without floating IP addresses
You can install "Red Hat OpenShift Container Platform" on Red Hat OpenStack Platform (RHOSP) without providing floating IP addresses.

In the install-config.yaml file, do not define the following parameters:

platform.openstack.ingressFloatingIP

platform.openstack.apiFloatingIP


If you cannot provide an external network, you can also leave platform.openstack.externalNetwork blank. If you do not provide a value for platform.openstack.externalNetwork, a router is not created for you, and, without additional action, the installer will fail to retrieve an image from Glance. You must configure external connectivity on your own.

If you run the installer from a system that cannot reach the cluster API due to a lack of floating IP addresses or name resolution, installation fails. To prevent installation failure in these cases, you can use a proxy network or run the installer from a system that is on the same network as your machines.

You can enable name resolution by creating DNS records for the API and Ingress ports. For example:

api.<cluster_name>.<base_domain>.  IN  A  <api_port_IP>
*.apps.<cluster_name>.<base_domain>. IN  A <ingress_port_IP>
If you do not control the DNS server, you can add the record to your /etc/hosts file. This action makes the API accessible to only you, which is not suitable for production deployment but does allow installation for development and testing.
Deploying the cluster
You can install "Red Hat OpenShift Container Platform" on a compatible cloud platform.

You can run the create cluster command of the installation program only once, during initial installation.
You have the "Red Hat OpenShift Container Platform" installation program and the pull secret for your cluster.

You have verified that the cloud provider account on your host has the correct permissions to deploy the cluster. An account with incorrect permissions causes the installation process to fail with an error message that displays the missing permissions.


Change to the directory that contains the installation program and initialize the cluster deployment:


When the cluster deployment completes successfully:

The terminal displays directions for accessing your cluster, including a link to the web console and credentials for the kubeadmin user.

Credential information also outputs to <installation_directory>/.openshift_install.log.


Do not delete the installation program or the files that the installation program creates. Both are required to delete the cluster.
...
INFO Install complete!
INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/myuser/install_dir/auth/kubeconfig'
INFO Access the OpenShift web-console here: https://console-openshift-console.apps.mycluster.example.com
INFO Login to the console with user: "kubeadmin", and password: "password"
INFO Time elapsed: 36m22s
The Ignition config files that the installation program generates contain certificates that expire after 24 hours, which are then renewed at that time. If the cluster is shut down before renewing the certificates and the cluster is later restarted after the 24 hours have elapsed, the cluster automatically recovers the expired certificates. The exception is that you must manually approve the pending node-bootstrapper certificate signing requests (CSRs) to recover kubelet certificates. See the documentation for Recovering from expired control plane certificates for more information.

It is recommended that you use Ignition config files within 12 hours after they are generated because the 24-hour certificate rotates from 16 to 22 hours after the cluster is installed. By using the Ignition config files within 12 hours, you can avoid installation failure if the certificate update runs during installation.
Verifying cluster status
You can verify your "Red Hat OpenShift Container Platform" cluster's status during or after installation.

In the cluster environment, export the administrator's kubeconfig file:

View the control plane and compute machines created after a deployment:

View your cluster's version:

View your Operators' status:

View all running pods in the cluster:
Logging in to the cluster by using the CLI
You can log in to your cluster as a default system user by exporting the cluster kubeconfig file. The kubeconfig file contains information about the cluster that is used by the CLI to connect a client to the correct cluster and API server. The file is specific to a cluster and is created during "Red Hat OpenShift Container Platform" installation.

You deployed an "Red Hat OpenShift Container Platform" cluster.

You installed the oc CLI.


Export the kubeadmin credentials:

Verify you can run oc commands successfully using the exported configuration:


See Accessing the web console for more details about accessing and understanding the "Red Hat OpenShift Container Platform" web console.
Telemetry access for "Red Hat OpenShift Container Platform"
In "Red Hat OpenShift Container Platform" {product-version}, the Telemetry service, which runs by default to provide metrics about cluster health and the success of updates, requires internet access. If your cluster is connected to the internet, Telemetry runs automatically, and your cluster is registered to OpenShift Cluster Manager Hybrid Cloud Console.

After you confirm that your OpenShift Cluster Manager Hybrid Cloud Console inventory is correct, either maintained automatically by Telemetry or manually by using OpenShift Cluster Manager, use subscription watch to track your "Red Hat OpenShift Container Platform" subscriptions at the account or multi-cluster level.

See About remote health monitoring for more information about the Telemetry service
Next steps
Customize your cluster.

If necessary, you can
opt out of remote health reporting.

If you need to enable external access to node ports, configure ingress cluster traffic by using a node port.

If you did not configure RHOSP to accept application traffic over floating IP addresses, configure RHOSP access with floating IP addresses.