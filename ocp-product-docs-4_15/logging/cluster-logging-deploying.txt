Installing Logging

You can install the logging subsystem for Red Hat OpenShift by deploying the Red Hat OpenShift Logging Operator. The logging subsystem Operator creates and manages the components of the logging stack.

For new installations, Vector and LokiStack are recommended. Documentation for logging is in the process of being updated to reflect these underlying component changes.
As of logging version 5.6 Fluentd is deprecated and is planned to be removed in a future release. Red Hat will provide bug fixes and support for this feature during the current release lifecycle, but this feature will no longer receive enhancements and will be removed. As an alternative to Fluentd, you can use Vector instead.
Installing the logging subsystem for Red Hat OpenShift using the web console
If you do not want to use the default Elasticsearch log store, you can remove the internal Elasticsearch logStore and Kibana visualization components from the ClusterLogging custom resource (CR). Removing these components is optional but saves resources.
Logging is provided as an installable component, with a distinct release cycle from the core "Red Hat OpenShift Container Platform". The Red Hat OpenShift Container Platform Life Cycle Policy outlines release compatibility.
In the "Red Hat OpenShift Container Platform" web console, click Operators -> OperatorHub.

Choose  Red Hat OpenShift Logging from the list of available Operators, and click Install.

Ensure that A specific namespace on the cluster is selected under Installation mode.

Ensure that Operator recommended namespace is openshift-logging under Installed Namespace.

Select Enable operator recommended cluster monitoring on this namespace.

Select stable-5.x as the Update channel.

Select an Update approval.

Select Enable or Disable for the Console plugin.

Click Install.


Verify that the Red Hat OpenShift Logging Operator is installed by switching to the Operators → Installed Operators page.

Create a ClusterLogging instance.

Create an OpenShift Logging instance:

Verify the install:


If Alertmanager logs alerts such as Prometheus could not scrape fluentd for more than 10m, make sure that openshift.io/cluster-monitoring is set to "true" for the OpenShift Elasticsearch Operator and OpenShift Logging Operator. See the Red Hat KnowledgeBase for more information: Prometheus could not scrape fluentd for more than 10m alert in Alertmanager
Installing the logging subsystem for Red Hat OpenShift using the CLI
You can use the "Red Hat OpenShift Container Platform" CLI to install the OpenShift Elasticsearch and Red Hat OpenShift Logging Operators.

Ensure that you have the necessary persistent storage for Elasticsearch. Note that each Elasticsearch node requires its own storage volume.


To install the OpenShift Elasticsearch Operator and Red Hat OpenShift Logging Operator using the CLI:

Create a namespace for the OpenShift Elasticsearch Operator.

Create a namespace for the Red Hat OpenShift Logging Operator:

Install the OpenShift Elasticsearch Operator by creating the following objects:

Install the Red Hat OpenShift Logging Operator by creating the following objects:

Create an OpenShift Logging instance:

Verify the installation by listing the pods in the openshift-logging project.
Installing the Elasticsearch Operator
As of logging version 5.4.3 the OpenShift Elasticsearch Operator is deprecated and is planned to be removed in a future release. Red Hat will provide bug fixes and support for this feature during the current release lifecycle, but this feature will no longer receive enhancements and will be removed. As an alternative to using the OpenShift Elasticsearch Operator to manage the default log storage, you can use the Loki Operator.
Storage considerations for Elasticsearch
A persistent volume is required for each Elasticsearch deployment configuration. On "Red Hat OpenShift Container Platform" this is achieved using persistent volume claims (PVCs).

If you use a local volume for persistent storage, do not use a raw block volume, which is described with volumeMode: block in the LocalVolume object. Elasticsearch cannot use raw block volumes.
The OpenShift Elasticsearch Operator names the PVCs using the Elasticsearch resource name.

Fluentd ships any logs from systemd journal and /var/log/containers/*.log to Elasticsearch.

Elasticsearch requires sufficient memory to perform large merge operations. If it does not have enough memory, it becomes unresponsive. To avoid this problem, evaluate how much application log data you need, and allocate approximately double that amount of free storage capacity.

By default, when storage capacity is 85% full, Elasticsearch stops allocating new data to the node. At 90%, Elasticsearch attempts to relocate existing shards from that node to other nodes if possible. But if no nodes have a free capacity below 85%, Elasticsearch effectively rejects creating new indices and becomes RED.

These low and high watermark values are Elasticsearch defaults in the current release. You can modify these default values. Although the alerts use the same default values, you cannot change these values in the alerts.
Installing the OpenShift Elasticsearch Operator by using the web console
The OpenShift Elasticsearch Operator creates and manages the Elasticsearch cluster used by OpenShift Logging.

Elasticsearch is a memory-intensive application. Each Elasticsearch node needs at least 16GB of memory for both memory requests and limits, unless you specify otherwise in the ClusterLogging custom resource.

Ensure that you have the necessary persistent storage for Elasticsearch. Note that each Elasticsearch node
requires its own storage volume.


In the "Red Hat OpenShift Container Platform" web console, click Operators -> OperatorHub.

Click OpenShift Elasticsearch Operator from the list of available Operators, and click Install.

Ensure that the All namespaces on the cluster is selected under Installation mode.

Ensure that openshift-operators-redhat is selected under Installed Namespace.

Select Enable operator recommended cluster monitoring on this namespace.

Select stable-5.x as the Update channel.

Select an Update approval strategy:

Click Install.


Verify that the OpenShift Elasticsearch Operator installed by switching to the Operators → Installed Operators page.

Ensure that OpenShift Elasticsearch Operator is listed in all projects with a Status of Succeeded.


Installing Operators from the OperatorHub

Removing unused components if you do not use the default Elasticsearch log store
Postinstallation tasks
If your network plugin enforces network isolation, allow network traffic between the projects that contain the logging subsystem Operators.

Allowing traffic between projects when network isolation is enabled
Your cluster network plugin might enforce network isolation. If so, you must allow network traffic between the projects that contain the operators deployed by OpenShift Logging.

Network isolation blocks network traffic between pods or services that are in different projects. The logging subsystem installs the OpenShift Elasticsearch Operator in the openshift-operators-redhat project and the Red Hat OpenShift Logging Operator in the openshift-logging project. Therefore, you must allow traffic between these two projects.

"Red Hat OpenShift Container Platform" offers two supported choices for the network plugin, OpenShift SDN and OVN-Kubernetes. These two providers implement various network isolation policies.

OpenShift SDN has three modes:


network policy
This is the default mode. If no policy is defined, it allows all traffic. However, if a user defines a policy, they typically start by denying all traffic and then adding exceptions. This process might break applications that are running in different projects. Therefore, explicitly configure the policy to allow traffic to egress from one logging-related project to the other.
subnet
This mode allows all traffic. It does not enforce network isolation. No action is needed.


OVN-Kubernetes always uses a network policy. Therefore, as with OpenShift SDN, you must configure the policy to allow traffic to egress from one logging-related project to the other.

If you are using OpenShift SDN in multitenant mode, join the two projects. For example:

Otherwise, for OpenShift SDN in network policy mode and OVN-Kubernetes, perform the following actions: