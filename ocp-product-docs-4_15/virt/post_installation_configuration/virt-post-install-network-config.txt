Postinstallation network configuration

By default, OpenShift Virtualization is installed with a single, internal pod network.

After you install OpenShift Virtualization, you can install networking Operators and configure additional networks.
Installing networking Operators
You must install the Kubernetes NMState Operator to configure a Linux bridge network for live migration or external access to virtual machines (VMs).

You can install the SR-IOV Operator to manage SR-IOV network devices and network attachments.

Installing the Kubernetes NMState Operator by using the web console
You can install the Kubernetes NMState Operator by using the web console. After it is installed, the Operator can deploy the NMState State Controller as a daemon set across all of the cluster nodes.

You are logged in as a user with cluster-admin privileges.


Select Operators -> OperatorHub.

In the search field below All Items, enter nmstate and click Enter to search for the Kubernetes NMState Operator.

Click on the Kubernetes NMState Operator search result.

Click on Install to open the Install Operator window.

Click Install to install the Operator.

After the Operator finishes installing, click View Operator.

Under Provided APIs, click Create Instance to open the dialog box for creating an instance of kubernetes-nmstate.

In the Name field of the dialog box, ensure the name of the instance is nmstate.

Accept the default settings and click Create to create the instance.


Once complete, the Operator has deployed the NMState State Controller as a daemon set across all of the cluster nodes.
Installing the SR-IOV Network Operator
As a cluster administrator, you can install the Single Root I/O Virtualization (SR-IOV) Network Operator by using the "Red Hat OpenShift Container Platform" CLI or the web console.

CLI: Installing the SR-IOV Network Operator
As a cluster administrator, you can install the Operator using the CLI.

A cluster installed on bare-metal hardware with nodes that have hardware that supports SR-IOV.

Install the OpenShift CLI (oc).

An account with cluster-admin privileges.


To create the openshift-sriov-network-operator namespace, enter the following command:

To create an OperatorGroup CR, enter the following command:

Subscribe to the SR-IOV Network Operator.

To verify that the Operator is installed, enter the following command:
Web console: Installing the SR-IOV Network Operator
As a cluster administrator, you can install the Operator using the web console.

A cluster installed on bare-metal hardware with nodes that have hardware that supports SR-IOV.

Install the OpenShift CLI (oc).

An account with cluster-admin privileges.


Install the SR-IOV Network Operator:

Verify that the SR-IOV Network Operator is installed successfully:
Configuring a Linux bridge network
After you install the Kubernetes NMState Operator, you can configure a Linux bridge network for live migration or external access to virtual machines (VMs).

Creating a Linux bridge NNCP
You can create a NodeNetworkConfigurationPolicy (NNCP) manifest for a Linux bridge network.

You have installed the Kubernetes NMState Operator.


Create the NodeNetworkConfigurationPolicy manifest. This example includes sample values that you must replace with your own information.
Creating a Linux bridge NAD by using the web console
You can create a network attachment definition (NAD) to provide layer-2 networking to pods and virtual machines by using the "Red Hat OpenShift Container Platform" web console.

A Linux bridge network attachment definition is the most efficient method for connecting a virtual machine to a VLAN.

Configuring IP address management (IPAM) in a network attachment definition for virtual machines is not supported.
In the web console, click Networking -> NetworkAttachmentDefinitions.

Click Create Network Attachment Definition.

Enter a unique Name and optional Description.

Select CNV Linux bridge from the Network Type list.

Enter the name of the bridge in the Bridge Name field.

Optional: If the resource has VLAN IDs configured, enter the ID numbers in the VLAN Tag Number field.

Optional: Select MAC Spoof Check to enable MAC spoof filtering. This feature provides security against a MAC spoofing attack by allowing only a single MAC address to exit the pod.

Click Create.
Next steps
Attaching a virtual machine (VM) to a Linux bridge network
Configuring a network for live migration
After you have configured a Linux bridge network, you can configure a dedicated network for live migration. A dedicated network minimizes the effects of network saturation on tenant workloads during live migration.

Configuring a dedicated secondary network for live migration
To configure a dedicated secondary network for live migration, you must first create a bridge network attachment definition (NAD) by using the CLI. Then, you add the name of the NetworkAttachmentDefinition object to the HyperConverged custom resource (CR).

You installed the OpenShift CLI (oc).

You logged in to the cluster as a user with the cluster-admin role.

Each node has at least two Network Interface Cards (NICs).

The NICs for live migration are connected to the same VLAN.


Create a NetworkAttachmentDefinition manifest according to the following example:

Open the HyperConverged CR in your default editor by running the following command:

Add the name of the NetworkAttachmentDefinition object to the spec.liveMigrationConfig stanza of the HyperConverged CR:

Save your changes and exit the editor. The virt-handler pods restart and connect to the secondary network.


When the node that the virtual machine runs on is placed into maintenance mode, the VM automatically migrates to another node in the cluster. You can verify that the migration occurred over the secondary network and not the default pod network by checking the target IP address in the virtual machine instance (VMI) metadata.
Selecting a dedicated network by using the web console
You can select a dedicated network for live migration by using the "Red Hat OpenShift Container Platform" web console.

You configured a Multus network for live migration.


Navigate to Virtualization > Overview in the "Red Hat OpenShift Container Platform" web console.

Click the Settings tab and then click Live migration.

Select the network from the Live migration network list.
Configuring an SR-IOV network
After you install the SR-IOV Operator, you can configure an SR-IOV network.

Configuring SR-IOV network devices
The SR-IOV Network Operator adds the SriovNetworkNodePolicy.sriovnetwork.openshift.io CustomResourceDefinition to "Red Hat OpenShift Container Platform". You can configure an SR-IOV network device by creating a SriovNetworkNodePolicy custom resource (CR).

When applying the configuration specified in a SriovNetworkNodePolicy object, the SR-IOV Operator might drain the nodes, and in some cases, reboot nodes.

It might take several minutes for a configuration change to apply.
You installed the OpenShift CLI (oc).

You have access to the cluster as a user with the cluster-admin role.

You have installed the SR-IOV Network Operator.

You have enough available nodes in your cluster to handle the evicted workload from drained nodes.

You have not selected any control plane nodes for SR-IOV network device configuration.


Create an SriovNetworkNodePolicy object, and then save the YAML in the <name>-sriov-node-network.yaml file. Replace <name> with the name for this configuration.

Optional: Label the SR-IOV capable cluster nodes with SriovNetworkNodePolicy.Spec.NodeSelector if they are not already labeled. For more information about labeling nodes, see "Understanding how to update labels on nodes".

Create the SriovNetworkNodePolicy object:

To verify that the SR-IOV network device is configured, enter the following command. Replace <node_name> with the name of a node with the SR-IOV network device that you just configured.
Next steps
Attaching a virtual machine (VM) to an SR-IOV network
Enabling load balancer service creation by using the web console
You can enable the creation of load balancer services for a virtual machine (VM) by using the "Red Hat OpenShift Container Platform" web console.

You have configured a load balancer for the cluster.

You are logged in as a user with the cluster-admin role.


Navigate to Virtualization -> Overview.

On the Settings tab, click Cluster.

Expand LoadBalancer service and select Enable the creation of LoadBalancer services for SSH connections to VirtualMachines.